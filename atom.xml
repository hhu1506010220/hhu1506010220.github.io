<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>想飞的小菜鸡</title>
  
  <subtitle>默默发声，踽踽独行</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://hhu1506010220.github.io/"/>
  <updated>2021-05-06T15:03:30.572Z</updated>
  <id>http://hhu1506010220.github.io/</id>
  
  <author>
    <name>Vodkazy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>模型训练与测试常用模板代码</title>
    <link href="http://hhu1506010220.github.io/2021/05/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%B5%8B%E8%AF%95%E5%B8%B8%E7%94%A8%E6%A8%A1%E6%9D%BF%E4%BB%A3%E7%A0%81/"/>
    <id>http://hhu1506010220.github.io/2021/05/06/模型训练与测试常用模板代码/</id>
    <published>2021-05-06T14:57:19.000Z</published>
    <updated>2021-05-06T15:03:30.572Z</updated>
    
    <content type="html"><![CDATA[<p>无意中发现了一份模型训练的模板，觉得很不错，刻意码下来备用。<br><a id="more"></a></p><p>原文转自：<a href="https://cloud.tencent.com/developer/article/1686256" target="_blank" rel="noopener">【colab pytorch】训练和测试常用模板代码</a></p><p><strong>1、分类模型训练代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">total_step = len(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i ,(images, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward and optimizer</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;'</span></span><br><span class="line">                  .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure><p><strong>2、分类模型测试代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the model</span></span><br><span class="line">model.eval()  <span class="comment"># eval mode(batch norm uses moving mean/variance </span></span><br><span class="line">              <span class="comment">#instead of mini-batch mean/variance)</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Test accuracy of the model on the 10000 test images: &#123;&#125; %'</span></span><br><span class="line">          .format(<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><p><strong>3、自定义损失函数</strong></p><p>继承torch.nn.Module类写自己的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        loss = torch.mean((x - y) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p><strong>4、标签平滑</strong></p><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, e=<span class="number">0.1</span>, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        self.e = e</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one_hot</span><span class="params">(self, labels, classes, value=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Convert labels to one hot vectors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            labels: torch tensor in format [label1, label2, label3, ...]</span></span><br><span class="line"><span class="string">            classes: int, number of classes</span></span><br><span class="line"><span class="string">            value: label value in one hot vector, default to 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            return one hot format labels in shape [batchsize, classes]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        one_hot = torch.zeros(labels.size(<span class="number">0</span>), classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#labels and value_added  size must match</span></span><br><span class="line">        labels = labels.view(labels.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        value_added = torch.Tensor(labels.size(<span class="number">0</span>), <span class="number">1</span>).fill_(value)</span><br><span class="line"></span><br><span class="line">        value_added = value_added.to(labels.device)</span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line"></span><br><span class="line">        one_hot.scatter_add_(<span class="number">1</span>, labels, value_added)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span><span class="params">(self, target, length, smooth_factor)</span>:</span></span><br><span class="line">        <span class="string">"""convert targets to one-hot format, and smooth</span></span><br><span class="line"><span class="string">        them.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            target: target in form with [label1, label2, label_batchsize]</span></span><br><span class="line"><span class="string">            length: length of one-hot format(number of classes)</span></span><br><span class="line"><span class="string">            smooth_factor: smooth factor for label smooth</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            smoothed labels in one hot format</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        one_hot = self._one_hot(target, length, value=<span class="number">1</span> - smooth_factor)</span><br><span class="line">        one_hot += smooth_factor / (length - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot.to(target.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.size(<span class="number">0</span>) != target.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)'</span></span><br><span class="line">                    .format(x.size(<span class="number">0</span>), target.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Expected input tensor to have least 2 dimensions(got &#123;&#125;)'</span></span><br><span class="line">                    .format(x.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Only 2 dimension tensor are implemented, (got &#123;&#125;)'</span></span><br><span class="line">                    .format(x.size()))</span><br><span class="line"></span><br><span class="line">        smoothed_target = self._smooth_label(target, x.size(<span class="number">1</span>), self.e)</span><br><span class="line">        x = self.log_softmax(x)</span><br><span class="line">        loss = torch.sum(- x * smoothed_target, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">'none'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.sum(loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'unrecognized option, expect reduction to be one of none, mean, sum'</span>)</span><br></pre></td></tr></table></figure><p>或者直接在训练文件里做label smoothing</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line">    N = labels.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># C is the number of classes.</span></span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=<span class="number">0.1</span> / (C - <span class="number">1</span>)).cuda()</span><br><span class="line">    smoothed_labels.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(labels, dim=<span class="number">1</span>), value=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    score = model(images)</span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=<span class="number">1</span>)</span><br><span class="line">    loss = -torch.sum(log_prob * smoothed_labels) / N</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p><strong>5、mixup训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup images and labels.</span></span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()</span><br><span class="line">    index = torch.randperm(images.size(<span class="number">0</span>)).cuda()</span><br><span class="line">    mixed_images = lambda_ * images + (<span class="number">1</span> - lambda_) * images[index, :]</span><br><span class="line">    label_a, label_b = labels, labels[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup loss.</span></span><br><span class="line">    scores = model(mixed_images)</span><br><span class="line">    loss = (lambda_ * loss_function(scores, label_a)</span><br><span class="line">            + (<span class="number">1</span> - lambda_) * loss_function(scores, label_b))</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p><strong>6、L1正则化</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">loss = ...  <span class="comment"># Standard cross-entropy loss</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><p><strong>7、不对偏置进行权重衰减</strong></p><p>pytorch里的weight decay相当于l2正则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] == <span class="string">'bias'</span>)</span><br><span class="line">others_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] != <span class="string">'bias'</span>)</span><br><span class="line">parameters = [&#123;<span class="string">'parameters'</span>: bias_list, <span class="string">'weight_decay'</span>: <span class="number">0</span>&#125;,                </span><br><span class="line">              &#123;<span class="string">'parameters'</span>: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure><p><strong>8、梯度裁剪</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><p><strong>9、得到当前学习率</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If there is one global learning rate (which is the common case).</span></span><br><span class="line">lr = next(iter(optimizer.param_groups))[<span class="string">'lr'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If there are multiple learning rates for different layers.</span></span><br><span class="line">all_lr = []</span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    all_lr.append(param_group[<span class="string">'lr'</span>])</span><br></pre></td></tr></table></figure><p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p><p><strong>10、学习率衰减</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reduce learning rate when validation accuarcy plateau.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">'max'</span>, patience=<span class="number">5</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cosine annealing learning rate.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="number">80</span>)</span><br><span class="line"><span class="comment"># Reduce learning rate by 10 at given epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">50</span>, <span class="number">70</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate warmup by 10 epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> t: t / <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure><p><strong>11、优化器链式更新</strong></p><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ExponentialLR, StepLR</span><br><span class="line">model = [torch.nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>))]</span><br><span class="line">optimizer = SGD(model, <span class="number">0.1</span>)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler2 = StepLR(optimizer, step_size=<span class="number">3</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    print(epoch, scheduler2.get_last_lr()[<span class="number">0</span>])</span><br><span class="line">    optimizer.step()</span><br><span class="line">    scheduler1.step()</span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure><p><strong>12、模型训练可视化</strong></p><p>pip install tensorboard</p><p>tensorboard —logdir=runs</p><p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_iter <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">'Loss/train'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Loss/test'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Accuracy/train'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Accuracy/test'</span>, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure><p><strong>13、保存和加载断点</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">tart_epoch = <span class="number">0</span></span><br><span class="line"><span class="comment"># Load checkpoint.</span></span><br><span class="line"><span class="keyword">if</span> resume: <span class="comment"># resume为参数，第一次训练时设为0，中断再训练时设为1</span></span><br><span class="line">    model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'best_checkpoint.pth.tar'</span>)</span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(model_path)</span><br><span class="line">    checkpoint = torch.load(model_path)</span><br><span class="line">    best_acc = checkpoint[<span class="string">'best_acc'</span>]</span><br><span class="line">    start_epoch = checkpoint[<span class="string">'epoch'</span>]</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">'model'</span>])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">'optimizer'</span>])</span><br><span class="line">    print(<span class="string">'Load checkpoint at epoch &#123;&#125;.'</span>.format(start_epoch))</span><br><span class="line">    print(<span class="string">'Best accuracy so far &#123;&#125;.'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, num_epochs): </span><br><span class="line">    ... </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test the model</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save checkpoint</span></span><br><span class="line">    is_best = current_acc &gt; best_acc</span><br><span class="line">    best_acc = max(current_acc, best_acc)</span><br><span class="line">    checkpoint = &#123;</span><br><span class="line">        <span class="string">'best_acc'</span>: best_acc,</span><br><span class="line">        <span class="string">'epoch'</span>: epoch + <span class="number">1</span>,</span><br><span class="line">        <span class="string">'model'</span>: model.state_dict(),</span><br><span class="line">        <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'checkpoint.pth.tar'</span>)</span><br><span class="line">    best_model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'best_checkpoint.pth.tar'</span>)</span><br><span class="line">    torch.save(checkpoint, model_path)</span><br><span class="line">    <span class="keyword">if</span> is_best:</span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure><p><strong>14、提取Imagenet预训练模型某层的特征</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VGG-16 relu5-3 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>).features[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># VGG-16 pool5 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>).features</span><br><span class="line"><span class="comment"># VGG-16 fc7 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:<span class="number">-3</span>])</span><br><span class="line"><span class="comment"># ResNet GAP feature.</span></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model = torch.nn.Sequential(collections.OrderedDict(</span><br><span class="line">    list(model.named_children())[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.eval()</span><br><span class="line">    conv_representation = model(image)</span><br></pre></td></tr></table></figure><p><strong>15、提取imagenet预训练模型多层卷积特征</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Helper class to extract several convolution features from the given</span></span><br><span class="line"><span class="string">    pre-trained model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        _model, torch.nn.Module.</span></span><br><span class="line"><span class="string">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(</span></span><br><span class="line"><span class="string">                list(model.named_children())[:-1]))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; conv_representation = FeatureExtractor(</span></span><br><span class="line"><span class="string">                pretrained_model=model,</span></span><br><span class="line"><span class="string">                layers_to_extract=&#123;'layer1', 'layer2', 'layer3', 'layer4'&#125;)(image)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrained_model, layers_to_extract)</span>:</span></span><br><span class="line">        torch.nn.Module.__init__(self)</span><br><span class="line">        self._model = pretrained_model</span><br><span class="line">        self._model.eval()</span><br><span class="line">        self._layers_to_extract = set(layers_to_extract)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            conv_representation = []</span><br><span class="line">            <span class="keyword">for</span> name, layer <span class="keyword">in</span> self._model.named_children():</span><br><span class="line">                x = layer(x)</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">in</span> self._layers_to_extract:</span><br><span class="line">                    conv_representation.append(x)</span><br><span class="line">            <span class="keyword">return</span> conv_representation</span><br></pre></td></tr></table></figure><p><strong>16、微调全连接层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)  <span class="comment"># Replace the last fc layer</span></span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span></span><br></pre></td></tr></table></figure><p><strong>17、以较大学习率微调全连接层，较小学习率微调卷积层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> id(p) <span class="keyword">not</span> <span class="keyword">in</span> finetuned_parameters)</span><br><span class="line">parameters = [&#123;<span class="string">'params'</span>: conv_parameters, <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;, </span><br><span class="line">              &#123;<span class="string">'params'</span>: model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2021/05/06/模型训练与测试常用模板代码" target="_blank" rel="noopener">https://vodkazy.cn/2021/05/06/模型训练与测试常用模板代码</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无意中发现了一份模型训练的模板，觉得很不错，刻意码下来备用。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>关于SQL的一些非SELECT知识点</title>
    <link href="http://hhu1506010220.github.io/2021/01/04/%E5%85%B3%E4%BA%8ESQL%E7%9A%84%E4%B8%80%E4%BA%9B%E9%9D%9ESELECT%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>http://hhu1506010220.github.io/2021/01/04/关于SQL的一些非SELECT知识点/</id>
    <published>2021-01-04T14:34:52.000Z</published>
    <updated>2021-01-04T14:41:02.435Z</updated>
    
    <content type="html"><![CDATA[<p>复习一波分布式SQL的基础语法，其中有一些之前都没怎么用到过。</p><a id="more"></a><h2><span id="运算符">运算符</span></h2><p><code>BETWWEN A AND B</code>，判断 A ≤ x ≤ B。</p><p><code>a || b || c</code>，字符串连接符，相当于concat(a,b,c)。</p><p><code>key IN (1,2,3)</code>，判断key是否在集合中，后边跟的必须是个集合，和Python的in不太一样。</p><p><code>a&lt;&gt;b</code>，不等于，等价于<code>a!=b</code>。</p><p><code>a&lt;=&gt;b</code>，判断a不等于b是否非空，等价于<code>ifNull(a!=b, false)</code>。</p><h2><span id="select">SELECT</span></h2><p><code>SELECT</code>可以通过正则来选择符合特定规则的列名。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="string">`(ds_1|dss_1)?+.+`</span> <span class="keyword">FROM</span> A t;</span><br></pre></td></tr></table></figure><p>可以通过表名后边跟一个别名来为查询表取别名。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="number">2</span>) (<span class="number">2</span>,<span class="number">3</span>) t(a,b);</span><br></pre></td></tr></table></figure><p>可以通过<code>VALUES</code>语句象征性得产生一个临时表。</p><p><code>DELETE FROM A [WHERE ...]</code>用于删除某些行，<code>UPDATE A SET a=&#39;a&#39; [WHERE ...]</code>用于更新某些行的值。</p><h2><span id="group-by">GROUP BY</span></h2><p><code>GROUP BY</code>后跟的KEY必须是<strong>输入表</strong>的列名或基于列名的表达式，不能是<code>SELECT</code>输出列（别名也不行，除非输出列的列名是原表的列名）。在SQL解析中，<strong><code>GROUP BY</code>操作通常是先于<code>SELECT</code>操作的</strong>，因此<code>GROUP BY</code>只能接受输入表的列或表达式为KEY。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a <span class="keyword">AS</span> b <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a;<span class="comment">-- 对，因为a本身是A的列名</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="keyword">AS</span> b <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> b;<span class="comment">-- 错，b是输出列</span></span><br></pre></td></tr></table></figure><p>并且<code>GROUP BY</code>和<code>SELECT</code>之间存在制约关系，<code>SELECT</code>的所有列中，没有使用聚合函数的列，<strong>必须</strong>出现在<code>GROUP BY</code>中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a,b <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a;<span class="comment">-- 错，会报错说b也必须出现在GROUP BY里</span></span><br><span class="line"><span class="keyword">SELECT</span> a,<span class="keyword">SUM</span>(b) <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a;<span class="comment">-- 对，因为b进行了聚合操作</span></span><br><span class="line"><span class="keyword">SELECT</span> a <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a;<span class="comment">-- 对，因为只有一个变量</span></span><br></pre></td></tr></table></figure><h2><span id="order-by">ORDER BY</span></h2><p><code>ORDER BY</code>必须与<code>LIMIT</code>联用（因为是全局排序），且KEY必须是<code>SELECT</code>的输出列的列名或者别名。</p><p>对于<code>NULL</code>，在mysql中认为比任何值都小，在oracle中则不是这样。</p><p>排序的关键字：<code>DESC</code>是降序，<code>ASC</code>是升序，默认是<code>DESC</code>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a;<span class="comment">-- 错，因为没和LIMIT连用</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">ORDER</span> <span class="keyword">BY</span> a <span class="keyword">DESC</span>, b <span class="keyword">ASC</span>;<span class="comment">-- 对，分别排序规则</span></span><br></pre></td></tr></table></figure><h2><span id="distribute-by">DISTRIBUTE BY</span></h2><p>对数据按照某几列的值做hash分片，必须使用<code>SELECT</code>的输出列别名。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a <span class="keyword">FROM</span> A <span class="keyword">DISTRIBUTE</span> <span class="keyword">BY</span> a;</span><br></pre></td></tr></table></figure><p><code>SORT BY</code>：局部排序，似乎主要在诸如MapReduce-based的分布式数据库查询中见到。如果<code>SORT BY</code>语句前有<code>DISTRIBUTE BY</code>，<code>SORT BY</code>即对<code>DISTRIBUTE BY</code>的结果进行局部排序。<code>DISTRIBUTE BY</code>控制 map 的输出在reduer中是如何划分的。</p><p>如果<code>SORT BY</code>语句前没有<code>DISTRIBUTE BY</code>，<code>SORT BY</code>即对每个reduce中的数据进行排序，同样是执行一个局部排序过程。这可以保证每个reduce的输出落盘数据都是有序的，从而能够增加存储压缩率，同时读取的时候如果有过滤，能够利用这个信息减少真正从磁盘读取的数据量，提高后面进行的全局排序的效率。</p><h2><span id="cluster-by">CLUSTER BY</span></h2><p><code>CLUSTER BY</code>相当于<code>DISTRIBUTE BY</code>和<code>SORT BY</code>合用，但是<code>CLUSTER BY</code>不能指定排序为asc或 desc 的规则。</p><p><code>ORDER BY</code>不和<code>DISTRIBUTE BY</code>/<code>SORT BY</code>共用，同时<code>GROUP BY</code>也不和<code>DISTRIBUTE BY</code>/<code>SORT BY</code>共用。</p><p><code>ORDER BY</code>/<code>DISTRIBUTE BY</code>/<code>SORT BY</code>是后于<code>SELECT</code>操作的，因此它们只能接受<code>SELECT</code>语句的输出列为key。</p><h2><span id="unionintersectexcept">UNION/INTERSECT/EXCEPT</span></h2><p>三者相当于对集合进行的<strong>交并补</strong>操作。三种语法均有<code>ALL</code>和<code>DISTINCT</code>两种方式，其中<code>ALL</code>方式下不做去重，而<code>DISTINCT</code>方式下会做去重。默认是<code>DISTINCT</code>方式，因此<code>DISTINCT</code>关键字可以省略。</p><ul><li><strong>UNION</strong>: 求两个数据集的并集。即将两个数据集合并成一个数据集。UNION可以把多个SELECT操作返回的结果，联合成一个数据集。但是连接的两个SELECT查询语句，两个SELECT的列个数、列名称、列类型必须严格一致。如果原名称不一致，可以通过别名设置成相同的名称。</li><li><strong>INTERSECT</strong>: 求两个数据集的交集。即输出两个数据集均包含的记录。</li><li><strong>EXCEPT</strong>: 求第二个数据集在第一个数据集中的补集。即输出第一个数据集包含而第二个数据集不包含的记录。</li><li><strong>MINUS</strong>: 等同于EXCEPT。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">UNION</span>|<span class="keyword">INTERSECT</span>|<span class="keyword">EXCEPT</span> [ALL|<span class="keyword">DISTINCT</span>]</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> B;</span><br></pre></td></tr></table></figure><p>注意事项：</p><ul><li>集合操作的结果不保证顺序。</li><li>集合操作左右两个分支要求列个数必须一致。如果类型不一致，可能会插入隐式类型转换。</li><li>支持通过括号指定优先级。</li><li>UNION后如果有ORDER BY | LIMIT等子句，则其将作用于前面最后一个UNION的结果。也可以通过一些语句进行设置，使之作用于前面所有UNION。</li></ul><h2><span id="join">JOIN</span></h2><p>{LEFT OUTER | RIGHT OUTER | FULL OUTER | INNER} JOIN，四种JOIN操作。这里忽视CROSS JOIN 笛卡尔积的方式。</p><ul><li><strong>INNER JOIN</strong>：如果表中有至少一个匹配，则返回行，关键字INNER可省。</li><li><strong>LEFT OUTER  JOIN</strong>：即使右表中没有匹配，也从左表返回所有的行。</li><li><strong>RIGHT OUTER  JOIN</strong>：即使左表中没有匹配，也从右表返回所有的行。</li><li><strong>FULL OUTER  JOIN</strong>：只要其中一个表中存在匹配，则返回行。</li></ul><p>连接条件，<strong>只允许AND</strong>连接的等值条件。只有在诸如MapReduct-based的分布式查询中可以使用不等值连接或者使用OR连接多个条件。支持通过括号指定JOIN的优先级。</p><p>NATURAL JOIN（自然连接） ，参与JOIN的两张表根据字段名字自动决定连接字段。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--表A的字段(key1, a1)，表B的字段(key1, b1)</span></span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> B;</span><br><span class="line"><span class="comment">-- 等价于</span></span><br><span class="line"><span class="keyword">SELECT</span> A.key1 <span class="keyword">as</span> key1, A.a1, B.b1 <span class="keyword">FROM</span> A <span class="keyword">INNER</span> <span class="keyword">JOIN</span> B <span class="keyword">ON</span> A.key1 = B.key1;</span><br></pre></td></tr></table></figure><p>LEFT SEMI JOIN（半连接），右表只用来过滤左表的数据而不出现在结果集中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">LEFT</span> SEMI <span class="keyword">JOIN</span> B <span class="keyword">ON</span> A.id=B.id;</span><br><span class="line"><span class="comment">-- 只要A中某行的id在B的所有id中出现过，此行就保留在结果集中</span></span><br></pre></td></tr></table></figure><p>LEFT ANTI JOIN（反连接），当JOIN条件不成立时返回左表中的数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">LEFT</span> ANTI <span class="keyword">JOIN</span> B <span class="keyword">ON</span> A.id=B.id;</span><br><span class="line"><span class="comment">-- 只要A中某行的id在B的所有id中都没有出现过，此行就保留在结果集中</span></span><br></pre></td></tr></table></figure><h2><span id="having">HAVING</span></h2><p>聚合函数无法和<code>WHERE</code>合用，但是可以和<code>HAVING</code>合用来做条件过滤。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a, <span class="keyword">SUM</span>(b) <span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">SUM</span>(b)&lt;<span class="number">2000</span>;</span><br></pre></td></tr></table></figure><h2><span id="子查询subquery">子查询SUBQUERY</span></h2><p>普通的select是从几张表中读数据，但查询的对象也可以是另外一个<code>SELECT</code>操作。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A.a, A.b <span class="keyword">FROM</span> (<span class="keyword">SELECT</span> * <span class="keyword">FROM</span> AA) A <span class="keyword">JOIN</span> B <span class="keyword">ON</span> A.a=B.a;</span><br></pre></td></tr></table></figure><p>因为<code>SELECT</code>的返回值实际上是一个集合，所以也可以联合<code>IN</code>操作来进行数据判断。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> A <span class="keyword">WHERE</span> a <span class="keyword">NOT</span> <span class="keyword">IN</span> (<span class="keyword">SELECT</span> a <span class="keyword">FROM</span> B);</span><br></pre></td></tr></table></figure><p><code>NOT IN SUBQUERY</code>类似于<code>LEFT ANTI JOIN</code>，但是不同的一点是，如果B中有任何为<code>NULL</code>的列，则 <code>NOT IN</code>表达式会为<code>NULL</code>，导致<code>WHERE</code>条件不成立，无数据返回，此时与<code>LEFT ANTI JOIN</code>不同。</p><p>当SUBQUERY的输出结果为单行单列的时候（例如SUBQUERY的SELECT是COUNT函数），可以当做标量来使用。以<code>SELECT COUNT()</code>为例，输出结果是一个row set，但可以判断这条语句的输出有且仅有一行一列，因此可以将其当做标量使用。例如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELCT * FROM A WHERE (SELECT COUNT(*) FROM B WHERE A.a = B.a) &gt; 1;</span><br><span class="line"><span class="comment">-- 这里的COUNT的结果直接作为标量与1进行计算</span></span><br></pre></td></tr></table></figure><p>注意，能当成标量来使用的SUBQUERY必须是<strong>在编译阶段就能够确认返回结果只有一行一列</strong>的查询，如果一条语句，即使能够确定在实际运行过程中只会产生一行数据，但是编译过程中确定不了，编译器也是会报错。也就是说并不是根据返回数据的条数判断，而是根据函数本身的返回值判断的。该使用方式只能发生在<code>WHERE</code>中。</p><h2><span id="窗口函数">窗口函数</span></h2><p>可以使用窗口函数进行灵活的分析处理工作，窗口函数只能出现在select子句中，窗口函数中不能嵌套使用窗口函数和聚合函数，且不可以和同级别的聚合函数一起使用。窗口函数的作用是<strong>开某扇窗，从某个角度看数据</strong>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">window_func() OVER (PARTITION BY a ORDER BY b [windowing_clause]) as name</span><br><span class="line"><span class="comment">-- partition by部分用来指定开窗的列。分区列的值相同的行被视为在同一个窗口内。</span></span><br><span class="line"><span class="comment">-- order by用来指定数据在一个窗口内如何排序。</span></span><br><span class="line"><span class="comment">-- windowing_clause部分可以用rows指定开窗方式</span></span><br></pre></td></tr></table></figure><ul><li><code>COUNT</code>，计数。</li><li><code>AVG</code>，取均值。</li><li><code>MAX</code>，取最大值。</li><li><code>MIN</code>，取最小值。</li><li><code>MEDIAN</code>，取中位数。</li><li><code>STDDEV</code>，计算总体标准差。</li><li><code>STDDEV_SAMP</code>，计算样本标准差。</li><li><code>SUM</code>，求和。</li><li><code>DENSE_RANK</code>，计算连续排名。</li><li><code>RANK</code>，计算排名。</li><li><code>LAG</code>，按偏移量取当前行之前第几行的值，如当前行号为rn，则取行号为rn-offset的值。</li><li><code>LEAD</code>，按偏移量取当前行之后第几行的值，如当前行号为rn则取行号为rn+offset的值。</li><li><code>PERCENT_RANK</code>，计算一组数据中某行的相对排名。</li><li><code>ROW_NUMBER</code>，用于计算行号，从 1 开始。</li><li><code>CLUSTER_SAMPLE</code>，分组抽样。</li><li><code>NTILE</code>，将分组数据按照顺序切分成n片，并返回当前切片值，如果切片不均匀，默认增加第一个切片的分布。</li><li><code>cume_dist</code>，用于取得累计分布，相当于求分组中值小于等于当前值的行数占分组总行数的比例。</li></ul><h2><span id="其他">其他</span></h2><h3><span id="lateral-view">LATERAL VIEW</span></h3><p>分布式SQL独有的，通常和<code>SPLIT</code>、 <code>EXPLODE</code>等UDTF（User-Defined Table-Generating Functions）一起使用。它能够<strong>将一行数据拆成多行数据</strong>，在此基础上可以对拆分后的数据进行聚合。</p><ul><li><p><code>LATERAL VIEW</code>首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，<code>LATERAL VIEW</code>再把结果聚合，产生一个支持别名表的虚拟表。</p></li><li><p>相当于该函数的作用对象是一个Table，然后用<code>explode</code>函数来作用到具体的某一个list上，输出是一个新的row set。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, b <span class="keyword">FROM</span> A LATERAL <span class="keyword">VIEW</span> explode(A_list) T <span class="keyword">AS</span> b;</span><br><span class="line"><span class="comment">-- 假设原来的每行是(id, A_list)，(1, [1,2,3])</span></span><br><span class="line"><span class="comment">-- 打散之后的输出是(1,1),(1,2),(1,3)</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, c <span class="keyword">FROM</span> A LATERAL <span class="keyword">VIEW</span> explode(<span class="keyword">SPLIT</span>(A_string, <span class="string">' '</span>)) T <span class="keyword">AS</span> c;</span><br><span class="line"><span class="comment">-- 假设原来的每行是(id, A_string)，(1, 'a b c')</span></span><br><span class="line"><span class="comment">-- 打散之后的输出是(1,a),(1,b),(1,c)</span></span><br><span class="line"><span class="keyword">SELECT</span> b, c <span class="keyword">FROM</span> A LATERAL <span class="keyword">VIEW</span> explode(A_list) T1 <span class="keyword">AS</span> b</span><br><span class="line">LATERAL <span class="keyword">VIEW</span> explode(A_string) T2 <span class="keyword">AS</span> c;</span><br><span class="line"><span class="comment">-- 也可以同时多条LATERAL VIEW同时打散，产生更多组合</span></span><br><span class="line"><span class="comment">-- 假设原来的每行是(A_list, A_string)，([1,2], 'a b')</span></span><br><span class="line"><span class="comment">-- 打散之后的输出是(1,a),(1,b),(2,a),(2,b)</span></span><br></pre></td></tr></table></figure></li></ul><h3><span id="grouping-sets">GROUPING SETS</span></h3><p>如果同时要对好几列分别做聚合，传统的方法需要写很多UNION ALL语句，但是<code>GROUPING SETS</code>只需要一句。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a,b,c,<span class="keyword">COUNT</span>(*) <span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a,b,c <span class="keyword">GROUPING</span> <span class="keyword">SETS</span> ((a,b), (c), ());</span><br><span class="line"><span class="comment">-- 相当于逐层UNION</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> A</span><br><span class="line"><span class="keyword">UNION</span> ALL</span><br><span class="line"><span class="keyword">SELECT</span> a, b, <span class="literal">NULL</span>, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> a, b</span><br><span class="line"><span class="keyword">UNION</span> ALL</span><br><span class="line"><span class="keyword">SELECT</span> <span class="literal">NULL</span>, <span class="literal">NULL</span>, c, <span class="keyword">COUNT</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> A <span class="keyword">GROUP</span> <span class="keyword">BY</span> c;</span><br><span class="line"><span class="comment">-- 对于不使用的变量，使用NULL充当占位符，这样可以使得结果集做UNION操作</span></span><br></pre></td></tr></table></figure><p>为了区分占位符NULL和真正的NULL，<code>GROUPING(一个列名)</code>和<code>GROUPING_ID(一个或多个列名)</code>函数就出现了。<code>GROUPING(a)</code>如果列a的NULL是占位符则返回1，否则返回0。<code>GROUPING_ID</code>的作用则是将参数列的<code>GROUPING</code>结果按照BitMap的方式组成整数。</p><h3><span id="cube-amp-rollup">CUBE &amp; ROLLUP</span></h3><p><code>CUBE</code>和<code>ROLLUP</code>可以认为是特殊的<code>GROUPING SETS</code>。</p><p><code>CUBE</code>会枚举指定列的所有可能组合作为<code>GROUPING SETS</code>，而<code>ROLLUP</code>会以按层级聚合的方式产生<code>GROUPING SETS</code>。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">GROUP BY CUBE(a, b, c) 等价于 GROUPING SETS((a,b,c),(a,b),(a,c),(b,c),(a),(b),(c),())</span><br><span class="line">GROUP BY ROLLUP(a, b, c) 等价于 GROUPING SETS((a,b,c),(a,b),(a), ())</span><br><span class="line">GROUP BY CUBE ( (a, b), (c, d) ) 等价于 GROUPING SETS (</span><br><span class="line">    ( a, b, c, d ),</span><br><span class="line">    ( a, b       ),</span><br><span class="line">    (       c, d ),</span><br><span class="line">    (            )</span><br><span class="line">)</span><br><span class="line">GROUP BY ROLLUP ( a, (b, c), d ) 等价于GROUPING SETS (</span><br><span class="line">    ( a, b, c, d ),</span><br><span class="line">    ( a, b, c    ),</span><br><span class="line">    ( a          ),</span><br><span class="line">    (            )</span><br><span class="line">)</span><br><span class="line">GROUP BY a, CUBE (b, c), GROUPING SETS ((d), (e)) 等价于 GROUP BY GROUPING SETS (</span><br><span class="line">    (a, b, c, d), (a, b, c, e),</span><br><span class="line">    (a, b, d),    (a, b, e),</span><br><span class="line">    (a, c, d),    (a, c, e),</span><br><span class="line">    (a, d),       (a, e)</span><br><span class="line">)</span><br><span class="line">GROUP BY grouping sets((b), (c),rollup(a,b,c)) 等价于 GROUP BY GROUPING SETS ( </span><br><span class="line">    (b), (c), </span><br><span class="line">    (a,b,c), (a,b), (a), ()</span><br><span class="line"> )</span><br></pre></td></tr></table></figure><h3><span id="执行计划查询explain">执行计划查询（EXPLAIN）</span></h3><p>用来显示对应于DML语句的最终执行计划结构的描述。所谓执行计划就是最终用来执行SQL语义的程序。</p><h3><span id="common-table-expressioncte">Common Table Expression（CTE）</span></h3><p>提高SQL语句的可读性与执行效率。</p><h3><span id="视图view">视图View</span></h3><p>略。</p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2021/01/04/关于SQL的一些非SELECT知识点" target="_blank" rel="noopener">https://vodkazy.cn/2021/01/04/关于SQL的一些非SELECT知识点</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;复习一波分布式SQL的基础语法，其中有一些之前都没怎么用到过。&lt;/p&gt;
    
    </summary>
    
      <category term="SQL" scheme="http://hhu1506010220.github.io/categories/SQL/"/>
    
      <category term="数据库" scheme="http://hhu1506010220.github.io/categories/SQL/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="SQL" scheme="http://hhu1506010220.github.io/tags/SQL/"/>
    
      <category term="数据库" scheme="http://hhu1506010220.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>我想去面试系列——Attention与Transformer</title>
    <link href="http://hhu1506010220.github.io/2020/10/29/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Attention%E4%B8%8ETransformer/"/>
    <id>http://hhu1506010220.github.io/2020/10/29/我想去面试系列——Attention与Transformer/</id>
    <published>2020-10-29T06:18:15.000Z</published>
    <updated>2020-11-04T02:18:55.415Z</updated>
    
    <content type="html"><![CDATA[<p>假如捕获了你的注意力，我已然成为了变形金刚…<br><a id="more"></a></p><!-- toc --><ul><li><a href="#基本原理">基本原理</a><ul><li><a href="#seq2seq中的attention">Seq2Seq中的Attention</a></li><li><a href="#self-attention">Self-Attention</a></li><li><a href="#multi-head-attention">Multi-Head-Attention</a></li><li><a href="#attention分类">Attention分类</a></li><li><a href="#transformer">Transformer</a></li></ul></li><li><a href="#代码实现">代码实现</a></li><li><a href="#细节-面试题搜集">细节 &amp; 面试题搜集</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- tocstop --><h2><span id="基本原理">基本原理</span></h2><p>Attention的基本原理就是对于输入实现有权重的加权。大体思路是：输入$X$，分别乘三个权重矩阵$W^Q,W^K,W^V$得到$Q,K,V$，然后按照$Softmax(sim(Q,K))V$得到结果。</p><h3><span id="seq2seq中的attention">Seq2Seq中的Attention</span></h3><p><img src="data:image/png;base64,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" data-original="1.webp" alt=""></p><p>Encoder部分先得到隐藏状态 $(h_1,h_2,…,h_T)$，然后在Decoder部分，比如在$t$时刻进行解码时，比如我们已经知道了前一个时间步骤decoder的隐藏状态$s_{t-1}$，我们先计算每一个encoder的位置的隐藏状态和decoder当前时间隐藏状态的关联性，$e_{tj}=a(s_{t-1},h_j),j\in[1,T]$，写成向量的形式就是$e_t$，对其进行softmax归一化就得到了对于decoder当前隐藏状态的attention分布$\alpha_{t j}=\frac{\exp \left(e_{t j}\right)}{\sum_{k=1}^{T} \exp \left(e_{t k}\right)}$。从而利用$\alpha$作为权重对encoder的隐藏状态加权求和就得到了对应的上下文向量$c_t=\sum_{j=1}^{T}\alpha_{tj}h_j$。由此就可以计算decoder的下一个隐藏状态$s_t=f(s_{t-1},y_{t-1},c_t)$以及该位置的输出 $p(y_t|y_1,…,y_{t-1},x)=g(y_{i-1},s_i,c_i)$。</p><p>这里的Encoder实际上还是用了RNN那一套老路子，只是对Decoder解码的时候进行了一些改进，所以算是比较基础的东西，为了进一步改进，克服RNN编码长距离信息能力较弱的特点，进入了self-attention自注意力机制。</p><h3><span id="self-attention">Self-Attention</span></h3><p>引入这玩意的优势就在于，1.编码可以拥有更加完善的全局信息了，而不再受距离的限制；2.可以并行化了，而不是像之前RNN一样必须前面的算完才能算后续时间步的。self-attention就是利用了注意力机制，在计算某个单词时，考虑该单词与其他所有单词之间的关联。下面我们来看一看Self-attention的结构究竟是啥样的。</p><p><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></p><p>对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入（由同一输入X乘以三个不同的权重矩阵），首先我们要计算Q与K之间的点乘，然后为了防止其结果过大，会除以一个尺度标度$\sqrt{d_k}$（这里的这个操作属于缩放点积，事实上不只有这一种计算方式），其中$d_k = hidden_size / num_heads$。再利用Softmax操作将其结果归一化为概率分布，然后再乘以矩阵V就得到权重求和的表示。该操作可以表示为</p><script type="math/tex; mode=display">Fusion\_vector (Z) = Attention(Q, K, V) = softmax(\frac{QK^\top}{\sqrt{d_k}})V</script><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a>里给了一个示意图。</p><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><p>上图的具体的含义可以解释为，假如我们要翻译一个词组Thinking Machines，分别转换成embedding之后是$x_1$、$x_2$，然后分别乘以$W^Q,W^K,W^V$得到两个词分别对应的向量以thinking为例得到$q_{thinking}、k_{thinking}、v_{thinking}$，然后需要计算Thinking这个词与句子中所有词的attention score，相当于拿$q_{thinking}$作为搜索的Query，去和句子中所有词（包含其本身）的Key做匹配，看相关性有多高。这里直接用点乘作为相似度计算函数，然后就得到了Thinking与Thinking和Machines两个单词的相似度，再做个尺度缩放和softmax归一化，最后就得到了一个注意力分布，再拿这个注意力分布就乘每个词分别对应的Value，就可以得到Thinking翻译后得到的结果变量。可以解释为，当我们在思考如何翻译Thinking的时候，主要注意力是在这个词本身上，同时还注意到了一点它周围上下文词的语境含义。</p><p>如果多个输入向量合并成矩阵的形式（矩阵运算的方式，使得 Self Attention 的计算能够并行化，这也是 Self Attention 最终的实现方式），就相当于来了一个输入矩阵$X$，然后用同一个$X$计算得到$Q、K、V$，当然这里的$Q、K、V$肯定不是一样的而是分别过三个不同的矩阵（$W^Q$、$W^K$、$W^V$，这三个矩阵是可学习参数）得到的三个不同的矩阵。所以实际上self-attention就是$Q,K,V$都是由同一个输入计算来的的attention。经过self-attention之后，输出矩阵/向量$Z$，该输出就包含了每个词和其他上下文的综合信息，其中每个单词自身占大头，上下文占小头。<strong>可以将self-attention理解为一种融合上下文信息的方式。</strong></p><h3><span id="multi-head-attention">Multi-Head-Attention</span></h3><p>多头注意力，就是把X往不同的方向投影形成很多组$Q_i,K_i,V_i$，切割成不同方面的attention（也就是不同的head），期待每一个attention学出来的东西不一样，可以捕获不同特征。然后最后再把不同head捕获到的结果$Z_i$拼起来，乘以最终的大权重矩阵$W^O$就得到了最终的融合了更多信息的特征向量。实际中，K、V 矩阵的序列长度是一样的，而 Q 矩阵的序列长度可以不一样。</p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="4.png" alt=""></p><h3><span id="attention分类">Attention分类</span></h3><p><strong>Soft or Hard Attention</strong>：</p><ul><li>Soft attention是指传统的attention，是可以被嵌入到模型中去训练并且传播梯度的；</li><li>Hard attention不计算所有输出，依据概率对encoder的输出采样，在反向传播时需采用蒙特卡洛进行梯度估计。</li></ul><p><strong>Global or Local Attention</strong>：</p><ul><li>Global attention是传统的attention，对所有encoder输出进行计算</li><li>Local attention是预测特定位置并选取一个窗口进行attention权重加权计算，介于soft和hard之间。</li></ul><h3><span id="transformer">Transformer</span></h3><p>attention的东西实际还是很好理解的，那么当attention引入特征提取器之后，就形成了现在盛行的特征提取器大杀器——Transformer。</p><p><img src="data:image/png;base64,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" data-original="5.png" alt=""></p><p>对于encoder来说，就是直接利用最朴素的multi-head-attention，更准确的说因为输入只有一个所以应该是multi-head-self-attention，每一层(共N层)encoder的key, query, value均来自前一层encoder的输出，即encoder的每个位置都可以注意到之前一层encoder的所有位置。Encoder部分的计算归纳如下：</p><script type="math/tex; mode=display">X = Positional\_embedding(inputs) + Token\_embedding(inputs) \tag{Encoder.1}</script><script type="math/tex; mode=display">Q_i = X W_i^Q;K_i=XW_i^K;V_i=XW_i^V \tag{Encoder.2}</script><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V \tag{Encoder.3}</script><script type="math/tex; mode=display">head_i = Attention(Q_i, K_i, V_i) \tag{Encoder.4}</script><script type="math/tex; mode=display">MultiHead(X)=concat(head_1,...,head_h)W^O \tag{Encoder.5}</script><script type="math/tex; mode=display">Y = LayerNorm(X+MultiHead(X)) \tag{Encoder.6}</script><script type="math/tex; mode=display">Z_{encoder} = LayerNorm(Y + FFN(Y)) \tag{Encoder.7}</script><p>对于decoder来讲，我们注意到有两个与encoder不同的地方，一个是第一级的<strong>Masked Multi-head</strong>，另一个是第二级的Multi-Head Attention不仅接受来自前一级的输出，还要接收encoder的输出。Decoder的输入是Output（Target右移一位），也就是相当于一开始的输入是<code>[CLS]</code>，然后结合输入句子的第一个词的Attention，mask掉除了<code>[CLS]</code>之后词，输出了第一个翻译过来的词，依次类推。下面分别解释一下是什么原理。</p><p>第一级decoder的key, query, value均来自前一层decoder的输出，但加入了Mask操作，即我们只能attend到前面已经翻译过的输出的词语，因为翻译过程我们当前还并不知道下一个输出词语，这是我们之后才会推测到的。（这里的Mask是指在预测第t个词的时候要把t+1到末尾的词遮住，只对前面t个词做self attention，并且<strong>mask只对于train的时候做，在predict阶段不用mask</strong>）。</p><p>而第二级decoder也被称作encoder-decoder attention layer，即它的<strong>query来自于之前一级的decoder层的输出</strong>，但其<strong>key和value来自于encoder的输出</strong>，这使得decoder的每一个位置都可以attend到输入序列的每一个位置。总结一下，$K$和$V$的来源总是相同的，$Q$在encoder及第一级decoder中与$K$、$V$来源相同，在encoder-decoder attention layer中与$K$、$V$来源不同。</p><p>Decoder部分的计算归纳如下，输出P代表着当前位置下输出词的概率分布：</p><script type="math/tex; mode=display">X_{input\_layer1} = Positional\_embedding(outputs) + Token\_embedding(outputs) \tag{Decoder.1}</script><script type="math/tex; mode=display">Y_{output\_layer1} = LayerNorm(X_1+MaskedMultiHead(X_{input\_layer1})) \tag{Decoder.2}</script><script type="math/tex; mode=display">Q_{layer2,i} = Y_{output\_layer1} W_i^Q;K_{layer2,i}=Z_{encoder}W_i^K;V_{layer2,i}=Z_{encoder}W_i^V; \tag{Decoder.3}</script><script type="math/tex; mode=display">head_i = Attention(Q_{layer2,i},K_{layer2,i},V_{layer2,i}) \tag{Decoder.4}</script><script type="math/tex; mode=display">MultiHead(Z_{encoder},Y_{output\_layer1})=concat(head_1,...,head_h)W^O \tag{Decoder.5}</script><script type="math/tex; mode=display">Y_{output\_layer2} = LayerNorm(Y_{output\_layer1} + MultiHead(Z_{encoder},Y_{output\_layer1})) \tag{Decoder.6}</script><script type="math/tex; mode=display">Z_{decoder} = LayerNorm(Y_{output\_layer2} + MultiHead(Z_{encoder}, Y_{output\_layer1})) \tag{Decoder.7}</script><script type="math/tex; mode=display">P=Softmax(Linear(Z_{decoder})) \tag{Decoder.8}</script><p>还有一个要注意的是，原生transformer里的positional embedding是通过sin、cos直接计算的固定值，具体公式为$PositionalEmbedding(pos,2i)=sin(pos/10000^{2i/d_{model}});$$PositionalEmbedding(pos,2i+1)=cos(pos/10000^{2i/d_{model}})$。而该embedding的含义很明显就是加入单词在原句中的位置信息。残差Add的作用是为了解决多层神经网络训练困难的问题，通过将前一层的信息无差的传递到下一层，可以有效的仅关注差异部分，$Output(x) = F(x)+x$。正则LayerNorm的作用是通过对层的激活值的归一化，可以加速模型的训练过程，使其更快的收敛。<strong>这里要特别注意一下，编码可以并行计算，一次性全部encoding出来；但解码不能并行，因为需要像rnn一样一个一个顺序解码出来。</strong></p><h2><span id="代码实现">代码实现</span></h2><p>Pytorch有封装好的多头注意力，<code>torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)</code>，调用时<code>foward</code>函数为<code>forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)</code>。</p><p>手写的话，代码如下，multihead的实现竟然只是用了一个权重矩阵，维度是$d_{hidden_size} \times d_{hidden_size}$，并且要求$d_{hidden_size}=d_{k} \times d_{num_heads}$，而不是多个权重矩阵获得多个$QKV$，竟然只是把一个300维的向量拆成6个50维的向量…（我上边说的那种用多个权重矩阵的应该属于另外一种实现方式，我写的那个方法的话$W_i$的维度应该是$d_{hidden_size} \times d_{k}$，然后搞$d_{num_heads}$个$W_i$）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiheadAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># n_heads：多头注意力的数量</span></span><br><span class="line">    <span class="comment"># hid_dim：每个词输出的向量维度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hid_dim, n_heads, dropout)</span>:</span></span><br><span class="line">        super(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 强制 hid_dim 必须整除 h</span></span><br><span class="line">        <span class="keyword">assert</span> hid_dim % n_heads == <span class="number">0</span></span><br><span class="line">        <span class="comment"># 定义 W_q 矩阵</span></span><br><span class="line">        self.w_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_k 矩阵</span></span><br><span class="line">        self.w_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_v 矩阵</span></span><br><span class="line">        self.w_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 缩放</span></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="comment"># K: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># V: [64,10,300], batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300], batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        bsz = query.shape[<span class="number">0</span>]</span><br><span class="line">        Q = self.w_q(query)</span><br><span class="line">        K = self.w_k(key)</span><br><span class="line">        V = self.w_v(value)</span><br><span class="line">        <span class="comment"># 这里把 K Q V 矩阵拆分为多组注意力，变成了一个 4 维的矩阵</span></span><br><span class="line">        <span class="comment"># 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50</span></span><br><span class="line">        <span class="comment"># 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span></span><br><span class="line">        <span class="comment"># K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span></span><br><span class="line">        Q = Q.view(bsz, <span class="number">-1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        K = K.view(bsz, <span class="number">-1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        V = V.view(bsz, <span class="number">-1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 1 步：Q 乘以 K的转置，除以scale</span></span><br><span class="line">        <span class="comment"># [64,6,12,50] * [64,6,50,10] = [64,6,12,10]</span></span><br><span class="line">        <span class="comment"># attention：[64,6,12,10]</span></span><br><span class="line">        attention = torch.matmul(Q, K.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)) / self.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            attention = attention.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span></span><br><span class="line">        <span class="comment"># 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span></span><br><span class="line">        <span class="comment"># attention: [64,6,12,10]</span></span><br><span class="line">        attention = self.do(torch.softmax(attention, dim=<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三步，attention结果与V相乘，得到多头注意力的结果</span></span><br><span class="line">        <span class="comment"># [64,6,12,10] * [64,6,10,50] = [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50]</span></span><br><span class="line">        x = torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为 query 有 12 个词，所以把 12 放到前面，把 5 和 60 放到后面，方便下面拼接多组的结果</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># 这里的矩阵转换就是：把多组注意力的结果拼接起来</span></span><br><span class="line">        <span class="comment"># 最终结果就是 [64,12,300]</span></span><br><span class="line">        <span class="comment"># x: [64,12,6,50] -&gt; [64,12,300]</span></span><br><span class="line">        x = x.view(bsz, <span class="number">-1</span>, self.n_heads * (self.hid_dim // self.n_heads))</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">query = torch.rand(<span class="number">64</span>, <span class="number">12</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span></span><br><span class="line">key = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span></span><br><span class="line">value = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line">attention = MultiheadAttention(hid_dim=<span class="number">300</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">output = attention(query, key, value)</span><br><span class="line"><span class="comment">## output: torch.Size([64, 12, 300])</span></span><br><span class="line">print(output.shape)</span><br></pre></td></tr></table></figure><h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><p>后续更新…</p><h2><span id="参考文献">参考文献</span></h2><ol><li>Attention机制详解（一）——Seq2Seq中的Attention：<a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47063917</a></li><li>Attention机制详解（二）——Self-Attention与Transformer：<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li><li>【NLP】Transformer模型原理详解：<a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44121378</a></li><li>NLP中的Attention原理和源码解析：<a href="https://zhuanlan.zhihu.com/p/43493999" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43493999</a></li><li>jalammar.github.io：<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">https://jalammar.github.io/illustrated-transformer/</a></li><li>图解Transformer（完整版）：<a href="https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&amp;mid=2247539981&amp;idx=7&amp;sn=07b00654c6f38d743a91b39d33d68d6a&amp;chksm=fc86bc1ecbf135087d885408704ff90c95562c00108bcad7ded93fb468edd6534010e709299f&amp;scene=0&amp;xtrack=1#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU2OTA0NzE2NA==&amp;mid=2247539981&amp;idx=7&amp;sn=07b00654c6f38d743a91b39d33d68d6a&amp;chksm=fc86bc1ecbf135087d885408704ff90c95562c00108bcad7ded93fb468edd6534010e709299f&amp;scene=0&amp;xtrack=1#rd</a></li><li>NLPer看过来，一些关于Transformer的问题整理：<a href="https://www.nowcoder.com/discuss/258321?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=1009&amp;source_id=search_post" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/258321?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=1009&amp;source_id=search_post</a></li><li>一文看懂 Attention（本质原理+3大优点+5大类型）：<a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#x6f;&#x3a;&#x68;&#116;&#116;&#112;&#x73;&#x3a;&#47;&#47;&#109;&#101;&#100;&#105;&#117;&#109;&#46;&#x63;&#x6f;&#x6d;&#47;&#x40;&#112;&#x6b;&#x71;&#x69;&#97;&#110;&#x67;&#52;&#57;&#x2f;&#37;&#x45;&#52;&#37;&#66;&#x38;&#37;&#x38;&#48;&#x25;&#69;&#54;&#37;&#57;&#54;&#37;&#56;&#x37;&#37;&#69;&#x37;&#37;&#57;&#67;&#37;&#56;&#66;&#37;&#x45;&#x36;&#x25;&#x38;&#x37;&#x25;&#x38;&#50;&#45;&#x61;&#116;&#116;&#101;&#110;&#116;&#x69;&#111;&#110;&#x2d;&#x25;&#69;&#54;&#37;&#57;&#67;&#37;&#x41;&#67;&#37;&#69;&#x38;&#37;&#x42;&#x34;&#37;&#65;&#x38;&#x25;&#69;&#x35;&#x25;&#56;&#69;&#x25;&#x39;&#70;&#37;&#x45;&#55;&#x25;&#x39;&#48;&#37;&#56;&#54;&#45;&#51;&#37;&#x45;&#x35;&#37;&#65;&#x34;&#x25;&#x41;&#x37;&#37;&#x45;&#52;&#x25;&#x42;&#x43;&#37;&#57;&#56;&#x25;&#x45;&#x37;&#37;&#x38;&#x32;&#37;&#66;&#x39;&#45;&#x35;&#37;&#69;&#53;&#x25;&#x41;&#52;&#37;&#65;&#55;&#37;&#69;&#x37;&#x25;&#66;&#49;&#37;&#66;&#x42;&#x25;&#69;&#53;&#37;&#x39;&#69;&#37;&#x38;&#x42;&#x2d;&#101;&#52;&#102;&#98;&#101;&#x34;&#98;&#x36;&#100;&#x30;&#x33;&#x30;">&#x68;&#116;&#116;&#112;&#x73;&#x3a;&#47;&#47;&#109;&#101;&#100;&#105;&#117;&#109;&#46;&#x63;&#x6f;&#x6d;&#47;&#x40;&#112;&#x6b;&#x71;&#x69;&#97;&#110;&#x67;&#52;&#57;&#x2f;&#37;&#x45;&#52;&#37;&#66;&#x38;&#37;&#x38;&#48;&#x25;&#69;&#54;&#37;&#57;&#54;&#37;&#56;&#x37;&#37;&#69;&#x37;&#37;&#57;&#67;&#37;&#56;&#66;&#37;&#x45;&#x36;&#x25;&#x38;&#x37;&#x25;&#x38;&#50;&#45;&#x61;&#116;&#116;&#101;&#110;&#116;&#x69;&#111;&#110;&#x2d;&#x25;&#69;&#54;&#37;&#57;&#67;&#37;&#x41;&#67;&#37;&#69;&#x38;&#37;&#x42;&#x34;&#37;&#65;&#x38;&#x25;&#69;&#x35;&#x25;&#56;&#69;&#x25;&#x39;&#70;&#37;&#x45;&#55;&#x25;&#x39;&#48;&#37;&#56;&#54;&#45;&#51;&#37;&#x45;&#x35;&#37;&#65;&#x34;&#x25;&#x41;&#x37;&#37;&#x45;&#52;&#x25;&#x42;&#x43;&#37;&#57;&#56;&#x25;&#x45;&#x37;&#37;&#x38;&#x32;&#37;&#66;&#x39;&#45;&#x35;&#37;&#69;&#53;&#x25;&#x41;&#52;&#37;&#65;&#55;&#37;&#69;&#x37;&#x25;&#66;&#49;&#37;&#66;&#x42;&#x25;&#69;&#53;&#37;&#x39;&#69;&#37;&#x38;&#x42;&#x2d;&#101;&#52;&#102;&#98;&#101;&#x34;&#98;&#x36;&#100;&#x30;&#x33;&#x30;</a></li><li>碎碎念：Transformer的解码加速：<a href="https://zhuanlan.zhihu.com/p/75796168" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/75796168</a></li><li>Transformer的矩阵维度分析和Mask详解：<a href="https://blog.csdn.net/qq_35169059/article/details/101678207" target="_blank" rel="noopener">https://blog.csdn.net/qq_35169059/article/details/101678207</a></li><li>Transformer源码剖析：<a href="https://zhuanlan.zhihu.com/p/149766082" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/149766082</a></li><li>transformer详解：transformer/ universal transformer/ transformer-XL：<a href="https://akeeper.space/blog/701.html" target="_blank" rel="noopener">https://akeeper.space/blog/701.html</a></li><li>Attention：<a href="https://looperxx.github.io/Attention/" target="_blank" rel="noopener">https://looperxx.github.io/Attention/</a></li></ol><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2020/10/29/我想去面试系列——Attention与Transformer" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/29/我想去面试系列——Attention与Transformer</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;假如捕获了你的注意力，我已然成为了变形金刚…&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/NLP/"/>
    
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/tags/NLP/"/>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/tags/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>我想去面试系列——Word2vec</title>
    <link href="http://hhu1506010220.github.io/2020/10/19/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Word2vec/"/>
    <id>http://hhu1506010220.github.io/2020/10/19/我想去面试系列——Word2vec/</id>
    <published>2020-10-19T01:59:04.000Z</published>
    <updated>2020-10-29T13:31:40.159Z</updated>
    
    <content type="html"><![CDATA[<p>搜罗万象，找寻word2vecv背后的奥妙…<br><a id="more"></a></p><!-- toc --><ul><li><a href="#基本原理">基本原理</a><ul><li><a href="#n-gram">N-gram</a></li><li><a href="#word2vec">Word2vec</a><ul><li><a href="#cbow">CBOW</a></li><li><a href="#skip-gram">Skip-gram</a></li><li><a href="#hierarchical-softmax">Hierarchical Softmax</a></li><li><a href="#negative-sampling">Negative Sampling</a></li></ul></li><li><a href="#glove">Glove</a></li></ul></li><li><a href="#细节-面试题搜集">细节 &amp; 面试题搜集</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- tocstop --><h2><span id="基本原理">基本原理</span></h2><h3><span id="n-gram">N-gram</span></h3><p>在NLP领域，如何计算一段文本序列在某种语言下出现的概率？对于一段文本序列$S=w_1, w_2, … , w_T$，统计语言模型将序列的联合概率转化为一系列条件概率的乘积：</p><script type="math/tex; mode=display">P(S)=P(w_1, w_2, ..., w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, ..., w_{t-1})</script><p>由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么用。我们更多的是采用其简化版本 —— N-gram模型，常见的如bi-gram模型（N=2）和tri-gram模型（N=3）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑N&gt;3的模型。我们可以用最大似然法去求解N-gram模型的参数——等价于去统计每个N-gram的条件词频：</p><script type="math/tex; mode=display">p(w_t|w_1, w_2, ..., w_{t-1}) \approx p(w_t|w_{t-n+1}, ..., w_{t-1})</script><p><strong>N-gram模型的缺点</strong>是，①无法处理更长的context（N＞3）②没有考虑词与词之间内在的联系性。Ngram本质上是将词当做一个个孤立的原子单元，然后形式化表达为一个个one-hot向量。显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（The Curse of Dimensionality）。于是连续的分布式向量表示（Distributed representation）就产生了。Distributed representation可以解决One-Hot编码存在的问题，它的思路是通过训练，<strong>将原来One-Hot编码的每个词都映射到一个较短的词向量</strong>上来，而这个较短的词向量的维度可以由我们自己在训练时根据任务需要来自己指定。</p><h3><span id="word2vec">Word2vec</span></h3><p>首先要说明的一点是，<a href="https://arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="noopener">word2vec</a>并不是一个深度模型，其背后只是一个浅层神经网络，并且是一个无监督学习模型。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。</p><p>Word2Vec 的训练模型本质上是只具有一个隐含层的神经元网络。</p><p><img src="data:image/png;base64,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" data-original="w2c.jpg" alt=""></p><p>它的输入是采用One-Hot编码的词汇表向量，它的输出也是One-Hot编码的词汇表向量。使用所有的样本，训练这个神经元网络，等到收敛之后，从输入层到隐含层的那些权重，便是每一个词的采用Distributed Representation的词向量。这样我们就把原本维数为V的词向量变成了维数为N的词向量（N远小于V），并且词向量间保留了一定的相关关系。Word2Vec其实就是指的是网络中的权重矩阵<code>W</code>，因为输入的每个onehot乘以W只会有一列起作用。</p><p>Word2Vec的论文中提出了CBOW和Skip-gram两种模型，CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。下面分别进行介绍：</p><h4><span id="cbow">CBOW</span></h4><p>从数学上看，CBoW（Continuous Bag-of-Words）模型等价于一个词袋模型的向量乘以一个Embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。</p><p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p><ul><li>输入层：c个上下文单词向量$c_i \in R^V$，每个向量都是长度为<code>V</code>（词表大小）的one-hot向量，这里的<code>c</code>由<code>skip_window</code>参数决定，<code>c=2*skip_window</code>，即在左右两侧分别选<code>skip_window</code>个上下文单词；边界条件：当中心词位于边缘时，窗口大小减小（word2vec没有做padding处理）</li><li>隐藏层：所有onehot分别乘以共享的输入权重矩阵 $W_{V \times N}$，得到维数为N的向量$w_1,w_2,…,w_c$，这里的$N$自行定义为多少。将这些向量$w_1,w_2,…,w_c$加权求平均作为隐藏层向量$h$；</li><li>输出层：$h$再乘一个权重矩阵$W’_{N \times V}$，再过一个激活函数（直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax和负采样），得到$y \in R^V$，该向量的每一维代表了相对应的单词的概率分布。</li><li>$y$中概率最大的元素所指示的单词为预测出的中间词（target word），与true label的onehot词向量做比较，误差越小越好。损失函数一般为交叉熵代价函数。训练完成后矩阵$W_{V \times N}$就是所需要的word embedding。</li></ul><h4><span id="skip-gram">Skip-gram</span></h4><p>Skip-gram是和CBOW反过来，从直观上理解，Skip-Gram是给定input word来预测上下文。它的做法是，将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词。假如我们有一个句子“The dog barked at the mailman”。首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；有了input word以后，我们再定义一个叫做<code>skip_window</code>的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。</p><p><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></p><p>如果我们设置<code>skip_window=2</code>，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。<code>skip_window=2</code>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小<code>span=2x2=4</code>。另一个参数叫<code>num_skips</code>，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当<code>skip_window=2，num_skips=</code>2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。模型训练的时候就相当于有两个样本数据，每次前向传播softmax求的是当前窗口内的这个词和当前输入词在同一窗口的概率。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。对于上边的样例，“The dog barked at the mailman”，假设输入词是”barked”，上下文词有[the, dog, at, the]，那么barked这一个batch中就有四个训练样例，[the,barked], [dog,barked], [barked,at], [barked,the]，然后相当于一个barked要训练四次。但是在预测的时候，只会输入一个barked，然后最后对单词表softmax，得到可能作为barked上下文出现的词的概率分布，最终的topK个单词即为最后的上下文中窗口中预测出现的词。（这就出现了一个问题，比如上下文中有两个the，但是只能预测出来一个，所以这就凸显出来了词袋模型的弊端，忽略次数）</p><p>需要注意一点的是，实际上skip-gram的预测部分是没人用的，也就是说word2vec它这个模型的初衷就是为了训练而生的，而不是为了下游任务而生的。所以这也验证了之前经常看过的一句话“<strong>word2vec词向量其实是模型的一个副产品而已</strong>”，所以根本就不用纠结怎么拿word2vec进行预测，只需要掌握它训练的精髓就好了。</p><p><img src="data:image/png;base64,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" data-original="skip.jpeg" alt=""></p><h4><span id="hierarchical-softmax">Hierarchical Softmax</span></h4><p>Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。下边两个技巧其实并不是word2vec的精髓，只需要简单了解下即可。</p><p>Hierarchical Softmax是为了降低原始模型中softmax对于大量词表计算时的计算复杂度，<strong>本质是把 N 分类问题变成 log(N)次二分类</strong>。它对原模型的改进主要有两点，第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是直接对所有输入的词向量求和。假设输入的词向量为<code>（0,1,0,0）</code>和<code>（0,0,0,1）</code>，那么隐藏层的向量为<code>（0,1,0,1）</code>。第二点改进是采用哈夫曼树（根据频率建模，出现频率高的越靠近根节点）来替换了原先的从隐藏层到输出层的矩阵<code>W&#39;</code>。哈夫曼树的叶节点个数为词汇表的单词个数<code>V</code>，一个叶节点代表一个单词，而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。这棵哈夫曼树除了根结点以外的所有非叶节点中都含有一个由参数<code>θ</code>确定的<code>sigmoid</code>函数，不同节点中的<code>θ</code>不一样。训练时隐藏层的向量与这个<code>sigmoid</code>函数进行运算，根据结果进行分类，若分类为负类则沿左子树向下传递，编码为0；若分类为正类则沿右子树向下传递，编码为1。</p><h4><span id="negative-sampling">Negative Sampling</span></h4><p>对于一些不常见、较生僻的词汇，哈夫曼树在计算它们的词向量时仍然需要做大量的运算。负采样是另一种用来提高Word2Vec效率的方法，它是基于这样的观察：训练一个神经网络意味着使用一个训练样本就要稍微调整一下神经网络中所有的权重，这样才能够确保预测训练样本更加精确，如果能设计一种方法每次只更新一部分权重，那么计算复杂度将大大降低。具体方法就是选取一些期望神经网络输出为0的神经元对应的单词进行训练，这样的话就可以通过较少数目的负样本来更新大部分0对应的权重。<strong>本质是预测总体类别的一个子集</strong>。</p><h3><span id="glove">Glove</span></h3><p><a href="https://www.aclweb.org/anthology/D14-1162.pdf" target="_blank" rel="noopener">Glove</a>全称Global Vectors for Word Representation，是基于全局词频统计的词表征工具。他结合了LSA算法可以有效收集语料库全局统计信息的优点（但是无法捕捉上下文信息），还结合了word2vec这种滑动窗口机制的可以通过局部上下文特征表达更丰富语义的特点。它的<strong>核心思想</strong>是，对于任意的词$i$和$j$，假如有第三个词$k$，如果词$k$与$i$相比于词$k$与$j$有更深的关联，那么我们可以得到$k$在$i$的窗口词中出现的概率要大于$k$在$j$的窗口词中出现的概率，且数值较大。如果$k$和$i$与$j$的关系都不大，那么前述的概率应该是约等于的。Glove模型表示的语义词向量相似度尽可能接近在统计共现矩阵中统计相似度，并且不同共现的词有不同权值。可形式化为$F(w_i,w_j,\tilde{w}_k) = \frac{P_{ik}}{P_{jk}}$，式子左边是向量相似度函数，右边是全局的共现统计值。具体实现分为三步，可参考<a href="https://zhuanlan.zhihu.com/p/80335195" target="_blank" rel="noopener">这篇文章</a>：</p><ul><li><p>根据语料库（corpus）构建一个共现矩阵$X$，<strong>矩阵中的每一个元素$X_{ij}$代表单词$i$和上下文单词$j$在特定大小的上下文窗口内共同出现的次数。</strong>一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离$d$，提出了一个衰减函数：$decay=1/d$用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小</strong>。</p></li><li><p>构建词向量（Word Vector）和共现矩阵$X$之间的近似关系，损失函数描述如下：</p><script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><p>其中，$w_{i}^{T}$和$\tilde{w_{j}}$是我们最终要求解的词向量，$b_i$和$\tilde{b_j}$分别是两个词向量的bias term，$f(X_{ij})$的作用主要是描述相似度的程度，比如共现次数多的单词权重要大于那些共现次数少的（非递减），权重不能太过大到了一定程度就应该停止增加，没有共现过的词应该$f(0)=0$，所以采用了一个分段函数</p><script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}​</script></li><li><p>Glove其实是有监督的，这个label就是$\log(X_{ij})$，$\log(X_{ij})$是通过共现矩阵的统计数据可以直接计算出来的。</p></li></ul><h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><p>后续更新…</p><h2><span id="参考文献">参考文献</span></h2><ol><li>Word2Vec：<a href="https://zhuanlan.zhihu.com/p/140705629" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/140705629</a></li><li>Word2Vec详解：<a href="https://zhuanlan.zhihu.com/p/61635013" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61635013</a></li><li>Glove详解：<a href="http://www.fanyeong.com/2018/02/19/glove-in-detail/" target="_blank" rel="noopener">http://www.fanyeong.com/2018/02/19/glove-in-detail/</a></li><li>Glove模型的理解：<a href="https://zhuanlan.zhihu.com/p/80335195" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/80335195</a></li><li>【深度学习】Word2Vec：<a href="https://www.hrwhisper.me/deep-learning-word2vec/" target="_blank" rel="noopener">https://www.hrwhisper.me/deep-learning-word2vec/</a></li><li>Embedding之word2vec：<a href="https://zhuanlan.zhihu.com/p/59396559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59396559</a></li><li>关于word2vec的一些相关问题整理 &amp; 思考：<a href="https://blog.csdn.net/liujian20150808/article/details/105215414" target="_blank" rel="noopener">https://blog.csdn.net/liujian20150808/article/details/105215414</a></li><li>搞懂NLP中的词向量，看这一篇就足够：<a href="https://www.infoq.cn/article/PFvZxgGDm27453BbS24W" target="_blank" rel="noopener">https://www.infoq.cn/article/PFvZxgGDm27453BbS24W</a></li><li>GloVe与word2vec的区别：<a href="https://zhuanlan.zhihu.com/p/31023929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31023929</a></li><li>基于TensorFlow实现Skip-Gram模型：<a href="https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html</a></li></ol><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2020/10/19/我想去面试系列——Word2vec" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/19/我想去面试系列——Word2vec</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;搜罗万象，找寻word2vecv背后的奥妙…&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/NLP/"/>
    
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/tags/NLP/"/>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/tags/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>我想去面试系列——BERT源码品读</title>
    <link href="http://hhu1506010220.github.io/2020/10/14/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94BERT%E6%BA%90%E7%A0%81%E5%93%81%E8%AF%BB/"/>
    <id>http://hhu1506010220.github.io/2020/10/14/我想去面试系列——BERT源码品读/</id>
    <published>2020-10-14T13:15:41.000Z</published>
    <updated>2020-10-15T11:10:46.474Z</updated>
    
    <content type="html"><![CDATA[<p>细读BERT-Pytorch源码，附带模块解析和一点思考。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#代码目录说明">代码目录说明</a></li><li><a href="#解析参数">解析参数</a></li><li><a href="#读词表">读词表</a></li><li><a href="#获取训练集和测试集">获取训练集和测试集</a><ul><li><a href="#bertdataset">BERTDataset</a></li></ul></li><li><a href="#建模bert">建模BERT</a><ul><li><a href="#bertembedding">BERTEmbedding</a></li><li><a href="#transformer">Transformer △</a><ul><li><a href="#single-attention">Single Attention</a></li><li><a href="#multi-head-attention">Multi Head Attention</a></li><li><a href="#sublayerconnection">SublayerConnection</a></li><li><a href="#feedforwardnetwork">FeedForwardNetwork</a></li><li><a href="#transformer集成">Transformer集成</a></li></ul></li><li><a href="#bertmodel">BERTModel</a></li></ul></li><li><a href="#预训练">预训练</a><ul><li><a href="#nsp-mlm">NSP &amp; MLM</a></li><li><a href="#trainer">Trainer</a></li></ul></li><li><a href="#参考文献">参考文献</a></li></ul><!-- tocstop --><p>源码地址：<a href="https://github.com/codertimo/BERT-pytorch" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch</a>，过了一遍之后感觉这个repo其实bug挺多的..后续作者修了一点bug在 <a href="https://github.com/codertimo/BERT-pytorch/tree/alpha0.0.1a5" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch/tree/alpha0.0.1a5</a>。本文是参照两次的版本综合进行解析。</p><h2><span id="代码目录说明">代码目录说明</span></h2><p>我们后续的说明都认为根目录在bert_pytorch目录下。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">├──── bert_pytorch/</span><br><span class="line">│    ├──── __main__.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    ├──── dataset/</span><br><span class="line">│    │    ├──── dataset.<span class="keyword">py</span></span><br><span class="line">│    │    └──── vocab.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    ├──── model/</span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── attention/</span><br><span class="line">│    │    │    ├──── multi_head.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── single.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── embedding/</span><br><span class="line">│    │    │    ├──── bert.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── position.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── segment.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── token.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── utils/</span><br><span class="line">│    │    │    ├──── feed_forward.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── gelu.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── layer_norm.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── sublayer.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── bert.<span class="keyword">py</span></span><br><span class="line">│    │    ├──── language_model.<span class="keyword">py</span></span><br><span class="line">│    │    └──── transformer.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    └──── trainer/</span><br><span class="line">│         ├──── optim_schedule.<span class="keyword">py</span></span><br><span class="line">│         └──── pretrain.<span class="keyword">py</span></span><br><span class="line">│</span><br><span class="line">├──── LICENSE</span><br><span class="line">├──── Makefile</span><br><span class="line">├──── README.md</span><br><span class="line">├──── requirements.txt</span><br><span class="line">└──── setup.<span class="keyword">py</span></span><br></pre></td></tr></table></figure><h2><span id="解析参数">解析参数</span></h2><p><code>__main__.py</code>主入口进来先解析命令行参数，ArgumentParser()，包括default、type、required、help、default等参数设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 11行～38行</span></span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-c"</span>, <span class="string">"--train_dataset"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"train dataset for train bert"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-t"</span>, <span class="string">"--test_dataset"</span>, type=str, default=<span class="keyword">None</span>, help=<span class="string">"test set for evaluate train set"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-v"</span>, <span class="string">"--vocab_path"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"built vocab model path with bert-vocab"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-o"</span>, <span class="string">"--output_path"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"ex)output/bert.model"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-hs"</span>, <span class="string">"--hidden"</span>, type=int, default=<span class="number">256</span>, help=<span class="string">"hidden size of transformer model"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-l"</span>, <span class="string">"--layers"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"number of layers"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--attn_heads"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"number of attention heads"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-s"</span>, <span class="string">"--seq_len"</span>, type=int, default=<span class="number">20</span>, help=<span class="string">"maximum sequence len"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-d"</span>, <span class="string">"--dropout"</span>, type=float, default=<span class="number">0.1</span>, help=<span class="string">"dropout rate"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-b"</span>, <span class="string">"--batch_size"</span>, type=int, default=<span class="number">64</span>, help=<span class="string">"number of batch_size"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-e"</span>, <span class="string">"--epochs"</span>, type=int, default=<span class="number">10</span>, help=<span class="string">"number of epochs"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-w"</span>, <span class="string">"--num_workers"</span>, type=int, default=<span class="number">5</span>, help=<span class="string">"dataloader worker size"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--with_cuda"</span>, type=bool, default=<span class="keyword">True</span>, help=<span class="string">"training with CUDA: true, or false"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--log_freq"</span>, type=int, default=<span class="number">10</span>, help=<span class="string">"printing loss every n iter: setting n"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--corpus_lines"</span>, type=int, default=<span class="keyword">None</span>, help=<span class="string">"total number of lines in corpus"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--cuda_devices"</span>, type=int, nargs=<span class="string">'+'</span>, default=<span class="keyword">None</span>, help=<span class="string">"CUDA device ids"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--on_memory"</span>, type=bool, default=<span class="keyword">False</span>, help=<span class="string">"Loading on memory: true or false"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">1e-3</span>, help=<span class="string">"learning rate of adam"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_weight_decay"</span>, type=float, default=<span class="number">0.01</span>, help=<span class="string">"weight_decay of adam"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_beta1"</span>, type=float, default=<span class="number">0.9</span>, help=<span class="string">"adam first beta value"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_beta2"</span>, type=float, default=<span class="number">0.999</span>, help=<span class="string">"adam first beta value"</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><h2><span id="读词表">读词表</span></h2><p><code>vocab.py</code>里有三个类<code>TorchVocab</code>、<code>Vocab</code>、<code>WordVocab</code>，他们从左到右是依次继承的关系。构造词表整体的思路是使用集合中的Counter计数器，按照出现频率进行大到小排序，建立词与索引的映射字典，当字典的大小等于词表的大小，或者当前词的出现的频率小于设定的最小出现频率时，词典建立完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 40行~42行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Vocab"</span>, args.vocab_path)</span><br><span class="line">vocab = WordVocab.load_vocab(args.vocab_path)</span><br><span class="line">print(<span class="string">"Vocab Size: "</span>, len(vocab))</span><br></pre></td></tr></table></figure><p><code>TorchVocab</code>类里有三个属性，<code>freqs</code>代表语料库中每个token出现的次数、<code>stoi</code>是一个defaultdict负责将token string映射到数字Interger、<code>itos</code>是一个list里面的每个index对应着一个token string。<code>__init__(self, counter, max_size=None, min_freq=1, specials=[&#39;&lt;pad&gt;&#39;, &#39;&lt;oov&gt;&#39;], vectors=None, unk_init=None, vectors_cache=None)</code>会进行添加特殊token扩展词表specials； 对token按照出现的次数对<code>itos</code>进行排序，出现次数小于min_freq的token将不被添加进词表；此外还有向量初始化、unk初始化设置、向量缓存等设置。<code>vocab_rerank</code>方法可以根据已经排序好的itos对stoi进行重排序。<code>extend</code>方法对字典进行扩展，参数是待扩展的字符串[a,b,c]。</p><p><code>Vocab</code>类初始化时添加了五个token，分别是<code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, <code>&lt;eos&gt;</code>, <code>&lt;sos&gt;</code>, <code>&lt;mask&gt;</code>，分别对应index 0~4。这里的<code>eos</code>对应于<code>cls</code>，<code>sos</code>对应于<code>sep</code>。<code>to_seq</code>和<code>from_seq</code>负责stoi和itos之间的转化。<code>load_vocab</code>和<code>save_vocab</code>负责从文件中加载/存储词表。</p><p><code>WordVocab</code>初始化时<code>__init__(self, texts, max_size=None, min_freq=1)</code>先搞个<code>Counter</code>统计一下各个token的次数，然后调用父类<code>Vocab</code>的构造函数并把这个counter传参进去进行初始化。同时重写了<code>to_seq</code>和<code>from_seq</code>和<code>load_vocab</code>三个方法，这里的<code>to_seq</code>可以实现为自动加<code>[eos]</code>和<code>[sos]</code>。</p><h2><span id="获取训练集和测试集">获取训练集和测试集</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 44行～55行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Train Dataset"</span>, args.train_dataset)</span><br><span class="line">train_dataset = BERTDataset(args.train_dataset, vocab, seq_len=args.seq_len, corpus_lines=args.corpus_lines, on_memory=args.on_memory)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Test Dataset"</span>, args.test_dataset)</span><br><span class="line">test_dataset = BERTDataset(args.test_dataset, vocab, seq_len=args.seq_len, on_memory=args.on_memory) <span class="keyword">if</span> args.test_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Creating Dataloader"</span>)</span><br><span class="line">train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)</span><br><span class="line">test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) <span class="keyword">if</span> test_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure><h3><span id="bertdataset">BERTDataset</span></h3><p><code>BERTDataset</code>类继承自<code>Dataset</code>类。实际上BERT的两个训练任务的随机取样（随机mask token和随机采样next sentence都是在dataset读取时完成的）。<code>__init__()</code>负责初始化一些变量如数据集路径corpus_path、词表vocab、最大序列长度seq_len、是否加载至内存on_memory、数据集内数据的条数corpus_lines、编码encoding，<code>__len__</code>和<code>__getitem__</code>就是对<code>Dataset</code>类重写的正常操作，分别返回整个dataset的长度和想要获取的单个元素。</p><p><code>random_word(sentence)</code>的作用是随机mask token，输入参数是token序列，输出是token id list和label list。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 63行～90行</span></span><br><span class="line"></span><br><span class="line">tokens = sentence.split()</span><br><span class="line">output_label = []</span><br><span class="line"><span class="keyword">for</span> i, token <span class="keyword">in</span> enumerate(tokens):<span class="comment"># 这里的15%其实写的不对，应该是语料中15%的token被选择，而不是每个token都有15%的机会被选择，读者需要注意一下。</span></span><br><span class="line">    prob = random.random()</span><br><span class="line">    <span class="keyword">if</span> prob &lt; <span class="number">0.15</span>: </span><br><span class="line">        <span class="comment"># 随机选择15%的token进行mask</span></span><br><span class="line">        prob /= <span class="number">0.15</span></span><br><span class="line">        <span class="keyword">if</span> prob &lt; <span class="number">0.8</span>: </span><br><span class="line">            <span class="comment"># 80%的token转化成&lt;mask&gt;</span></span><br><span class="line">            tokens[i] = self.vocab.mask_index</span><br><span class="line">        <span class="keyword">elif</span> prob &lt; <span class="number">0.9</span>: </span><br><span class="line">            <span class="comment"># 10%的token随机转化成其他token</span></span><br><span class="line">            tokens[i] = random.randrange(len(self.vocab))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># 10%的token保持原token，如果token在stoi不存在就返回unk_index</span></span><br><span class="line">            tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> tokens, output_label</span><br></pre></td></tr></table></figure><p><code>random_sent(index)</code>的作用是随机采样next sentence，输入参数是index行号，输出是sen1、sen2和label。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 92行～99行</span></span><br><span class="line"></span><br><span class="line">t1, t2 = self.get_corpus_line(index)</span><br><span class="line"><span class="comment"># output_text, label(isNotNext:0, isNext:1)</span></span><br><span class="line"><span class="keyword">if</span> random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line"><span class="keyword">return</span> t1, t2, <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> t1, self.get_random_line(), <span class="number">0</span></span><br></pre></td></tr></table></figure><p>那么给定一个index之后如何<code>__getitem__(index)</code>获取一条数据呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 37行～61行</span></span><br><span class="line"></span><br><span class="line">t1, t2, is_next_label = self.random_sent(index) <span class="comment"># 随机选s1 s2</span></span><br><span class="line">t1_random, t1_label = self.random_word(t1)<span class="comment"># 随机mask单词</span></span><br><span class="line">t2_random, t2_label = self.random_word(t2)<span class="comment"># 随机mask单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [CLS] tag = SOS tag, [SEP] tag = EOS tag</span></span><br><span class="line">t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]</span><br><span class="line">t2 = t2_random + [self.vocab.eos_index] <span class="comment"># 添加头尾特殊标签</span></span><br><span class="line"></span><br><span class="line">t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]</span><br><span class="line">t2_label = t2_label + [self.vocab.pad_index]<span class="comment"># label也要对应加上，用于MLM</span></span><br><span class="line"></span><br><span class="line">segment_label = ([<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(t1))] + [<span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(t2))])[:self.seq_len]<span class="comment"># 添加segment，根据属于s1的token都标1，属于s2的都标2</span></span><br><span class="line">bert_input = (t1 + t2)[:self.seq_len]<span class="comment"># 两句话拼起来成一个整句</span></span><br><span class="line">bert_label = (t1_label + t2_label)[:self.seq_len]<span class="comment"># segment label也拼起来</span></span><br><span class="line"></span><br><span class="line">padding = [self.vocab.pad_index <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.seq_len - len(bert_input))]<span class="comment"># 不够seq_len长的用pad_index补齐</span></span><br><span class="line">bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">"bert_input"</span>: bert_input,</span><br><span class="line">          <span class="string">"bert_label"</span>: bert_label,</span><br><span class="line">          <span class="string">"segment_label"</span>: segment_label,</span><br><span class="line">          <span class="string">"is_next"</span>: is_next_label&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;key: torch.tensor(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> output.items()&#125;</span><br></pre></td></tr></table></figure><p>除此之外<code>get_corpus_line(item)</code>是为了获取一条预料(s1, s2)，<code>get_random_line()</code>是为了获取单独一句话s2。不再赘述。</p><p><code>DataLoader(dataset, batch_size, num_workers)</code>获取dataloader也是正常操作，就不多说了。</p><h2><span id="建模bert">建模BERT</span></h2><h3><span id="bertembedding">BERTEmbedding</span></h3><p>BERTEmbedding考虑三种embedding，<code>TokenEmbedding</code>、<code>PositionalEmbedding</code>和<code>SegmentEmbedding</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/embedding/token.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TokenEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(vocab_size, embed_size, padding_idx=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/embedding/segment.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SegmentEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(<span class="number">3</span>, embed_size, padding_idx=<span class="number">0</span>) <span class="comment"># 这里的0、1、2对应于mask、s1、s2</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># model/embedding/position.py(这是原始transformer的写法，positionEmbedding是固定的向量表示)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEmbedding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, max_len=<span class="number">512</span>)</span>:</span> <span class="comment"># d_model实际上就是embed_size</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model).float()</span><br><span class="line">        pe.require_grad = <span class="keyword">False</span></span><br><span class="line"><span class="comment"># 以下这些代码需要理解，面试会考哦</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * -(math.log(<span class="number">10000.0</span>) / d_model)).exp()</span><br><span class="line">        odd_len = d_model - div_term.size(<span class="number">-1</span>)</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term[:odd_len])<span class="comment"># 源代码对于奇数的处理不太对，这里做了修改</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/embedding/position.py(现在的写法，直接继承父类了，是让BERT直接学一个0 1 2 3 ... 分别对应的向量表示)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, max_len=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(max_len, d_model)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight.data[:x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure><p>注意一点的是，BERT的positionEmbedding是随机初始化，然后目的是学0 1 2 3…等数字对应的向量表示的，而原始的Transformer的positionEmbedding是用<code>sin</code>和<code>cos</code>特定计算不变的，这其实是Transformer和BERT最大的区别之一。原始Transformer的<code>PositionalEmbedding</code>的计算是有数学理论的，见<a href="https://zhuanlan.zhihu.com/p/166244505" target="_blank" rel="noopener">这里</a>。具体计算公式如下：</p><script type="math/tex; mode=display">PE(pos,2i)=\sin(\frac{pos}{1000^{2i/d_{model}}})</script><script type="math/tex; mode=display">PE(pos,2i+1)=\cos(\frac{pos}{1000^{2i/d_{model}}})</script><p>整个<code>BertEmbedding</code>相当于就是把三个embedding直接加起来。至于为什么能加呢？见<a href="https://www.zhihu.com/question/374835153/answer/1069173198" target="_blank" rel="noopener">知乎讨论</a>，实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/embedding/bert.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTEmbedding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT Embedding which is consisted with under features</span></span><br><span class="line"><span class="string">        1. TokenEmbedding : normal embedding matrix</span></span><br><span class="line"><span class="string">        2. PositionalEmbedding : adding positional information</span></span><br><span class="line"><span class="string">        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)</span></span><br><span class="line"><span class="string">        sum of all these features are output of BERTEmbedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size</span></span><br><span class="line"><span class="string">        :param embed_size: embedding size of token embedding</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)</span><br><span class="line">        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)</span><br><span class="line">        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sequence, segment_label)</span>:</span></span><br><span class="line">        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 这里的dropout接在了embedding层后面来防止过拟合</span></span><br></pre></td></tr></table></figure><h3><span id="transformer">Transformer △</span></h3><p>Transformer的话是BERT的核心，也是面试必考的知识点。由于BERT是Transformer的Encoder，所以我们这里只考虑Encoder部分。它主要由两部分组成：一个是MultiHeadAttention还有一个就是带Residual和LayerNorm的FFN。放个图在这里方便看。</p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="1.png" alt=""></p><p>我们先来看Attention部分，多头注意力（Multi-headed attention）机制方法，在编码器和解码器中大量的使用了single Attention机制。由于Transformer的输入只有一个，所以里面用到的Attention都是Self-Attention。</p><h4><span id="single-attention">Single Attention</span></h4><p>在计算attention时主要分为三步，第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有<strong>点积、拼接、感知机</strong>等；然后第二步一般是使用一个softmax函数对这些权重进行归一化；最后将权重和相应的键值value进行加权求和得到最后的attention。目前在NLP研究中，key和value常常都是同一个，即key=value。</p><p>BERT这里的<strong>缩放点积Scaled Dot-Product</strong>就是计算Q与K之间的点乘的时候，为了防止其结果过大，会除以一个尺度标度$\sqrt{d_k}$，其中$\sqrt{d_k}$为一个query和key向量的维度。因为是用的自注意力，所以$d_{query}=d_{key}=d_{value}$，再利用Softmax操作将其结果归一化为概率分布，然后再乘以矩阵V就得到权重求和的表示。</p><p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/attention/single.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None, dropout=None)</span>:</span>  </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args: query, key, value 同源且 shape 相同</span></span><br><span class="line"><span class="string">            query: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">            key: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">            value: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">                 / math.sqrt(query.size(<span class="number">-1</span>)) </span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">        p_attn = F.softmax(scores, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            p_attn = dropout(p_attn)</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><h4><span id="multi-head-attention">Multi Head Attention</span></h4><p>Multi-head Attention其实就是多个single Attention结构的结合，每个attention head学习到在不同表示空间中的特征，搞h个头会比只有1个头能学到更多的信息。具体实现是Query，Key，Value首先分别各自过一个线性变换（这里的变换矩阵是不一样的），然后输入到放缩点积attention，注意这里要做h次，其实也就是所谓的多头，每一次算一个头。而且每次Q，K，V进行线性变换的参数W是不一样的。然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。</p><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/attention/multi_head.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param h: head的个数</span></span><br><span class="line"><span class="string">        :param d_model: hidden_size</span></span><br><span class="line"><span class="string">        """</span>        </span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        self.d_k = d_model // h<span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.h = h</span><br><span class="line">        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)])</span><br><span class="line">        self.output_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        self.attention = Attention()</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [l(x).view(batch_size, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linear_layers, (query, key, value))]</span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment"># 4) Applying Output Linear Model</span></span><br><span class="line">        x = self.output_linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4><span id="sublayerconnection">SublayerConnection</span></h4><p>这部分其实就是实现 <code>Add &amp; Normalize</code>，ResidualConnection和LayerNorm。根据<code>transformers</code>包对于这部分的实现，应该是先Add再Dropout再Norm。注意Attention和FFN的Sublayer是不同的，而非共享参数。我在读这里的时候存在一个疑问是LayerNorm为什么用的是《Attention All you need》里的<code>Annotated Transformer</code>自己实现的LayerNorm，而不用标准的 <code>torch.nn.LayerNorm</code>？最后在issue里我发现这里其实两个东西的差别不大，用哪个都行，只不过<code>Annotated Transformer</code>返回的梯度类型是<code>ThAddBackward</code> 而<code>torch.nn.LayerNorm</code>返回的是 <code>AddcmulBackward</code>，并且值虽然不一样但都还是归一化的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/utils/layer_norm.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/utils/sublayer.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer, dropout=True)</span>:</span><span class="comment"># 这里传参要传一个forward实例化的函数</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">    <span class="keyword">return</span> self.norm(x + self.dropout(sublayer(x)) <span class="keyword">if</span> dropout <span class="keyword">else</span> sublayer(x)) <span class="comment"># 先dropout再add再norm</span></span><br></pre></td></tr></table></figure><h4><span id="feedforwardnetwork">FeedForwardNetwork</span></h4><p>之前的输入要传给FFN，这里的FFN实际上就是两个线性层+dropout+GELU，然后FFN的输出再后续传给SublayerConnection。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/utils/feed_forward.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff)</span>:</span></span><br><span class="line">        super(FeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.activation = GELU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.w_1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.w_2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4><span id="transformer集成">Transformer集成</span></h4><p>最后的实现就是将前面几部分串起来，最后加个dropout即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/transformer.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Bidirectional Encoder = Transformer (self-attention)</span></span><br><span class="line"><span class="string">    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden, attn_heads, feed_forward_hidden, dropout)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: hidden size of transformer</span></span><br><span class="line"><span class="string">        :param attn_heads: head sizes of multi-head attention</span></span><br><span class="line"><span class="string">        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)</span><br><span class="line">        self.feed_forward = FeedForward(d_model=hidden, d_ff=feed_forward_hidden)</span><br><span class="line">        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        x = self.input_sublayer(x, <span class="keyword">lambda</span> _x: self.attention.forward(_x, _x, _x, mask=mask))</span><br><span class="line">        x = self.output_sublayer(x, <span class="keyword">lambda</span> _x: self.feed_forward.forward(_x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3><span id="bertmodel">BERTModel</span></h3><p>以上的东西一拼，就成了BERT模型了。输入是token id sequence和segment label sequence。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/bert.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT model : Bidirectional Encoder Representations from Transformers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden=<span class="number">768</span>, n_layers=<span class="number">12</span>, attn_heads=<span class="number">12</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param vocab_size: vocab_size of total words</span></span><br><span class="line"><span class="string">        :param hidden: BERT model hidden size</span></span><br><span class="line"><span class="string">        :param n_layers: numbers of Transformer blocks(layers)</span></span><br><span class="line"><span class="string">        :param attn_heads: number of attention heads</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = hidden</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.attn_heads = attn_heads</span><br><span class="line">        <span class="comment"># 原论文将FFN的hidden_size设置为普通hidden_size的4倍</span></span><br><span class="line">        self.feed_forward_hidden = hidden * <span class="number">4</span></span><br><span class="line">        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)</span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [TransformerBlock(hidden=hidden, attn_heads=attn_heads,</span><br><span class="line">                              feed_forward_hidden=hidden * <span class="number">4</span>, dropout=dropout)</span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, segment_info)</span>:</span></span><br><span class="line">        <span class="comment"># attention masking for padded token</span></span><br><span class="line">        <span class="comment"># torch.ByteTensor([batch_size, 1, seq_len, seq_len)</span></span><br><span class="line">        mask = (x &gt; <span class="number">0</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, x.size(<span class="number">1</span>), <span class="number">1</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># padding为0，非padding为1</span></span><br><span class="line">        <span class="comment"># embedding the indexed sequence to sequence of vectors</span></span><br><span class="line">        x = self.embedding(x, segment_info)</span><br><span class="line">        <span class="comment"># running over multiple transformer blocks</span></span><br><span class="line">        <span class="keyword">for</span> transformer <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">            x = transformer.forward(x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2><span id="预训练">预训练</span></h2><h3><span id="nsp-amp-mlm">NSP &amp; MLM</span></h3><p>主要包含两个训练任务，NSP就是bert后接一个 <code>linear+softmax</code> 做二分类，MLM是BERT后接linear+softmax做大小为<code>vocab_size</code>的多分类（但是transformers包里面的实现是接 <code>linear(hidden\*hidden) + activation + layerNorm + linear(hidden\*vocab_size)</code> 相当于两层MLP多分类）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/language_model.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NextSentencePrediction</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    2-class classification model : is_next, is_not_next</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: BERT model output size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(hidden, <span class="number">2</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(x[:, <span class="number">0</span>]).tanh())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedLanguageModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    predicting origin token from masked input sequence</span></span><br><span class="line"><span class="string">    n-class classification problem, n-class = vocab_size</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden, vocab_size, embedding=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: output size of BERT model</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(hidden, vocab_size)</span><br><span class="line">        <span class="keyword">if</span> embedding <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.linear.weight.data = embedding.weight.data</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(x))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTLM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT Language Model</span></span><br><span class="line"><span class="string">    Next Sentence Prediction Model + Masked Language Model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert: BERT, vocab_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param bert: BERT model which should be trained</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size for masked_lm</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.bert = bert</span><br><span class="line">        self.next_sentence = NextSentencePrediction(self.bert.hidden)</span><br><span class="line">        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size,</span><br><span class="line">                                           embedding=self.bert.embedding.token)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, segment_label)</span>:</span></span><br><span class="line">        x = self.bert(x, segment_label)</span><br><span class="line">        <span class="keyword">return</span> self.next_sentence(x), self.mask_lm(x)</span><br></pre></td></tr></table></figure><h3><span id="trainer">Trainer</span></h3><p>这里需要注意的一点是，两个预训练任务都采用<code>NLL loss</code>，对于MLM任务要忽略0（代表<code>&lt;pad&gt;</code>）所以加了个ignore，但是对于NSP由于句子的标签是0或1，所以这里的0是不可以忽略的，也就是说<code>&lt;pad&gt;</code>要参与进训练。优化器使用了<code>AdamW</code>，这里是原作者自己实现的没用现成的包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trainer/pretrain.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTTrainer</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERTTrainer make the pretrained BERT model with two LM training method.</span></span><br><span class="line"><span class="string">        1. Masked Language Model : 3.3.1 Task #1: Masked LM</span></span><br><span class="line"><span class="string">        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction</span></span><br><span class="line"><span class="string">    please check the details on README.md with simple example.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert: BERT, vocab_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lr: float = <span class="number">1e-4</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, weight_decay: float = <span class="number">0.01</span>, warmup_steps=<span class="number">10000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 with_cuda: bool = True, cuda_devices=None, log_freq: int = <span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param bert: BERT模型</span></span><br><span class="line"><span class="string">        :param vocab_size: 词汇表大小</span></span><br><span class="line"><span class="string">        :param train_dataloader: 训练集dataloader</span></span><br><span class="line"><span class="string">        :param test_dataloader: 测试机dataloader[can be None]</span></span><br><span class="line"><span class="string">        :param lr: 学习率</span></span><br><span class="line"><span class="string">        :param betas: 优化器偏差 Adam optimizer betas</span></span><br><span class="line"><span class="string">        :param weight_decay: 权重衰减参数 Adam optimizer weight decay param</span></span><br><span class="line"><span class="string">        :param with_cuda: 是否使用cpu traning with cuda</span></span><br><span class="line"><span class="string">        :param log_freq: 日志频率 logging frequency of the batch iteration</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置是否使用gpu, argument -c, --cuda should be true</span></span><br><span class="line">        cuda_condition = torch.cuda.is_available() <span class="keyword">and</span> with_cuda</span><br><span class="line">        self.device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> cuda_condition <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This BERT model will be saved every epoch</span></span><br><span class="line">        self.bert = bert</span><br><span class="line">        <span class="comment"># Initialize the BERT Language Model, with BERT model</span></span><br><span class="line">        <span class="comment"># 初始化BERT并作为参数初始化BERT Lanuage Model</span></span><br><span class="line">        self.model = BERTLM(bert, vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分布式GPU训练 Distributed GPU training if CUDA can detect more than 1 GPU</span></span><br><span class="line">        <span class="keyword">if</span> with_cuda <span class="keyword">and</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">"Using %d GPUS for BERT"</span> % torch.cuda.device_count())</span><br><span class="line">            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the train and test data loader</span></span><br><span class="line">        self.train_data = train_dataloader</span><br><span class="line">        self.test_data = test_dataloader</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the Adam optimizer with hyper-param</span></span><br><span class="line">        self.optim = AdamW(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)</span><br><span class="line">        <span class="comment"># self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 两个任务都采用NLL loss，对于MLM任务0（代表&lt;pad&gt;）要忽略所以加了个ignore，但是对于NSP由于句子的标签是0或1，所以这里的0是不可以忽略的，也就是说&lt;pad&gt;要参与进训练。</span></span><br><span class="line">        <span class="comment"># Using Negative Log Likelihood Loss function for predicting the masked_token</span></span><br><span class="line">        self.masked_criterion = nn.NLLLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">        self.next_criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">        self.log_freq = log_freq</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"Total Parameters:"</span>, sum([p.nelement() <span class="keyword">for</span> p <span class="keyword">in</span> self.model.parameters()]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        self.iteration(epoch, self.train_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        self.iteration(epoch, self.test_data, train=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">iteration</span><span class="params">(self, epoch, data_loader, train=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        loop over the data_loader for training or testing</span></span><br><span class="line"><span class="string">        if on train status, backward operation is activated</span></span><br><span class="line"><span class="string">        and also auto save the model every peoch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param epoch: current epoch index</span></span><br><span class="line"><span class="string">        :param data_loader: torch.utils.data.DataLoader for iteration</span></span><br><span class="line"><span class="string">        :param train: boolean value of is train or test</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        str_code = <span class="string">"train"</span> <span class="keyword">if</span> train <span class="keyword">else</span> <span class="string">"test"</span></span><br><span class="line"></span><br><span class="line">        avg_loss = <span class="number">0.0</span></span><br><span class="line">        total_correct = <span class="number">0</span></span><br><span class="line">        total_element = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">            <span class="comment"># 0. batch_data will be sent into the device(GPU or cpu)</span></span><br><span class="line">            data = &#123;key: value.to(self.device) <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items()&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 1. forward the next_sentence_prediction and masked_lm model</span></span><br><span class="line">            next_sent_output, mask_lm_output = self.model.forward(data[<span class="string">"bert_input"</span>], data[<span class="string">"segment_label"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-1. NLL(negative log likelihood) loss of is_next classification result</span></span><br><span class="line">            next_loss = self.next_criterion(next_sent_output, data[<span class="string">"is_next"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-2. NLLLoss of predicting masked token word</span></span><br><span class="line">            mask_loss = self.masked_criterion(mask_lm_output.transpose(<span class="number">1</span>, <span class="number">2</span>), data[<span class="string">"bert_label"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure</span></span><br><span class="line">            loss = next_loss + mask_loss</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 3. backward and optimization only in train</span></span><br><span class="line">            <span class="keyword">if</span> train:</span><br><span class="line">                self.optim.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                self.optim.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># next sentence prediction accuracy</span></span><br><span class="line">            correct = next_sent_output.argmax(dim=<span class="number">-1</span>).eq(data[<span class="string">"is_next"</span>]).sum().item()</span><br><span class="line">            avg_loss += loss.item()</span><br><span class="line">            total_correct += correct</span><br><span class="line">            total_element += data[<span class="string">"is_next"</span>].nelement()</span><br><span class="line"></span><br><span class="line">            post_fix = &#123;</span><br><span class="line">                <span class="string">"epoch"</span>: epoch,</span><br><span class="line">                <span class="string">"iter"</span>: <span class="string">"[%d/%d]"</span> % (i, len(data_loader)),</span><br><span class="line">                <span class="string">"avg_loss"</span>: avg_loss / (i + <span class="number">1</span>),</span><br><span class="line">                <span class="string">"mask_loss"</span>: mask_loss.item(),</span><br><span class="line">                <span class="string">"next_loss"</span>: next_loss.item(),</span><br><span class="line">                <span class="string">"avg_next_acc"</span>: total_correct / total_element * <span class="number">100</span>,</span><br><span class="line">                <span class="string">"loss"</span>: loss.item()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % self.log_freq == <span class="number">0</span>:</span><br><span class="line">                print(post_fix)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Logging for PaperSpace matrix monitor</span></span><br><span class="line">                <span class="comment"># index = epoch * len(data_loader) + i</span></span><br><span class="line">                <span class="comment"># for code in ["avg_loss", "mask_loss", "next_loss", "avg_next_acc"]:</span></span><br><span class="line">                <span class="comment">#     print(json.dumps(&#123;"chart": code, "y": post_fix[code], "x": index&#125;))</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"EP%d_%s, avg_loss="</span> % (epoch, str_code), avg_loss / len(data_loader), <span class="string">"total_acc="</span>,</span><br><span class="line">              total_correct * <span class="number">100.0</span> / total_element)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, epoch, file_path=<span class="string">"output/bert_trained.model"</span>)</span>:</span></span><br><span class="line">        <span class="string">""</span></span><br><span class="line">        Saving the current BERT model on file_path</span><br><span class="line">        :param epoch: current epoch number</span><br><span class="line">        :param file_path: model output path which gonna be file_path+<span class="string">"ep%d"</span> % epoch</span><br><span class="line">        :<span class="keyword">return</span>: final_output_path</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        output_path = file_path + ".ep%d" % epoch</span></span><br><span class="line"><span class="string">        torch.save(self.bert.cpu(), output_path)</span></span><br><span class="line"><span class="string">        self.bert.to(self.device)</span></span><br><span class="line"><span class="string">        print("EP:%d Model Saved on:" % epoch, output_path)</span></span><br><span class="line"><span class="string">        return output_path</span></span><br></pre></td></tr></table></figure><h2><span id="参考文献">参考文献</span></h2><ol><li>BERT-pytorch：<a href="https://github.com/codertimo/BERT-pytorch/" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch/</a></li><li>BERT源码-tensorflow：<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></li><li>图解什么是Transformer：<a href="https://www.jianshu.com/p/e7d8caa13b21" target="_blank" rel="noopener">https://www.jianshu.com/p/e7d8caa13b21</a></li><li>transformers包：<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">https://github.com/huggingface/transformers</a></li><li>Attention机制详解（二）——Self-Attention与Transformer：<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li></ol><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;细读BERT-Pytorch源码，附带模块解析和一点思考。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/NLP/"/>
    
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/tags/NLP/"/>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/tags/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>我想去面试系列——BERT</title>
    <link href="http://hhu1506010220.github.io/2020/10/10/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94BERT/"/>
    <id>http://hhu1506010220.github.io/2020/10/10/我想去面试系列——BERT/</id>
    <published>2020-10-10T15:46:25.000Z</published>
    <updated>2020-11-04T02:37:24.332Z</updated>
    
    <content type="html"><![CDATA[<p>最近在准备一些面试的东西，正在以面试的角度去温习一些知识点，本文记录的是Bert相关的内容，主要包括基本原理、模型框架、其他变型、细节解读&amp;面试题。</p><a id="more"></a><!-- toc --><ul><li><a href="#基本原理">基本原理</a></li><li><a href="#模型框架">模型框架</a></li><li><a href="#实现">实现</a></li><li><a href="#其他变形">其他变形</a></li><li><a href="#细节-面试题搜集">细节 &amp; 面试题搜集</a></li><li><a href="#参考文献">参考文献</a></li></ul><!-- tocstop --><blockquote><p>Bert的面试重点在transformer架构、multi-head attention、position embedding、<a href="https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读" target="_blank" rel="noopener">源码</a>。</p></blockquote><h2><span id="基本原理">基本原理</span></h2><p><a href="https://arxiv.org/pdf/1810.04805" target="_blank" rel="noopener">BERT：<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers</a>，整体是一个自编码语言模型（Autoencoder LM）。Bidirectional体现在每一个词向量的产生都同时依赖该单词左侧和右侧的语境信息。</p><p><img src="data:image/png;base64,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" data-original="0.png" alt=""></p><p>Bert设计了两个任务来联合预训练该模型，MLM+NSP，训练时候的loss是两个任务的loss sum：</p><ul><li><p>第一个任务是采用 MaskLM 的方式来训练语言模型，形式化为在给定单词上下文序列之后，球当前单词出现的条件概率的乘积。具体的，训练集中选择15%的mask单词，并以特殊标记<code>[MASK]</code>进行替换。为减少微调时对<code>[MASK]</code>标记的过拟合，数据生成器将不是始终用<code>[MASK]</code>替换所选单词，而是80%的时间里将单词替换成<code>[MASK]</code>，10%的时间里用随机单词替换，10%的时间保持单词不变。这样做的目的是使表示偏向实际观察到的单词。这么做的主要原因是：在后续微调任务中语句中并不会出现 [MASK] 标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。上述提到了这样做的一个缺点，其实这样做还有另外一个缺点，就是每批次数据中只有 15% 的标记被预测，这意味着模型可能需要更多的预训练步骤来收敛。与去噪的自动编码器（<a href="https://www.researchgate.net/publication/221346269_Extracting_and_composing_robust_features_with_denoising_autoencoders" target="_blank" rel="noopener">Vincent et al., 2008</a>）不同的是，MLM任务只是让模型预测被遮蔽的标记，而不是要求模型重建整个输入。</p><p><img src="data:image/png;base64,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" data-original="MLM.png" alt=""></p></li><li><p>第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。（RoBerta里说这项任务其实没啥用，起主要作用的还是MLM），这里的masking是静态生成的，每次mask的在训练之前都固定好了。具体的，当为每个预训练选择句子A和B时，50%的时间是选择紧跟着A的实际的下一个句子作为B，而另外50%的时间是随机采样语料库中其他的错误句子。最终的预训练模型在这个任务中达到了 97%-98% 的准确率。</p></li><li><p>对于预训练语料库，使用 BooksCorpus（800M 单词）（<a href="https://arxiv.org/abs/1506.06724v1" target="_blank" rel="noopener">Zhu et al., 2015</a>）和英语维基百科（2,500M 单词）。对于维基百科，只提取文本段落，而忽略列表、表格和标题。</p><script type="math/tex; mode=display">BERT_{base}: L = 12, H = 768, A = 12, TotalParameters = 110M</script><script type="math/tex; mode=display">BERT_{large}: L = 24, H = 1024, A = 16, TotalParameters = 340M</script><p>这里$L$代表Transformer的层数，$H$代表隐藏层大小，$A$代表自注意力的头的个数。</p></li></ul><h2><span id="模型框架">模型框架</span></h2><ul><li><p>Encoder：Transformer Encoder。实际上，<strong>Transformer的Encoder（双向Transformer）就是Bert，Transformer的Decoder（单左向Transformer）稍微改了一点就是GPT。</strong>与 Transformer 本身的 Encoder 端相比，BERT 的 Transformer Encoder 端输入的向量表示多了 Segment Embeddings。</p></li><li><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p></li><li><p>每一层的输入是token+seg+pos embedding，记为$e_0$，将$e_0$输入mult-head attention（注意这里的multi-head的输入是QKV，但是里面的每一个self-attention的输入是QW,KW,VW），然后将该输出与原输入<strong>残差求和ResidualConnection</strong>，<strong>层归一化LayerNormalization</strong>，得到$e_{mid}$，再将$e_{mid}$输入FFN再残差求和得到每一层最后的输出。这里残差机制的意义是可以使得模型更深，避免梯度爆炸和梯度消失的问题；所有层的FFN是参数共享的。</p><script type="math/tex; mode=display">e_0 = Embedding_{token}(inputs) + Embedding_{seg}(inputs) + Embedding_{pos}(inputs)</script><script type="math/tex; mode=display">e_l = EncoderLayer(e_{l-1}, l \in [1.n])</script><p>其中 $e_i \in \mathbb{R}^{N \times d_{model}}$。EncoderLayer的架构是：</p><script type="math/tex; mode=display">e_{mid} = LayerNorm(e_{in} + MultiHeadAttention(e_{in}))</script><script type="math/tex; mode=display">e_{out} = LayerNorm(e_{mid} + FFN(e_{mid}))</script></li><li><p>Multi-Head-Attention：输入向量序列$e_{in} = (e_{in1},e_{in2},…,e_{inN}) \in \mathbb{R}^{N \times d_{model}}, Q = e_{in}, K = e_{in}, V = e_{in}$。</p><script type="math/tex; mode=display">MultiHeadAttention(e_{in}) = MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O</script><p>其中，多头输出$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$，$Concat(head_1, …, head_h) \in \mathbb{R}^{N \times hd_v}$，可学习的参数矩阵$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$，$W_i^K \in \mathbb{R}^{d_{model} \times d_k}$，$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$，$W_O \in \mathbb{R}^{hd_v \times d_{model}}$。</p><p>使用缩放点积作为打分函数的自注意力机制：</p><script type="math/tex; mode=display">Attention(QW_i^Q, KW_i^K, VW_i^K) = softmax(\frac{QW_i^Q(KW_i^K)^\top}{\sqrt{d_k}})VW_i^V</script></li><li><p>前馈神经网络FFN：这里需要注意的是使用了<strong>GELU</strong>作为激活函数（这里的目的是对于学习率的 warm-up 策略，使用的激活函数不再是普通的 ReLu），这里参数矩阵$W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$，$W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$，$b_1 \in \mathbb{R}^{d_{ff}}$，$b_2 \in \mathbb{R}^{d_{model}}$。</p><script type="math/tex; mode=display">FFN(e_{mid}) = GELU(e_{mid}W_1 + b_1)W_2 + b_2</script></li><li><p>下游任务：句子级别：Sentence Pair Classification（取<code>[CLS]</code>后接全连接层+sigmoid）、Single Sentence Classification（取<code>[CLS]</code>后接全连接层+softmax）；单词级别：Question Answering（取特定区间的token后接其他NN）、Single Sentence Tagging（序列标注多分类，对于每一个token有几个 label 就连接到几个全连接层，再接softmax，然后遍历特定区间的所有token）。分析证实，在QA任务上，使用最高层的<strong>[CLS]</strong>效果更好；在序列标注任务上，使用<strong>Attention</strong>方法集成多层词向量效果最好；在小数据集上BiLSTM要比BERT要好。这里存在个疑问，SQuAD上预测起始/结束位置时，在具体实现的时候是两个独立的条件概率呢，还是联合概率呢？</p><p><img src="data:image/png;base64,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" data-original="4.png" alt=""></p></li></ul><h2><span id="实现">实现</span></h2><p>利用huggingface的transformers包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="comment"># BertTokenizer: 分词工具</span></span><br><span class="line"><span class="comment"># BertModel: BERT模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs = tokenizer("This is sentence 1.", return_tensors="pt")</span></span><br><span class="line">inputs = tokenizer(<span class="string">"This is sentence 1."</span>, <span class="string">"This is sentence 2."</span>, return_tensors=<span class="string">"pt"</span>) <span class="comment"># 返回&#123;'input_ids':tensor([[]]), 'token_type_ids':tensor([[]]), 'attention_mask':tensor([[]])&#125;</span></span><br><span class="line">tokenizer.decode(inputs[<span class="string">"input_ids"</span>].data.cpu().numpy().reshape(<span class="number">-1</span>)) <span class="comment"># 可反向还原句子（OOV单词会被替换为[UNK]）</span></span><br><span class="line">sequence_outputs, pooled_outputs = model(**inputs) <span class="comment">#前者是token embedding（1*len_token*hidden_size）,后者是segment embedding（1*len_token），对[CLS]做了池化之后的结果</span></span><br><span class="line"><span class="comment"># 视频讲师说他基本上不用pooled_outputs来当整个句子的表示，因为不怎么能吸收整个句子的语义</span></span><br><span class="line"><span class="comment"># 他提供的trick是，所有向量取平均</span></span><br><span class="line">sen_vec = sequence_outputs.mean(<span class="number">1</span>)</span><br><span class="line">cos_similarity = (sen_vec * sen_vec).sum(<span class="number">-1</span>) / torch.sqrt((sen_vec * sen_vec).sum(<span class="number">-1</span>)) <span class="comment"># cosine similarity</span></span><br></pre></td></tr></table></figure><h2><span id="其他变形">其他变形</span></h2><p><strong><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">XLNet</a></strong>，是BERT之后的一个自回归（Auto Regression，AR）语言模型。可以理解为BERT是一个自编码（Auto Encoder，AE）模型，将输入句子的某些单词 mask 掉，然后再通过 BERT 还原数据。而XLNet这类自回归语言模型则是不断地使用当前得到的信息预测下一个输出。AR 的方法可以更好地学习 token 之间的依赖关系但是它只能利用单向信息（纯前向或者纯后向），而 AE 的方法可以更好地利用深层的双向信息。因此 XLNet 希望将 AR 和 AE 两种方法的优点结合起来，XLNet 使用了 <strong>Permutation Language Model (PLM)</strong>实现这一目的。目的就是为了解决BERT的[MASK]在训练和推理中不一致的问题。</p><ul><li>使用排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然。Permutation 指排列组合的意思，XLNet 将句子中的 token 随机排列，然后采用 AR 的方式预测末尾的几个 token。这样一来，在预测 token 的时候就可以同时利用该 token 双向的信息，并且能学到 token 间的依赖。XLNet 中通过 Attention Mask 实现 PLM，而无需真正修改句子 token 的顺序。</li><li>采用基于目标感知特征的双流自注意力。无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果。为了解决这个问题，论文中提出来新的分布计算方法Two-Stream Self-Attention，来实现目标位置感知。这个需要看原论文。<ul><li>如果目标是预测$x_{z_t}，g_{\theta}(x_{z_{&lt;t}}，z_t)$那么只能有其位置信息$z_t$而不能包含内容信息$x_{z_t}$</li><li>如果目标是预测其他tokens即$x_{z_j}， j&gt;t$，那么应该包含$x_{z_t}$的内容信息这样才有完整的上下文信息</li></ul></li><li>作者还将transformer-xl的两个最重要的技术点应用了进来，即<strong>相对位置编码</strong>与<strong>片段循环机制</strong>。transformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度，对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。对于超长文本，如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。另一方面BERT的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码（相对距离）。</li><li>去除了NSP任务，作者发现该任务对结果的提升并没有太大的影响，主要原因是NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。</li></ul><p><strong><a href="https://arxiv.org/pdf/1905.07129.pdf" target="_blank" rel="noopener">Ernie清华</a></strong>，提出用知识图谱来增强预训练模型的能力，其实就是在与训练的时候在BERT的基础上加入一个实体对齐<strong>Entity Alignment</strong>任务。模型有两个encoder，T-encoder和K-encoder，其实这里的K-encoder只有在预训练的时候有作用，在之后的fine-tuning阶段只要使用T-encoder就可以了，所以这里的重要就是引入了实体对齐这个任务而已。本文提出了随机mask tokens-entity中的entity，然后去预测该位置对应的entity，本质上和MLM（mask language model）任务一致，都属于去噪自编码。KG的引入使得在一些和知识图谱相关的上游任务中，该模型的表现要优于BERT。</p><p><img src="data:image/png;base64,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" data-original="ernie_thu.png" alt=""></p><p><strong><a href="https://arxiv.org/pdf/1904.09223v1.pdf" target="_blank" rel="noopener">Ernie百度</a></strong>，也是引入了知识信息， 但是做法与清华的Ernie完全不一样，这里主要的改变是针对bert中的MLM任务做了一些改进。在bert中只是mask了单个token，但是在语言中，很多时候都是以短语或者实体存在的，如果不考虑短语或者实体中词之间的相关性，而将所有的词独立开来，不能很好的表达句法，语义等信息，因此本文引入了三种mask的方式，分别对token，entity，phrase进行mask。除此之外，本论文中还引入了对话语料，丰富语料的来源，并针对对话语料，给出了一个和NSP相似的任务<strong>Dialogue Language Model (DLM)</strong>。</p><p><strong><a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">RoBERTa</a></strong>，改进版Bert，是目前最好用的，它在模型层面没有改变原BERT，改变的只是预训练的方法。</p><ul><li>更大的batch size，加大训练数据16GB-&gt;160GB，训练时间更长。原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。为什么要用更大的batch size呢？（除了因为他们有钱玩得起外）作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</li><li>不需要Next Sentence Prediction Loss（同时也发现了segment embedding其实作用不大）RoBERTa去除了NSP，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。</li><li>使用更长的训练sequence。</li><li>dynamic masking。原来的BERT采用的是<strong>static masking</strong>，也就是在dataloader创造预训练数据的时候就已经决定了要mask哪个。整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</li></ul><p><strong><a href="https://openreview.net/pdf?id=H1eA7AEtvS" target="_blank" rel="noopener">Albert</a></strong>，新的轻量版的BERT，参数比BERT少了18倍。</p><ul><li>对Embedding因式分解（Factorized embedding parameterization）。ALBERT的作者注意到，对于BERT、XLNet和RoBERTa，word embedding的维度(<code>E</code>)与encoder输出的hidden embedding维度(<code>H</code>)是一样的都是768，<code>H==E</code>，这就意味着二者表达含义的是同等重要的。然而实际上word embedding是用来学习上下文独立表示的，hidden embedding是为了学习上下文依赖表示的。理论上来说hidden embedding的表述包含的信息应该更多一些，因此应该让 <code>H&gt;&gt;E</code>。在NLP任务中，通常词典都会很大，embedding matrix的大小是 <code>E×V</code>，如果和BERT一样让 <code>H==E</code>，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为<code>E</code>，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从<code>O(V×H)</code>降低到了<code>O(V×E+E×H)</code>，当<code>E&lt;&lt;H</code>时参数量减少的很明显。</li><li>跨层的参数共享（Cross-layer parameter sharing）。Albert的核心思想是共享层与层之间的参数，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，每一层的hidden_size变大，所有24层的transformer的参数都用同一个。虽说看起来模型更小更深了，但是实际inference的时候还是会比bert慢的，因为虽然参数共享了，但推理的时候还是得过相同的N层。实际上是通过参数共享的方式降低了内存，预测阶段还是需要和BERT一样的时间，如果采用了xxlarge版本的ALBERT，那实际上预测速度会更慢。</li><li>把NSP任务换成了sentence ordering objective。BERT的NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了，并且在MLM任务中其实也有类型的效果。在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 <strong>sentence-order prediction（SOP）</strong>，SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务。</li><li>ALBERT的作者还发现一个很有意思的点，ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</li></ul><p><strong><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank" rel="noopener">Electra</a></strong>，最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。</p><ul><li>提出了 <strong>Replaced Token Detection (RTD)</strong> 预训练任务，判断每个词是否是被替换过的词。</li><li>ELECTRA 由两个部分组成，第一部分是生成器 (Generator)，生成器将句子中的部分单词进行替换。第二部分是判别器 (Discriminator)，判别器用于判断一个句子中每一个单词是否被替换了，训练的过程会预测所有的单词，比 BERT 更高效。</li><li>使用GAN的训练思路，对于一段文本，ELECTRA 使用了 MLM 对生成器进行训练，也是随机 [mask] 部分单词，然后用Generator预测的结果替换该单词；Discriminator的任务是预测每个位置的单词是来自于原文还是来自Generator的文本。该模型的主观思想是，生成mask的时候有些位置很好学，但是有些位置的东西很难学，那么模型更有机会学到一些很难的场景，能力更强。</li><li>因为由于这种对抗生成的方式不同于GAN可以梯度连续从Generator传到Discriminator，Electra的梯度不能从Generator到Discriminator，所以只能综合两者的损失值对Generator进行损失传递。ELECTRA 总体的损失函数由生成器的损失函数 LMLM 和判别器的损失函数 LDisc 组成。生成器的训练损失函数仍然是 MLM 损失函数，主要原因是生成器将单词进行替换，而单词是离散的，导致判别器到生成器的梯度中断了。具体来说是利用Generator loss对Generator进行传导，用Generator loss + Discriminator loss对Discriminator进行传导。</li></ul><p><strong><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">OpenAI-GPT</a></strong>、<strong><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">ELMo</a></strong>的架构和Bert几乎是一样的，都是输入之后用特征提取器提取特征，然后输出。</p><p>ELMo和OpenAI GPT的思想其实非常非常简单，就是用海量的无标注数据学习语言模型，在学习语言模型的过程中自然而然的就学到了上下文的语义关系。它们都是来学习一个语言模型，前者使用的是LSTM而后者使用Transformer，在进行下游任务处理的时候也有所不同，ELMo是把它当成特征。拿分类任务来说，输入一个句子，ELMo用LSTM把它扫一次，这样就可以得到每个词的表示，这个表示是考虑上下文的，因此”He deposited his money in this bank”和”His soldiers were arrayed along the river bank”中的两个bank的向量是不同的。下游任务用这些向量来做分类，它会增加一些网络层，但是ELMo语言模型的参数是固定的。而OpenAI GPT不同，它直接用特定任务来Fine-Tuning Transformer的参数。因为用特定任务的数据来调整Transformer的参数，这样它更可能学习到与这个任务特定的上下文语义关系，因此效果也更好。</p><p>差别是，GPT只用了前序序列没用后续序列（Bert发现了这点于是用了MLM或者是CBOW），ELMo用的是BiLSTM（Bert采用了更强的特征提取器Transformer）。Bert相对于其之前工作word2vec、GPT、ELMo的优势在于：</p><ul><li>相比于<strong>word2vec</strong>包含了语境信息；</li><li>相比于<strong>ELMo</strong>速度更快，并行程度更高，ELMo 使用独立训练的从左到右和从右到左的 LSTM 的连接来为下游任务生成特征；</li><li>相比于<strong>GPT</strong>包含了双向的语境信息。BERT Transformer 使用的是双向的自注意力，而 GPT Transformer 使用的是受限的自注意力，每个标记只能关注其左边的语境。</li><li>更多的训练语料，GPT 是在 BooksCorpus 上训练出来的然而 BERT 是在 BooksCorpus和 Wikipedia上训练出来的。GPT 仅在微调时使用<code>[SEP]</code>和<code>[CLS]</code> 而BERT 在预训练时使用 <code>[SEP]</code>， <code>[CLS]</code> 和 <code>Segment embedding</code>。</li></ul><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="2.png" alt=""></p><p>总体来说，基于Transformer的模型统治了NLP，主要原因有：①更大规模的预训练数据搭配更大的模型和更强大的算力②一些局部的小技巧（数据预处理、masking、训练任务等）③模型的压缩与蒸馏、加速与并行。缺点就是在少训练数据时容易过拟合。</p><p><img src="data:image/png;base64,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" data-original="summary.png" alt=""></p><h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><p>后续更新…</p><h2><span id="参考文献">参考文献</span></h2><ol><li>Pre-training of Deep Bidirectional Transformers for Language Understanding：<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></li><li>BERT 模型详解：<a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/bert-theory/</a></li><li>七月在线NLP褚博士：BERT模型深度修炼指南：<a href="http://www.julyedu.com/video/play/264/8448" target="_blank" rel="noopener">http://www.julyedu.com/video/play/264/8448</a></li><li>NewBeeNLP关于BERT，面试官们都怎么问：<a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247484380&amp;idx=1&amp;sn=55fccbb2565520bf747fa35359271673&amp;chksm=97aee50ea0d96c180ccf9177c653e78097ee7a9f18a3fb02811c128548cb585b427887d0dc8f&amp;mpshare=1&amp;scene=23&amp;srcid=1010Pr5bZpA0VDQksaXrjOif&amp;sharer_sharetime=1602343574372&amp;sharer_shareid=ac5afd09812a03f906a0b77a8c423be9#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com</a></li><li>一文读懂BERT（原理篇）：<a href="https://blog.csdn.net/jiaowoshouzi/article/details/89073944" target="_blank" rel="noopener">https://blog.csdn.net/jiaowoshouzi/article/details/89073944</a></li><li>The Illustrated Transformer：<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-transformer/</a></li><li>A Visual Guide to Using BERT for the First Time：<a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank" rel="noopener">http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a></li><li>XLNet详解：<a href="https://zhuanlan.zhihu.com/p/110204573" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110204573</a></li><li>XLNet详解：<a href="https://www.jianshu.com/p/2b5b368cbaa0" target="_blank" rel="noopener">https://www.jianshu.com/p/2b5b368cbaa0</a></li><li>超细节的BERT/Transformer知识点：<a href="https://zhuanlan.zhihu.com/p/132554155" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/132554155</a></li><li>一文揭开ALBERT的神秘面纱：<a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">https://blog.csdn.net/u012526436/article/details/101924049</a></li><li>BERT知识点总结：<a href="https://blog.csdn.net/XiangJiaoJun_/article/details/107129808" target="_blank" rel="noopener">https://blog.csdn.net/XiangJiaoJun_/article/details/107129808</a></li><li>后BERT时代：15个预训练模型对比分析与关键点探索：<a href="https://www.jiqizhixin.com/articles/2019-08-26-16" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-08-26-16</a></li><li>BERT模型详解：<a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/bert-theory/</a></li></ol><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2020/10/10/我想去面试系列——BERT" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/10/我想去面试系列——BERT</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在准备一些面试的东西，正在以面试的角度去温习一些知识点，本文记录的是Bert相关的内容，主要包括基本原理、模型框架、其他变型、细节解读&amp;amp;面试题。&lt;/p&gt;
    
    </summary>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/categories/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/NLP/"/>
    
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/tags/NLP/"/>
    
      <category term="我想去面试" scheme="http://hhu1506010220.github.io/tags/%E6%88%91%E6%83%B3%E5%8E%BB%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>《百面机器学习》阅读笔记</title>
    <link href="http://hhu1506010220.github.io/2020/10/06/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://hhu1506010220.github.io/2020/10/06/《百面机器学习》阅读笔记/</id>
    <published>2020-10-06T05:29:00.000Z</published>
    <updated>2020-10-15T10:29:07.612Z</updated>
    
    <content type="html"><![CDATA[<p>特征工程、模型评估、机器学习、神经网络。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#特征工程">特征工程</a></li><li><a href="#模型评估">模型评估</a></li><li><a href="#机器学习">机器学习</a><ul><li><a href="#svm">SVM</a></li><li><a href="#逻辑回归-线性回归">逻辑回归、线性回归</a></li><li><a href="#降维">降维</a></li><li><a href="#非监督">非监督</a></li><li><a href="#概率图">概率图</a></li><li><a href="#优化算法">优化算法</a></li><li><a href="#采样">采样</a></li><li><a href="#集成学习">集成学习</a></li><li><a href="#梯度提升决策树gbdt">梯度提升决策树GBDT</a></li></ul></li><li><a href="#神经网络">神经网络</a><ul><li><a href="#前向神经网络">前向神经网络</a></li><li><a href="#激活函数">激活函数</a></li><li><a href="#反向传播">反向传播</a></li><li><a href="#循环神经网络">循环神经网络</a><ul><li><a href="#梯度爆炸">梯度爆炸</a></li><li><a href="#lstm">LSTM</a></li><li><a href="#seq2seq">Seq2Seq</a></li><li><a href="#attention">Attention</a></li></ul></li><li><a href="#训练技巧">训练技巧</a></li><li><a href="#其他">其他</a></li></ul></li></ul><!-- tocstop --><h2><span id="特征工程">特征工程</span></h2><hr><ol><li><p>为什么需要做归一化？答：归一化到相同数值区间之后，更新速度更为一致，容易更快地通过梯度下降找到最优解。主要方法包括线性归一化、零均值归一化。（适用于梯度下降求解的方法，对于决策树之类的不适用）</p></li><li><p>类别特征怎么处理？答：序号编码（保留偏序关系）、独热编码（0 0 0 1 0 0）、二进制编码（允许多位为1，hash）。</p></li><li><p>什么是组合特征？答：一阶离散特征组合成高阶，提高复杂关系的拟合能力。</p></li><li><p>文本表示模型？</p><ul><li>词袋模型：是将文章以词为单位切分开，忽略词的出现顺序，将文章表示成一个长向量，每一维代表一个单词，该维的权重表示重要程度。常用<strong>TF-IDF</strong>来计算权重。</li><li>N-gram：指有些词组不能拆开，那么由这n个词组成的词组(N-gram)也作为一个单独的特征放到向量表示中，构成N-gram模型。</li><li>TF-IDF：TF(t,d)单词t在文档d中出现的概率，IDF(t)逆文档概率。TF-IDF=TF×IDF。IDF(t) = log(文章总数/(包含单词t的文章总数+1))</li><li>Topic：将具有<strong>相同主题的词或词组映射到同一维度上</strong>，映射到的这一维度表示某个主题。</li><li>词嵌入：向量化。</li></ul></li><li><p>Word2vec？</p><ul><li><p>COBW根据上下文出现的词语来预测当前词的生成概率，Skip-gram根据当前词来预测上下文中各词的生成概率。图中w(t)即当前词，w(t-2)、w(t-1)、w(t+1)、w(t+2)即上下文中出现的词，所谓的滑动窗口大小就是上下文取词个数，为2。</p><p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p></li><li><p>模型结构：输入层+映射层+输出层。</p></li><li><p>输入层是one-hot编码的上下文词，如果词汇表中单词总数为N，则输入层的词表示即为N维one-hot向量。</p></li><li><p>映射层(隐含层)有个K隐含单元，即我们要得到的K维词嵌入向量，这里就是K个隐含单元。映射层的值由输入层的N维向量和N×K维权重矩阵计算得到。<br>注：CBOW模型需要将输入词计算的隐含单元值求和，比Skip-gram多这一部分。</p></li><li><p>输出层向量的值由隐含层的K维向量和K×N维向量计算得到，输出也是N维向量。但是这里的输出向量并不能保证所有维度加起来为1(one-hot编码加起来就为1)，要通过Softmax激活函数进行归一化。</p></li><li><p>训练的目标是使得语料库中所有单词的整体生成概率最大化。可以用反向传播，沿梯度更优的方向更新权重，但是由于Softmax存在归一化项，推导出来的迭代公式需要对词汇表中所有单词进行遍历(毕竟是维词向量)，所以每次迭代过程非常缓慢。有Hierarchical Softmax和Negative Sampling两种改进方法。</p></li><li><p>得到训练出来的各词对应的向量：训练得到的维度为N×K和K×N的矩阵，可以选择任意一个作为N个词的K维词向量。</p></li></ul></li></ol><h2><span id="模型评估">模型评估</span></h2><hr><ol><li>Precision、Recall、F1、Accuracy、ROC、AUC</li><li>余弦距离（相对差异）、欧氏距离（数值绝对差异）。距离公理（正定、对称、三角不等式）</li><li>Holdout 7/3验证、K-fold交叉验证（分词k份，k-1份训练，1份验证，最后k次取平均，经常取10折）、自助验证（n次有放回的随机抽样，抽出过的训练，没抽出过的验证，极限趋向于0.368）</li><li>超参数调优：网格搜索（复杂度高，通常先广度范围+长步长找大范围，再缩小找精确最优值）、随机搜索（比网格快一些）、贝叶斯优化（学习目标函数形状）</li><li>过拟合（数据增广、降低模型复杂度、加正则项、集成学习）、欠拟合（添加新特征、增加模型复杂度、减少正则化系数）</li></ol><h2><span id="机器学习">机器学习</span></h2><hr><h3><span id="svm">SVM</span></h3><p>若训练集中不存在两个点在同一位置，则存在一组参数$\{\alpha_1,…,\alpha_m,b\}$以及参数$\gamma$使得该SVM的训练误差为0。训练误差为0的SVM分类器一定存在。加入松弛变量的SVM的训练误差可以为0吗？答案是，<strong>并不一定</strong>能得到训练误差为0的模型。如果使用SMO算法来训练一个加入松弛变量的线性SVM模型，则我们的<strong>优化目标改变了</strong>，并不再是使训练误差最小。当C取0时，w也取0时即可达到优化目标。</p><h3><span id="逻辑回归-线性回归">逻辑回归、线性回归</span></h3><p>【逻辑回归和线性回归的区别】为什么逻辑回归名字叫回归但是处理的却是分类任务？答：Logistic回归是通过log函数将线性回归方程转为0到1之间的概率值，再以阈值threshold(默认是0.5)将概率值低于0.5的概率值转为0，高于等于0.5的概率值转为1，这就是为什么logistic回归可以解决分类的问题。而线性回归中实际上求解的是$y’=\theta^Tx$，是对我们假设的真实关系$y=\theta^Tx+\epsilon$的一个近似，其中$\epsilon$代表误差项，我们使用这个近似项来处理回归问题。还有一个很大的区别是，逻辑回归中的因变量是<strong>离散</strong>的，线性回归中的因变量是<strong>连续</strong>的。两者都使用了<strong>极大似然估计</strong>来对训练样本进行建模。线性回归使用最小二乘法，逻辑回归使用如下似然函数。都可以用梯度下降的方法进行求解。</p><script type="math/tex; mode=display">L(\theta)=\prod_{i=1}^{N}P(y_i|x_i;\theta)=\prod_{i=1}^{N}(\pi(x_i))^{y_i}(1-\pi(x_i))^{1-y_i}</script><h3><span id="降维">降维</span></h3><p><a href="https://www.jianshu.com/p/c888fe7ad900" target="_blank" rel="noopener">主成分分析PCA、线性判别LDA</a></p><h3><span id="非监督">非监督</span></h3><p><a href="https://www.jianshu.com/p/7676f3b9808f" target="_blank" rel="noopener">K-Mean聚类、高斯混合GMM、自组织映射神经网络SOM、聚类评估</a></p><h3><span id="概率图">概率图</span></h3><p><a href="https://www.jianshu.com/p/4b75ef727218" target="_blank" rel="noopener">联合概率分布、概率图表示、生成式与判别式模型、马尔科夫模型</a>、<a href="https://www.jianshu.com/p/1696319bedcb" target="_blank" rel="noopener">主题模型pLSA+LDA</a></p><h3><span id="优化算法">优化算法</span></h3><ol><li><a href="https://www.jianshu.com/p/6bd07708e608" target="_blank" rel="noopener">有监督的二分类损失函数：0-1损失、Hinge损失、Logistic损失、交叉熵损失</a></li><li><a href="https://www.jianshu.com/p/6bd07708e608" target="_blank" rel="noopener">凸函数 &amp; 凸优化</a></li><li><a href="https://www.jianshu.com/p/6bd07708e608" target="_blank" rel="noopener">经典优化方法：直接法、迭代法</a></li><li><a href="https://www.jianshu.com/p/6bd07708e608" target="_blank" rel="noopener">随机梯度下降：随机梯度下降、小批量梯度下降</a></li><li><a href="https://www.jianshu.com/p/873f02053bd5" target="_blank" rel="noopener">优化的梯度下降方法：动量(Momentum)、AdaGrad、Adam、Nesterov Accelerated Gradient、AdaDelta、RMSProp、AdaMax、Nadam</a></li><li><a href="https://www.jianshu.com/p/873f02053bd5" target="_blank" rel="noopener">L1、L2正则化，稀疏性</a></li></ol><p>为什么L1正则化产生稀疏解？答：①<strong>解空间形状</strong>，L2正则项约束后的解空间是圆形，而L1正则项约束的解空间是多边形。显然，多边形的解空间更容易在尖角处与等高线碰撞出稀疏解；②<strong>函数叠加</strong>，观察最小值，L1的最小值在零点处取得，对应的w是0，产生了稀疏性。L2<strong>只有减小w的作用</strong>，对解空间的稀疏性没有贡献；③贝叶斯先验，L1正则化相当于对模型参数w引入了<strong>拉普拉斯先验</strong>，L2正则化相当于引入了<strong>高斯先验</strong>，而拉普拉斯先验使参数为0的可能性更大。</p><h3><span id="采样">采样</span></h3><p><a href="https://www.jianshu.com/p/c602eaad78c1" target="_blank" rel="noopener">作用、常见采样方法、高斯分布采样、马尔科夫蒙特卡洛采样、贝叶斯网络采样、不均衡样本集重采样</a></p><p><a href="https://www.jianshu.com/p/35b76485aba5" target="_blank" rel="noopener">偏差与方差</a></p><h3><span id="集成学习">集成学习</span></h3><p>Boosting：Boosting方法训练基分类器时采用<strong>串行</strong>的方式，各个基分类器之间有<strong>依赖</strong>。其基本思想是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p><p>Bagging：Bagging是Bootstrap Aggregating的简称，意思就是再抽样，然后在每个样本上训练出来的模型取平均。Bagging方法在训练过程中，各基分类器之间<strong>无强依赖</strong>，可以进行<strong>并行</strong>训练。为了让基分类器之间相互独立，将训练集分为若干子集。由于个体之间存在差异性，最终做出的判断不会完全一致。在最终做决策时，每个个体单独作出判断，再通过投票的方式作出最后的集体决策。Bagging的主要好处是集成后的分类器的方差，比基分类器的方差小，所采用的基分类器最好是<strong>本身对样本分布较为敏感</strong>的(即所谓不稳定的分类器)，这样Bagging才能有用武之地。线性分类器或者K近邻都是较为稳定的分类器，本身方差就不大，所以用他们做基分类器Bagging并不能在原有基分类器的基础上获得更好的表现，甚至可能因为Bagging的采样，导致它们在训练中更难收敛，从而增大了集成分类器的偏差。</p><ul><li>Bagging能够提高弱分类器性能的原因是<strong>降低了方差</strong>。</li><li>Boosting能够提高弱分类器性能的原因是<strong>降低了偏差</strong>。</li><li><a href="https://www.jianshu.com/p/35b76485aba5" target="_blank" rel="noopener">从减小方差和偏差的角度解释Boosting和Bagging的原理</a></li></ul><p>基分类器的错误率要大于集成分类器，<strong>基分类器的错误是偏差和方差两种错误之和</strong>。<strong>偏差</strong>主要是由于分类器的<strong>表达能力有限</strong>导致的<strong>系统性误差</strong>，表现在训练误差<strong>不收敛</strong>。Boosting方法通过逐步聚焦于基分类器分错的样本，减小集成分类器的偏差。方差是由于分类器<strong>对于样本分布过于敏感</strong>，导致在训练样本数较少时，产生<strong>过拟合</strong>。Bagging方法则是采取<strong>分而治之</strong>的策略，通过对训练样本多次采样，训练多个不同模型进行综合，来减小集成分类器的<strong>方差</strong>。用简单多数投票方法来集成结果，超过半数基分类器出错的概率会<strong>随着基分类器的数量增加而下降</strong>。</p><p>集成学习一般可以分为以下3个步骤：</p><ul><li>找到误差相互独立的基分类器。</li><li>训练基分类器。</li><li>合并基分类器的结果。合并基分类器的方法有voting和stacking两种。stacking是用<strong>串行</strong>的方式，把前一个基分类器的结果<strong>输出到下一个分类器</strong>，将所有基分类器的输出结果相加作为最终的输出(或用更复杂的方法，将各基分类器的输出作为<strong>特征</strong>，用逻辑回归作为融合模型进行最后的结果预测)。一般voting的方式更多一点。</li></ul><p>Adaboost采取了Boosting的思想，对分类正确的样本降低了权重，对分类错误的样本<strong>升高或者保持权重不变</strong>。在最后进行模型融合的过程中，也根据错误率对基分类器进行<strong>加权融合</strong>。<br>另一个非常流行的是<strong>梯度提升决策树（GDBT）</strong>，思想是每一棵树学的是之前所有树结论和的<strong>残差</strong>，这个残差是一个加预测值后能得真实值的累加量。比如预测年龄，真实年龄是25岁，第一棵树预测22岁，则第二棵树将年龄设为3岁去学习。使用残差继续学习，就是GBDT中的Gradient Boosted所表达的意思。</p><p><strong><a href="https://www.jianshu.com/p/35285f01cc8e" target="_blank" rel="noopener">决策树</a></strong>：决策树可以较为方便地将样本的<strong>权重整合</strong>到训练过程中，而不需要使用过采样的方法来调整样本权重。决策树的<strong>表达能力</strong>和<strong>泛化能力</strong>，可以通过调节树的层数来做折中。数据样本的<strong>扰动</strong>对于决策树的<strong>影响较大</strong>，因此不同子样本集合生成的决策树基分类器<strong>随机性较大</strong>，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分类属性，很好地引入了随机性。其中很著名的算法是基于决策树基分类器的<strong>随机森林</strong>。ID3最大信息增益、C4.5最大信息增益比、CART树最大基尼指数。三种决策树的构造准则、前后剪枝。</p><h3><span id="梯度提升决策树gbdt">梯度提升决策树GBDT</span></h3><p>梯度提升决策树(Gradient Boosting Decision Tree，GBDT)是Boosting算法中非常流行的模型。Gradient Boosting是Boosting中的一大类算法，基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以<strong>累加</strong>的形式结合到现有模型中。采用<strong>决策树</strong>为弱分类器的Gradient Boosting算法被称为GBDT，有时也称MART(Multiple Additive Regression Tree)。GBDT中使用的决策树通常为<strong>CART树</strong>。</p><p><a href="https://www.jianshu.com/p/4d59c94fa0c6" target="_blank" rel="noopener">梯度提升和梯度下降的区别和联系？</a></p><p><a href="https://www.jianshu.com/p/4d59c94fa0c6" target="_blank" rel="noopener">GBDT的优点和局限性？</a></p><ul><li>优点：<ul><li>预测阶段的计算速度快，树与树之间可<strong>并行计算</strong>。</li><li>在分布<strong>稠密</strong>的数据集上，泛化能力和表达能力都很好。</li><li>采用决策树作为弱分类器使得GBDT模型具有较好的<strong>解释性</strong>和<strong>鲁棒性</strong>，能够自动发现<strong>特征间的高阶关系</strong>，也不需要对数据进行特殊的预处理如归一化等。</li></ul></li><li>局限：<ul><li>GBDT在<strong>高维稀疏</strong>的数据集上，表现不如支持向量机或神经网络。</li><li>GBDT在处理<strong>文本分类特征</strong>问题上，相对于其他模型优势不如在处理数值特征时明显。</li><li>训练过程需要<strong>串行</strong>训练，只能在决策树内部采用一些<strong>局部并行</strong>的手段提升训练速度。</li></ul></li></ul><p><a href="https://www.jianshu.com/p/4d59c94fa0c6" target="_blank" rel="noopener">XGBoost与GBDT的联系和区别？</a> </p><ul><li>GBDT是<strong>机器学习算法</strong>，XGBoost是该算法的<strong>工程实现</strong>。</li><li>在使用CART作为基分类器时，XGBoost就显式地加入了<strong>正则项</strong>来控制模型的复杂度，有利于防止过拟合，提高模型的泛化能力。</li><li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行<strong>二阶泰勒展开</strong>，可以同时使用一阶和二阶导数。</li><li>传统的GBDT使用CART作为基分类器，XGBoost<strong>支持多种类型的基分类器</strong>，比如线性分类器。</li><li>传统的GBDT在每轮迭代时使用<strong>全部</strong>的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行<strong>采样</strong>。</li><li>传统的GBDT没有设计对<strong>缺失值</strong>进行处理，XGBoost能够自动学习出缺失值的处理策略。 </li></ul><h2><span id="神经网络">神经网络</span></h2><hr><h3><span id="前向神经网络">前向神经网络</span></h3><ol><li>如果只使用一个隐藏层，需要多少隐节点能够实现包含$n$元输入的任意布尔函数？答：$2^{n-1}$个隐节点。</li><li>考虑多隐层的情况，实现包含$n$元输入的任意布尔函数最少需要多少个网络节点和网络层？<br>答：<strong>一个隐藏层即可计算异或函数</strong>。$n$元异或函数需要包括$3(n-1)$个节点(包括最终输出节点)。可以发现，多隐层结构可以将隐节点的数目从指数级$O(2^{n-1})$直接减少至线性级$O(3(n-1))$。需要的最少网络层数为$2\log_2N$(向上取整)。</li></ol><h3><span id="激活函数">激活函数</span></h3><p><a href="https://www.jianshu.com/p/f44b7b7afa18" target="_blank" rel="noopener">Sigmoid、Tanh、ReLU</a> </p><ol><li>Sigmoid和Tanh激活函数为什么会导致梯度消失的现象？答：因为它们的导数f’(z)在z很大或者很小的时候都会趋近于0，造成梯度消失的情况。</li><li>ReLU系列激活函数相对于Sigmoid和Tanh激活函数的优点是什么？它们有什么局限性以及如何改进？答：优点：从计算的角度上，Sigmoid和Tanh激活函数都<strong>需要计算指数函数</strong>，<strong>复杂度高</strong>，而ReLU<strong>只需要一个阈值</strong>即可得到激活值。ReLU的<strong>非饱和性</strong>可以有效解决梯度消失的问题，提供<strong>相对宽的激活边界</strong>。ReLU的<strong>单侧抑制</strong>提供了网络的<strong>稀疏表达能力</strong>。局限：其训练过程中会<strong>导致神经元死亡</strong>。函数$f(z)=\max(0,z)$导致负梯度在经过ReLU单元时被置为0，且在之后也不被任何数据激活，即流经该神经元的梯度永远为0，不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。从而有人推出了LReLU。</li></ol><h3><span id="反向传播">反向传播</span></h3><p>在网络训练中，前向传播最终产生一个标量损失函数，反向传播算法则将损失函数的信息沿网络层向后传播用以计算梯度，达到优化网络参数的目的。</p><p>损失函数：平方误差（更适合输出为<strong>连续</strong>，并且最后一层不含Sigmoid或Softmax激活函数的神经网络）、交叉熵误差（更适合<strong>二分类</strong>或<strong>多分类</strong>的场景）。</p><p>为什么平方损失不适合最后一层含有Sigmoid或Softmax激活函数的神经网络？答：平方误差损失函数相对于输出层的导数其中激活函数的导数的那一项$f’(z^{(L)})$，当激活函数为sigmoid的时候，如果$z^{(L)}$绝对值比较大，函数梯度会趋于饱和，从而使得基于梯度的<strong>学习速度非常缓慢</strong>。而交叉熵损失函数对于输出层的导数是线性的，不会存在学习速度过慢的问题。</p><h3><span id="循环神经网络">循环神经网络</span></h3><p>在网络训练中，前向传播最终产生一个标量损失函数，反向传播算法则将损失函数的信息沿网络层向后传播用以计算梯度，达到优化网络参数的目的。</p><h4><span id="梯度爆炸">梯度爆炸</span></h4><p>梯度爆炸的问题可以通过<strong>梯度裁剪</strong>来缓解，即当梯度的范式大于某个给定值时，对梯度进行<strong>等比收缩</strong>。<strong>梯度消失</strong>问题相对棘手，需要对模型本身进行改进。深度残差网络是对前馈神经网络的改进，通过<strong>残差学习</strong>的方式缓解了梯度消失的现象；对于循环神经网络来说，LSTM及其变种GRU等模型通过加入<strong>门控机制</strong>，很大程度上弥补了梯度消失所带来的损失。</p><p>在循环神经网络中使用ReLU作为激活函数，需要对矩阵W的<strong>初值做一定限制</strong>，否则十分容易引发<strong>数值问题</strong>。而在卷积神经网络中一般不会出现严重的数值问题。只有当W的取值在<strong>单位矩阵附近</strong>时才能取得比较好的效果，因此需要将W初始化为单位矩阵，在一些应用中取得了与LSTM相似的结果，并且学习速度比LSTM更快，是一个小trick。</p><h4><span id="lstm">LSTM</span></h4><p><a href="https://www.jianshu.com/p/711a40a3f609" target="_blank" rel="noopener">LSTM</a>输入门控制当前计算的新状态以多大程度更新到记忆单元中；遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉；输出门控制当前的输出有多大程度上取决于当前的记忆单元。与传统的循环神经网络不同的是，从上一个记忆单元的状态到当前的状态的转移不一定完全取决于激活函数计算得到的状态，还由<strong>输入门和遗忘门来共同控制</strong>。</p><p>为什么遗忘门、输入门和输出门使用Sigmoid，生成候选记忆用Tanh作为激活函数？答：这两个激活函数都是<strong>饱和</strong>的，即在输入达到一定值时输出不再明显变化。如果用非饱和激活函数如ReLU，则难以实现门控效果。<br> Sigmoid函数输出在0~1之间，符合<strong>门控的物理定义</strong>；Tanh函数输出在-1~1之间，与大多数场景下特征分布是<strong>0中心</strong>吻合。此外，Tanh函数在输入为0附近相比Sigmoid函数有更大的梯度，通常使模型收敛更快。</p><h4><span id="seq2seq">Seq2Seq</span></h4><p>在Seq2Seq模型中，当前输出词由当前隐状态以及上一个输出词决定，即：</p><script type="math/tex; mode=display">s_i=f(y_{i-1},s_{i-1}),</script><script type="math/tex; mode=display">p(y_i|y_1,y_2,...,y_{i-1})=g(y_{i-1},s_i).</script><p>核心思想是，通过深度神经网络将一个作为输入的序列映射为一个作为输出的序列，这一过程由<strong>编码输入</strong>和<strong>解码输出</strong>两个环节构成。在经典的实现中，编码器和解码器各由一个循环神经网络构成，既可以选择传统循环神经网络也可以使用LSTM、GRU等。Seq2Seq模型中，两个循环神经网络是共同训练的。Seq2Seq模型最核心的部分是其解码部分。最基础的解码方法是<strong>贪心法</strong>，即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。贪心法的<strong>计算代价低</strong>，适合作为<strong>基准结果</strong>与其他方法相比较。贪心法获得的是一个<strong>局部最优解</strong>，由于实际问题的复杂性，该方法往往并不能取得最好的结果。</p><p><strong>集束搜索</strong>是常见的改进算法，它时一种<strong>启发式算法</strong>。该方法会保存beam size个当前的较佳选择，然后解码时每一步根据保存的结果进行下一步扩展和排序，接着选择前beam size个进行保存，循环迭代，直到结束时选择最佳的一个作为解码的结果。实际应用中<strong>beam size取8~12为佳</strong>。除了集束搜索，解码时使用<strong>堆叠的RNN</strong>、增加<strong>Dropout</strong>机制、与解码器之间建立<strong>残差连接</strong>等，均是常见的改进措施。此外，解码时还可以采用<strong>记忆网络</strong>等，从外界获取知识。</p><p>实际使用中，随着输入序列的增长，模型的性能会显著下降。因为编码时输入序列的全部信息压缩到了一个向量表示中，随着序列增长，句子越前面的词的信息丢失就越严重。将源语言句子<strong>逆序输入</strong>，或者<strong>重复输入两遍</strong>来训练模型，可以得到一定的性能提升。使用LSTM也能够一定程度上缓解这个问题，但在实践中过长的序列仍然难以有很好的表现。同时，Seq2Seq模型的输出序列中，常常会损失部分输入序列的信息，这是因为在解码时，当前词及对应的源语言词的<strong>上下文信息</strong>和<strong>位置信息</strong>在解码过程中丢失了。</p><h4><span id="attention">Attention</span></h4><p><strong>注意力机制</strong>使得解码时每一步可以有针对性地关注与当前有关的编码结果，从而减小解码器输出表示的学习难度，也更容易学习到长期的依赖关系。</p><p>Seq2Seq中加入注意力机制也是为了解决过长序列的问题，在注意力机制中，仍然可以用普通的循环神经网络对输入序列进行解码，得到隐状态$h_1,h_2,…,h_T$。但在解码时，每一个输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态。</p><script type="math/tex; mode=display">s_i=f(s_{i-1},y_{i-1},c_i),</script><script type="math/tex; mode=display">p(y_i|y_1,y_2,...,y_{i-1})=g(y_{i-1},s_i,c_i).</script><p>其中$c_i$为<strong>语境向量</strong>，是输入序列全部隐状态$h_1,h_2,…,h_T$的<strong>加权和</strong></p><script type="math/tex; mode=display">c_i=\sum_{j=1}^T\alpha_{ij}h_j.</script><p>其中注意力权重参数$\alpha_{ij}$并不是一个固定权重，而是由另一个神经网络计算得到</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{k=1}^T\exp(e_{ik})},</script><script type="math/tex; mode=display">e_{ij}=a(s_{i-1},h_j).</script><p>神经网络$a$根据上一个时间步的输出序列隐状态$s_{i-1}$和当前时间步的输入序列隐状态$h_j$，计算出一个$x_j,y_j$对齐的值$e_{ij}$，再归一化得到权重$\alpha_{ij}$。一个直观的解释是，在生成一个输出词时，会考虑每一个输入词和当前输出词的<strong>对齐关系</strong>，对齐越好的词，会有越大的权重，对生成当前输出词的影响也就越大。</p><h3><span id="训练技巧">训练技巧</span></h3><ol><li>解决过拟合的方法有：数据集增强、参数范数惩罚/正则化、模型集成等；其中Dropout是<strong>模型集成方法</strong>中最高效与常用的技巧。同时，深度神经网络的训练中涉及诸多手调参数，如学习率、权重衰减系数、Dropout比例等，这些参数的选择会显著影响模型最终的训练效果。批量归一化(Batch Normalization，BN)方法有效规避了这些复杂参数对网络训练产生的影响，在<strong>加速训练收敛</strong>的同时也<strong>提升了网络的泛化能力</strong>。</li><li>神经网络训练时是否可以将全部参数初始化为0？答：不能。<br>考虑全连接的深度神经网络，同一层的神经元都是<strong>同构</strong>的，拥有相同的输入和输出，如果再将参数全部初始化为相同的值，那么无论前向传播还是反向传播的取值都是相同的。学习过程将永远无法打破这种<strong>对称性</strong>，最终同一个网络层中的各个参数仍然是相同的。</li><li>Dropout抑制过拟合的工作原理：Dropout随机丢弃部分神经元的机制，相当于每次迭代都在<strong>训练不同结构的神经网络</strong>。类比于Bagging方法，Dropout可以被认为是一种实用的<strong>大规模深度神经网络的模型集成算法</strong>。这是由于传统意义上的Bagging涉及多个模型的同时训练与测试评估，当网络与参数规模庞大时，这种集成方式需要消耗大量的运算时间与空间。Dropout在<strong>小批量级别</strong>上的操作，提供了一种<strong>轻量级的Bagging集成</strong>近似，能够实现指数级数量神经网络的训练与评测。对于包含N个神经元节点的网络，在Dropout的作用下可以看作$2^N$个模型的集成。这$2^N$个模型可认为是原始网络的子网络，它们共享部分权重，并且具有相同的网络层数，而模型整体的参数数目不变，大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程为<strong>减弱全体神经元之间的联合适应性</strong>，减少过拟合的风险，增强泛化能力。</li><li>批量归一化的基本动机与原理：神经网络训练过程的本质是学习数据分布，如果训练数据与测试数据的分布不同将会大大降低网络的泛化能力，因此我们需要在训练开始前对所有输入数据进行归一化处理。然而随着网络训练的进行，每个隐层的参数变化使得<strong>后一层的输入发生变化</strong>，从而每一批训练数据的分布也随之变化，致使网络在每次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。<br>批量归一化方法是针对每一批数据，<strong>在网络每一层输入之间增加归一化处理</strong>(均值为0，标准差为1)，将所有批数据强制性在同一的数据分布下。可以看作在每一层输入和上一层输出之间<strong>加入了一个新的计算层</strong>，对数据的分布进行了额外的约束，从而增强模型的泛化能力。但是批量归一化同时也<strong>降低了模型的拟合能力</strong>，归一化之后的输入分被强制为0均值和1标准差。在具体的应用中，是会加入变换重构以及其他可学习参数的。</li></ol><h3><span id="其他">其他</span></h3><p><a href="https://www.jianshu.com/p/3f726cfe7ff8" target="_blank" rel="noopener">深度卷积网络、深度残差网络</a> </p><p>参考文献：本文部分内容参考自<a href="https://www.jianshu.com/c/90223df0f45c" target="_blank" rel="noopener">蓝白绛</a>。</p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2020/10/06/《百面机器学习》阅读笔记" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/06/《百面机器学习》阅读笔记</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;特征工程、模型评估、机器学习、神经网络。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://hhu1506010220.github.io/categories/MachineLearning/"/>
    
    
      <category term="MachineLearning" scheme="http://hhu1506010220.github.io/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达CS230深度学习笔记（五）</title>
    <link href="http://hhu1506010220.github.io/2020/02/14/%E5%90%B4%E6%81%A9%E8%BE%BECS230%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://hhu1506010220.github.io/2020/02/14/吴恩达CS230深度学习笔记（五）/</id>
    <published>2020-02-14T15:59:59.000Z</published>
    <updated>2020-02-18T17:04:58.297Z</updated>
    
    <content type="html"><![CDATA[<p>涉及 序列模型。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#循环序列模型">循环序列模型</a><ul><li><a href="#rnn">RNN</a></li><li><a href="#语言模型">语言模型</a></li><li><a href="#gru-和-lstm-和-birnn">GRU 和 LSTM 和 BiRNN</a></li></ul></li><li><a href="#自然语言处理与词嵌入">自然语言处理与词嵌入</a><ul><li><a href="#词嵌入">词嵌入</a></li></ul></li><li><a href="#序列模型和注意力机制">序列模型和注意力机制</a><ul><li><a href="#注意力机制">注意力机制</a></li></ul></li></ul><!-- tocstop --><h2><span id="循环序列模型">循环序列模型</span></h2><h3><span id="rnn">RNN</span></h3><p>one-hot式神经网络（输入为n个one-hot向量，输出为一个长度为vocabulary的one-hot向量）的不足：</p><ul><li>输入和输出数据在不同例子中可以有不同的长度，不是所有的例子都有着同样输入长度$T_{x}$或是同样输出长度的$T_{y}$。即使每个句子都有最大长度，也许你能够填充（<strong>pad</strong>）或零填充（<strong>zero pad</strong>）使每个输入语句都达到最大长度，但仍然看起来不是一个好的表达方式。</li><li>像这样单纯的神经网络结构，它并不共享从文本的不同位置上学到的特征。不同位置上的相同字符会视为不同，缺乏对位置信息的捕捉。</li></ul>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的，我们用$W_{\text{ax}}$来表示管理着从$x^{<1>}$到隐藏层的连接的一系列参数，每个时间步使用的都是相同的参数$W_{\text{ax}}$。而激活值也就是水平联系是由参数$W_{aa}$决定的，同时每一个时间步都使用相同的参数$W_{aa}$，同样的输出结果由$W_{\text{ya}}$决定。<p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p>在这个循环神经网络中，它的意思是在预测${\hat{y}}^{< 3 >}$时，不仅要使用$x^{<3>}$的信息，还要使用来自$x^{<1>}$和$x^{<2>}$的信息，因为来自$x^{<1>}$的信息可以通过这样的路径（上图编号1所示的路径）来帮助预测${\hat{y}}^{<3>}$。这个循环神经网络的一个缺点就是它只使用了这个序列中之前的信息来做出预测，没有使用序列中后部分的信息，尤其当预测${\hat{y}}^{<3>}$时，它没有用到$x^{<4>}$，$x^{<5>}$，$x^{<6>}$等等的信息。<p>循环神经网络用的激活函数经常是<strong>tanh</strong>，不过有时候也会用<strong>ReLU</strong>，但是<strong>tanh</strong>是更通常的选择。选用哪个激活函数是取决于你的输出$y$，如果它是一个二分问题，那么我猜你会用<strong>sigmoid</strong>函数作为激活函数，如果是$k$类别分类问题的话，那么可以选用<strong>softmax</strong>作为激活函数。</p><p>更一般的情况下，在$t$时刻，</p><script type="math/tex; mode=display">a^{< t >} = g_{1}(W_{aa}a^{< t - 1 >} + W_{ax}x^{< t >} + b_{a})</script>$$\hat y^{< t >} = g_{2}(W_{{ya}}a^{< t >} + b_{y})$$<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="2.png" alt=""></p><p>我们来分析一下前向传播的计算，现在你有一个输入序列，$x^{<1>}$，$x^{<2>}$，$x^{<3>}$一直到$x^{&lt; T_{x} &gt;}$，然后用$x^{<1>}$还有$a^{<0>}$计算出时间步1的激活项，再用$x^{<2>}$和$a^{<1>}$计算出$a^{<2>}$，然后计算$a^{<3>}$等等，一直到$a^{&lt; T_{x} &gt;}$。所有的这些激活项都要取决于参数$W_{a}$和$b_{a}$。</3></2></1></2></0></1></3></2></1></p><p>然后为了计算反向传播，你还需要一个损失函数。我们先定义一个元素损失函数，叫交叉熵损失函数（<strong>Cross Entropy Loss</strong>）：</p><script type="math/tex; mode=display">L^{}( \hat y^{},y^{}) = - y^{}\log\hat y^{}-( 1- y^{})log(1-\hat y^{})</script><p>现在我们来定义整个序列的损失函数，将$L$定义为</p>$$L(\hat y,y) = \ \sum_{t = 1}^{T_{x}}{L^{< t >}(\hat y^{< t >},y^{< t >})}$$<p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><p>RNN的一些变种：</p><ul><li>“多对多”（many-to-many）：输入$x^{<1>}$来计算$\hat y^{<1>}$，$\hat y^{<2>}$等等一直到$\hat y^{<t_{y}>}$。输入和输出的长度可能不相同。</t_{y}></2></1></1></li><li>“多对一”（many-to-one）：不再在每个时间上都有输出了，而是让这个<strong>RNN</strong>网络读入整个句子，然后在最后一个时间上得到输出。</li><li>“一对多”（one-to-many）：只输入一个$x$，每个时间步都有输出。</li></ul><p><img src="data:image/png;base64,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" data-original="4.png" alt=""></p><p>本的<strong>RNN</strong>算法还有一个很大的问题，就是梯度消失的问题。一个数值主要与附近的输入有关，后面层的输出基本上很难受到序列靠前的输入的影响。这是因为不管输出是什么，不管是对的，还是错的，这个区域都很难反向传播到序列的前面部分，也因此网络很难调整序列前面的计算。梯度消失在训练<strong>RNN</strong>时是首要的问题，尽管梯度爆炸也是会出现，但是梯度爆炸很明显，因为指数级大的梯度会让你的参数变得极其大，以至于你的网络参数崩溃，你会看到很多<strong>NaN</strong>，或者不是数字的情况，这意味着你的网络计算出现了数值溢出。</p><h3><span id="语言模型">语言模型</span></h3><p>利用RNN实现语言模型，相当于第一步以$x^{<1>}$（全初始化为0）作为输入，然后计算$\hat y^{<1>}$，这一步其实就是通过一个softmax层来预测字典中的任意单词会是第一个词的概率；然后把$\hat y^{<1>}$当作第二步的输入$x^{<2>}$，计算$\hat y^{<2>}$，表示当第一个词是$\hat y^{<1>}$，去计算第二个词的概率…每次的计算都会考虑之前得到的所有词，以此类推。</1></2></2></1></1></1></p><p>现在有一个新句子，它是$y^{<1>}$，$y^{<2>}$，$y^{<3>}$，现在要计算出整个句子中各个单词的概率，方法就是第一个<strong>softmax</strong>层会告诉你$y^{<1>}$的概率，这也是第一个输出，然后第二个<strong>softmax</strong>层会告诉你在考虑$y^{<1>}$的情况下$y^{<2>}$的概率，然后第三个<strong>softmax</strong>层告诉你在考虑$y^{<1>}$和$y^{<2>}$的情况下$y^{<3>}$的概率，把这三个概率相乘，最后得到这个含3个词的整个句子的概率。</3></2></1></2></1></1></3></2></1></p><p>要用RNN训练一个语言模型，要做的第一件事就是将语料库中的句子标记化，建立一个字典，然后将每个单词都转换成对应的向量，也就是字典中的索引。</p><p>定义句子的结尾，一般的做法就是增加一个额外的标记，叫做<strong>EOS</strong>，它表示句子的结尾，这样能够帮助你搞清楚一个句子什么时候结束。</p><p>如果你遇到了一个不在你词表中的单词，答案就是创建一个新的标记，也就是一个叫做<strong>Unknow Word</strong>的伪造单词，用&lt;<strong>UNK</strong>&gt;作为标记，来表示不在词表中的单词。</p><p>如果你建立一个基于字符的语言模型，比起基于词汇的语言模型，你的序列$\hat y^{<1>}$，$\hat y^{<2>}$，$\hat y^{<3>}$在你的训练数据中将会是单独的字符，而不是单独的词汇。使用基于字符的语言模型有优点也有缺点，优点就是你不必担心会出现未知的标识（OOV单词），一个主要缺点就是你最后会得到太多太长的序列，所以基于字符的语言模型在捕捉句子中的依赖关系也就是句子较前部分如何影响较后部分不如基于词汇的语言模型那样可以捕捉长范围的关系，并且基于字符的语言模型训练起来计算成本比较高昂。</3></2></1></p><h3><span id="gru-和-lstm-和-birnn">GRU 和 LSTM 和 BiRNN</span></h3><p><img src="data:image/png;base64,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" data-original="5.png" alt=""></p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="6.png" alt=""></p><p><strong>LSTM</strong>反向传播计算：</p><p><strong>门求偏导：</strong></p><script type="math/tex; mode=display">d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) \ * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle})\tag{1}</script><script type="math/tex; mode=display">d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_i^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2) \tag{2}</script><script type="math/tex; mode=display">d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) \ * \tilde c^{\langle t \rangle} \ * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle})\tag{3}</script><script type="math/tex; mode=display">d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) \ * c_{prev} \ * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})\tag{4}</script><p><strong>参数求偏导 ：</strong></p><script type="math/tex; mode=display">dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{5}</script><script type="math/tex; mode=display">dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{6}</script><script type="math/tex; mode=display">dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{7}</script><script type="math/tex; mode=display">dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \ x_t\end{pmatrix}^T \tag{8}</script><p>为了计算$db_f, db_u, db_c, db_o$ 需要各自对$d\Gamma_f^{\langle t \rangle}, d\Gamma_u^{\langle t \rangle}, d\tilde c^{\langle t \rangle}, d\Gamma_o^{\langle t \rangle}$ 求和。</p><p>最后，计算隐藏状态、记忆状态和输入的偏导数：</p><script type="math/tex; mode=display">da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \tag{9}</script><script type="math/tex; mode=display">dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \tag{10}</script><script type="math/tex; mode=display">dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}\tag{11}</script><p><strong>GRU</strong>的优点是这是个更加简单的模型，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模，更加容易适应规模更大的问题。但是<strong>LSTM</strong>更加强大和灵活，因为它有三个门而不是两个。如果你想选一个使用，我认为<strong>LSTM</strong>在历史进程上是个更优先的选择，所以如果你必须选一个，我感觉今天大部分的人还是会把<strong>LSTM</strong>作为默认的选择来尝试。</p><p>具体的讲解建议去《动手学深度学习》看，上边讲的很浅显易懂，这里就不赘述了。</p><h2><span id="自然语言处理与词嵌入">自然语言处理与词嵌入</span></h2><h3><span id="词嵌入">词嵌入</span></h3><p>词嵌入在语言模型、机器翻译领域用的少一些，如果你从某一任务<strong>A</strong>迁移到某个任务<strong>B</strong>，只有<strong>A</strong>中有大量数据，而<strong>B</strong>中数据少时，迁移的过程才有用。所以对于很多<strong>NLP</strong>任务这些都是对的，而对于一些语言模型和机器翻译则不然。如何用词嵌入做迁移学习的步骤：</p><ul><li>第一步，先从大量的文本集中学习词嵌入。一个非常大的文本集，或者可以下载网上预训练好的词嵌入模型，网上你可以找到不少，词嵌入模型并且都有许可。</li><li>第二步，你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词。这样做的一个好处就是你可以用更低维度的特征向量代替原来的10000维的one-hot向量，现在你可以用一个300维更加紧凑的向量。尽管one-hot向量很快计算，而学到的用于词嵌入的300维的向量会更加紧凑。</li><li>第三步，当你在你新的任务上训练模型时，在你的命名实体识别任务上，只有少量的标记数据集上，你可以自己选择要不要继续微调，用新的数据调整词嵌入。实际中，只有这个第二步中有很大的数据集你才会这样做，如果你标记的数据集不是很大，通常我不会在微调词嵌入上费力气。</li></ul><p>Word2vec：<strong>CBOW</strong>是从原始语句推测目标字词；而<strong>Skip-Gram</strong>正好相反，是从目标字词推测出原始语句。<strong>CBOW</strong>对小型数据库比较合适，而<strong>Skip-Gram</strong>在大型语料中表现更好。 （下图左边为<strong>CBOW</strong>，右边为<strong>Skip-Gram</strong>）</p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="7.jpg" alt=""></p><p>负采样：刻意去生成一些负样本。</p><p>Glove：更简单的词嵌入方法。</p><p>词嵌入除偏：词嵌入本身可能具有一些如性别歧视、种族歧视的偏好。可采用作差取平均、中和步、均衡步等步骤。没碰到过，不仔细看了。</p><p>《动手学深度学习》里面讲过了，不再赘述。</p><h2><span id="序列模型和注意力机制">序列模型和注意力机制</span></h2><p>重点讲注意力机制，Seq2Seq、集束搜索等，参见《动手学深度学习》。</p><h3><span id="注意力机制">注意力机制</span></h3><p>注意力模型或者说注意力这种思想（The attention algorithm, the attention idea）已经是深度学习中最重要的思想之一。</p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="8.png" alt=""></p><p>像这样给定一个很长的法语句子，在你的神经网络中，这个绿色的编码器要做的就是读整个句子，然后记忆整个句子，再在感知机中传递，而对于这个紫色的神经网络，即解码网络（<strong>the decoder network</strong>）将生成英文翻译。人工翻译并不会通过读整个法语句子，再记忆里面的东西，然后从零开始，机械式地翻译成一个英语句子。而人工翻译，首先会做的可能是先翻译出句子的部分，再看下一部分，并翻译这一部分。看一部分，翻译一部分，一直这样下去。你会通过句子，一点一点地翻译，因为记忆整个句子是非常困难的。注意力模型让一个神经网络只注意到一部分的输入句子。当它在生成句子的时候，更像人类翻译。</p><p><img src="data:image/png;base64,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" data-original="9.png" alt=""></p><p>相当于对于$t$时刻的输入$x_t$，要额外地学一个向量$a^{<t,1>},a^{<t,2>}…a^{<t,n>}$，每个$a^{<t,i>}$都是一个注意力权重，表示当你生成第$t$个词时你应该放多少注意力在第$i$个单词上。所有的$a^{<t,i>}$应该最终过一个softmax来确保它们的和为1。这里的t和i可以同时是encoder里的，表示编码时候的注意力；也可以是t代表decoder的i代表encoder的，表示在解码第t个单词时，需要放多多少注意力在编码的第i个单词上。很多个$a^t$结合起来，就最终形成了注意力矩阵$A$。</t,i></t,i></t,n></t,2></t,1></p><p><img src="data:image/png;base64,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" data-original="10.png" alt=""></p><p>如何计算这些$e$项，一种方式是用小的神经网络去学习。$s^{t-1}$是神经网络在上个时间步的隐藏状态，$a^{t’}$是上个时间步的的特征。直观来讲就是，如果你想要决定要花多少注意力在$t’$的激活值上。然后用这个小网络去拟合$s^{t-1}$和$a^{t’}$到$e^{t,t’}$的计算函数，从而再计算$a^{&lt;t,t’&gt;}$。</p><p>这个算法的一个缺点就是它要花费三次方的时间，就是说这个算法的复杂是$O(n3)$的，如果你有$T_x$个输入单词和$T_y$个输出单词，于是注意力参数的总数就会是$T_x\times T_y$，所以这个算法有着三次方的消耗。但是在机器翻译的应用上，输入和输出的句子一般不会太长，可能三次方的消耗是可以接受。</p><p>这门课Attention讲的也只是最基础的概念，具体的学习还需要后续自己继续深入了解。</p><p>【原文出自黄海广博士 <strong><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">deeplearning_ai_books</a></strong>】</p></6></5></4></3></3></1></2></1></3></1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;涉及 序列模型。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达CS230深度学习笔记（四）</title>
    <link href="http://hhu1506010220.github.io/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BECS230%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://hhu1506010220.github.io/2020/02/13/吴恩达CS230深度学习笔记（四）/</id>
    <published>2020-02-13T06:05:20.000Z</published>
    <updated>2020-02-18T13:17:49.068Z</updated>
    
    <content type="html"><![CDATA[<p>涉及 卷积神经网络。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#卷积神经网络">卷积神经网络</a><ul><li><a href="#卷积conv">卷积Conv</a></li><li><a href="#池化pooling">池化Pooling</a></li></ul></li></ul><!-- tocstop --><p>前言：因为CNN我用的不多，也不感兴趣，所以会减少篇幅。</p><h2><span id="卷积神经网络">卷积神经网络</span></h2><h3><span id="卷积conv">卷积Conv</span></h3><p><img src="data:image/png;base64,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" data-original="1.png" alt=""></p><p>常用的边缘检测过滤器：$\begin{bmatrix}1 &amp; 0 &amp; - 1 \ 2 &amp; 0 &amp; - 2 \ 1 &amp; 0 &amp; - 1 \end{bmatrix}$，叫做<strong>Sobel</strong>的过滤器，$\begin{bmatrix} 3&amp; 0 &amp; - 3 \ 10 &amp; 0 &amp; - 10 \ 3 &amp; 0 &amp; - 3\ \end{bmatrix}$，叫做<strong>Scharr</strong>过滤器。</p><p>卷积操作，就是这么回事，卷积核依次滑过，得到每个单元的结果。如果我们有一个$n×n​$的图像，用$f×f​$的过滤器做卷积，那么输出的维度就是$(n-f+1)×(n-f+1)​$。卷积神经网络善于捕捉平移不变。</p><p>这样的话会有两个缺点，第一个缺点是每次做卷积操作，你的图像就会缩小；第二个缺点时，如果你注意角落边缘的像素，四个顶点的像素点只被一个输出所触碰或者使用。但如果是在中间的像素点，就会有许多的卷积区域与之重叠。所以那些在角落或者边缘区域的像素点在输出中采用较少，意味着你丢掉了图像边缘位置的许多信息。</p><p>为了解决这些问题，你可以在卷积操作之前填充这幅图像，也就是<strong>padding</strong>操作。习惯上，你可以用0去填充，如果$p$是填充的数量，比如$p=1$，因为我们在周围都填充了一个像素点，输出也就变成了$(n+2p-f+1)×(n+2p-f+1)$。这样一来，丢失信息或者更准确来说角落或图像边缘的信息发挥的作用较小的这一缺点就被削弱了。</p><p>至于选择填充多少像素，通常有两个选择，分别叫做<strong>Valid</strong>卷积和<strong>Same</strong>卷积。Valid卷积意味着不填充，这样的话，如果你有一个$n×n$的图像，用一个$f×f$的过滤器卷积，它将会给你一个$(n-f+1)×(n-f+1)$维的输出。另一个经常被用到的填充方法叫做Same卷积，那意味你填充后，你的输出大小和输入大小是一样的。根据这个公式$n-f+1$，当你填充$p$个像素点，$n$就变成了$n+2p$，最后公式变为$n+2p-f+1$。因此如果你有一个$n×n$的图像，用$p$个像素填充边缘，输出的大小就是这样的$(n+2p-f+1)×(n+2p-f+1)$。如果让$n+2p-f+1=n$的话，使得输出和输入大小相等，如果你用这个等式求解$p$，那么$p=(f-1)/2$。所以当$f$是一个奇数的时候，只要选择相应的填充尺寸，你就能确保得到和输入相同尺寸的输出。</p><p>卷积中的<strong>步幅</strong>是另一个构建卷积神经网络的基本操作，指卷积核每次移动的长度。如果你用一个$f×f$的过滤器卷积一个$n×n$的图像，你的padding为$p$，步幅为$s$，输出于是变为$\frac{n+2p - f}{s} + 1 \times \frac{n+2p - f}{s} + 1$。卷积有很多种标记方法，这是我们最常用的卷积符号。关于高度，宽度和通道的顺序并没有完全统一的标准卷积。本文主要按照高度、宽度和通道损失数量的顺序来计算。</p><p><strong>如果商不是一个整数怎么办？</strong>在这种情况下，我们向下取整。$⌊z⌋$这是向下取整的符号，这也叫做地板除(<strong>floor</strong>)，这意味着$z$向下取整到最近的整数。这个原则实现的方式是，你只在卷积核完全在图像内或填充之后的图像内部时，才对它进行运算。如果有任意一个卷积核单元移动到了外面，那你就不要进行相乘操作，这是一个惯例。因此正确计算输出维度的方法是向下取整，以免$\frac{n + 2p - f}{s}$不是整数。</p>多层卷积，无非就是把卷积核的维数++，不再赘述。在卷积网络层中，用$f^{[l]}$表示过滤器大小，我们说过过滤器大小为$f×f$，上标$\lbrack l\rbrack$表示$l$层中过滤器大小为$f×f$。通常情况下，上标$\lbrack l\rbrack$用来标记$l$层。用$p^{[l]}$来标记padding的数量，用$s^{[l]}$标记步幅。每一层$l$的输入为$n_{H}^{\left\lbrack l - 1 \right\rbrack} \times n_{W}^{\left\lbrack l - 1 \right\rbrack} \times n_{c}^{\left\lbrack l - 1\right\rbrack}$，为上一层的激活值，输出大小为$n_{H}^{[l]} \times n_{W}^{[l]} \times n_{c}^{[l]}$，其中$n_{H}^{[l]} = \lfloor\frac{n_{H}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$，$n_{W}^{[l]} = \lfloor\frac{n_{W}^{\left\lbrack l - 1 \right\rbrack} +2p^{[l]} - f^{[l]}}{s^{[l]}} +1\rfloor$。过滤器中通道的数量必须与输入中通道的数量一致。因此，输出通道数量就是输入通道数量，所以过滤器维度等于$f^{[l]} \times f^{[l]} \times n_{c}^{\left\lbrack l - 1 \right\rbrack}$。$f^{[l]} \times f^{[l]} \times n_{c}^{[l - 1]}$只是一个过滤器的维度，$n_{c}^{[l]}$则是过滤器的数量。<p>卷积层的参数就是卷积核的单元总数加上偏置数，比如每个过滤器都是5×5，一个过滤器有25个参数，再加上偏差参数，那么每个过滤器就有26个参数，一共有6个过滤器，所以参数共计156个。卷积网络映射这么少参数有两个原因：一是参数共享。整张图片共享特征检测器，每个特征检测器以及输出都可以在输入图片的不同区域中使用同样的参数，以便提取特征。它不仅适用于低阶特征，同样适用于高阶特征。第二个是稀疏链接。某一个位置的输出值$o$只和与它相关的$f×f$个元素相连接，而其他的$n×n-f×f$个元素不会对输出产生影响，这就是稀疏连接的概念。神经网络可以通过这两种机制减少参数，以便我们用更小的训练集来训练它，从而预防过度拟合。</p><h3><span id="池化pooling">池化Pooling</span></h3><p>除了卷积层，卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。</p><p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p><p><strong>MaxPooling</strong>操作的功能就是只要在任何一个象限内提取到某个特征，它都会保留在最大化的池化输出里。所以MaxPooling的实际作用就是，如果在过滤器中提取到某个特征，那么保留其最大值。如果没有提取到这个特征，可能在右上象限中不存在这个特征，那么其中的最大值也还是很小，这就是最大池化的直观理解。</p><p>一个有意思的特点就是，它有一组超参数，但并没有参数需要学习。实际上，梯度下降没有什么可学的，一旦确定了$f$和$s$，它就是一个固定运算，梯度下降无需改变任何值。之前讲的计算卷积层输出大小的公式同样适用于最大池化，即$\frac{n + 2p - f}{s} + 1$，这个公式也可以计算最大池化的输出大小。</p><p>另外还有一种类型的池化，<strong>平均池化</strong>，它不太常用。我简单介绍一下，这种运算顾名思义，选取的不是每个过滤器的最大值，而是平均值。在很深的神经网络中，可以用平均池化来分解规模到一个小的网络表示层。池化的超级参数包括过滤器大小$f$和步幅$s$，最大池化时，往往很少用到超参数padding。需要注意的一点是，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化。只有这些设置过的超参数，可能是手动设置的，也可能是通过交叉验证设置的。</p><p>注意：池化层没有参数。</p><p>有几个经典的深度卷积网络：LeNet-5、AlexNet和VGGNet，还有目标检测，因为不搞CV，不看了。</p><p>【原文出自黄海广博士 <strong><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">deeplearning_ai_books</a></strong>】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;涉及 卷积神经网络。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达CS230深度学习笔记（三）</title>
    <link href="http://hhu1506010220.github.io/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BECS230%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://hhu1506010220.github.io/2020/02/12/吴恩达CS230深度学习笔记（三）/</id>
    <published>2020-02-12T02:55:45.000Z</published>
    <updated>2020-02-18T04:35:31.880Z</updated>
    
    <content type="html"><![CDATA[<p>涉及 结构化机器学习项目 。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#机器学习ml策略">机器学习（ML）策略</a><ul><li><a href="#偏差和方差的处理办法">偏差和方差的处理办法</a></li><li><a href="#误差分析">误差分析</a></li><li><a href="#迁移学习">迁移学习</a></li><li><a href="#多任务学习">多任务学习</a></li><li><a href="#端到端网络">端到端网络</a></li></ul></li></ul><!-- tocstop --><h2><span id="机器学习ml策略">机器学习（ML）策略</span></h2><p>当你尝试优化一个深度学习系统时，你通常可以有很多想法可以去试，问题在于，如果你做出了错误的选择，你完全有可能白费6个月的时间，往错误的方向前进。如果有快速有效的方法能够判断哪些想法是靠谱的，或者甚至提出新的想法，判断哪些是值得一试的想法，哪些是可以放心舍弃的。</p><p>搭建建立机器学习系统的挑战之一是，你可以尝试和改变的东西太多太多了。包括，比如说，有那么多的超参数可以调。我留意到，那些效率很高的机器学习专家有个特点，他们思维清晰，对于要调整什么来达到某个效果，非常清楚，这个步骤我们称之为<strong>正交化</strong>。正交化的意思就是使得各个特征之间互不干扰，可以单独地依次调各个特征，而不是同时要兼顾好几个。</p><p>当我训练神经网络时，我一般不用<strong>early stopping</strong>，这个技巧也还不错，很多人都这么干。但个人而言，我觉得用<strong>early stopping</strong>有点难以分析，因为这会同时影响你对训练集的拟合，因为如果你早期停止，那么对训练集的拟合就不太好，但它同时也用来改善开发集的表现，所以这个方式没那么正交化。</p>查准率（precision）、查全率（recall）、$F_1$分数，$F_1$分数的定义是这个公式：$\frac{2}{\frac{1}{P} + \frac{1}{R}}$。我们还需要考虑运行时间，可以将准确度和运行时间组合成一个整体评估指标，比如说时间必须要小于100ms，只要小于就行了。<p>在分割训练集验证集测试集的时候，需要将所有数据随机打乱，以此来做到让开发集和测试集都来自于同一分布。对于传统的数据量很小的数据，按照7:3或者6:2:2划分是相当合理的。但是对于现在超大规模的数据集，比如100w，可能按照98:1:1的比例就足够了，因为就算这样分那测试集都还有1w个数据呢，足够测试了。</p><h3><span id="偏差和方差的处理办法">偏差和方差的处理办法</span></h3><p>处理机器学习问题时，应该把它切分成独立的步骤，将定义指标看成一步，然后在定义了指标之后，你才能想如何优化系统来提高这个指标评分。比如改变你神经网络要优化的成本函数。如果你在指标上表现很好，在当前开发集或者开发集和测试集分布中表现很好，但你的实际应用程序，你真正关注的地方表现不好，那么就需要修改指标或者你的开发测试集。</p><p>贝叶斯错误率或者对贝叶斯错误率的估计和训练错误率之间的差值称为可避免偏差，你可能希望一直提高训练集表现，直到你接近贝叶斯错误率，但实际上你也不希望做到比贝叶斯错误率更好，这理论上是不可能超过贝叶斯错误率的，除非过拟合。而这个训练错误率和开发错误率之前的差值，就大概说明你的算法在方差问题上还有多少改善空间。总结一下，如果你想理解偏差和方差，那么在人类可以做得很好的任务中，你可以估计人类水平的错误率，你可以使用人类水平错误率来估计贝叶斯错误率。所以你到贝叶斯错误率估计值的差距，告诉你可避免偏差问题有多大，可避免偏差问题有多严重，而训练错误率和开发错误率之间的差值告诉你方差上的问题有多大，你的算法是否能够从训练集泛化推广到开发集。</p><p>所以我想要让一个监督学习算法达到实用，基本上希望或者假设你可以完成两件事情。首先，你的算法对训练集的拟合很好，这可以看成是你能做到可避免偏差很低。还有第二件事你可以做好的是，在训练集中做得很好，然后推广到开发集和测试集也很好，这就是说方差不是太大。</p><p>如果你想用尽一切办法减少可避免<strong>偏差</strong>，我建议试试这样的策略：比如使用规模更大的模型，这样算法在训练集上的表现会更好，或者训练更久。使用更好的优化算法，比如说加入momentum或者RMSprop，或者使用更好的算法，比如Adam。你还可以试试寻找更好的新神经网络架构，或者说更好的超参数。这些手段包罗万有，你可以改变激活函数，改变层数或者隐藏单位数，虽然你这么做可能会让模型规模变大。或者试用其他模型，其他架构，如循环神经网络和卷积神经网络。新的神经网络架构能否更好地拟合你的训练集，有时也很难预先判断，但有时换架构可能会得到好得多的结果。另外当你发现<strong>方差</strong>是个问题时，你可以试用很多技巧，包括以下这些：你可以收集更多数据，因为收集更多数据去训练可以帮你更好地推广到系统看不到的开发集数据。你可以尝试正则化，包括$L2$正则化，dropout正则化或者我们在之前课程中提到**的数据增强。同时你也可以试用不同的神经网络架构，超参数搜索，看看能不能帮助你，找到一个更适合你的问题的神经网络架构。</p><h3><span id="误差分析">误差分析</span></h3><p>当你发现你的模型效果不是很好时，应该怎么办呢？建议手动检查你的错误样例，并且根据种类统计一下比例，比如说绘制一个excel表格。如果某一种类只有极少量错误，那么工作的重心就不应该在这上面而是应该在其他上面。在机器学习中，有时候我们很鄙视手工操作，或者使用了太多人为数值。但如果你要搭建应用系统，那这个简单的人工统计步骤，错误分析，可以节省大量时间，可以迅速决定什么是最重要的，或者最有希望的方向。</p><p>所以总结一下，进行错误分析，你应该找一组错误样本，可能在你的开发集里或者测试集里，观察错误标记的样本，看看假阳性（<strong>false positives</strong>）和假阴性（<strong>false negatives</strong>），统计属于不同错误类型的错误数量。在这个过程中，你可能会得到启发，归纳出新的错误类型，就像我们看到的那样。通过统计不同错误标记类型占总数的百分比，可以帮你发现哪些问题需要优先解决，或者给你构思新优化方向的灵感。</p><p>在做错误分析的时候，有时你会注意到开发集里有些样本被错误标记了，这时应该怎么做呢？</p><p>首先，我们来考虑训练集，事实证明，深度学习算法对于训练集中的随机错误是相当健壮的（<strong>robust</strong>）。只要你的标记出错的样本，只要这些错误样本离随机错误不太远，有时可能做标记的人没有注意或者不小心，按错键了，如果错误足够随机，那么放着这些错误不管可能也没问题，而不要花太多时间修复它们。当然你浏览一下训练集，检查一下这些标签，并修正它们也没什么害处。有时候修正这些错误是有价值的，有时候放着不管也可以，只要总数据集总足够大，实际错误率可能不会太高。我这里先警告一下，深度学习算法对随机误差很健壮，但对系统性的错误就没那么健壮了。所以比如说，如果做标记的人一直把白色的狗标记成猫，那就成问题了。因为你的分类器学习之后，会把所有白色的狗都分类为猫。但随机错误或近似随机错误，对于大多数深度学习算法来说不成问题。</p><p>其次，我强烈建议你要考虑同时检验算法判断正确和判断错误的样本，要检查算法出错的样本很容易，只需要看看那些样本是否需要修正，但还有可能有些样本算法判断正确，那些也需要修正。如果你只修正算法出错的样本，你对算法的偏差估计可能会变大，这会让你的算法有一点不公平的优势，我们就需要再次检查出错的样本，但也需要再次检查做对的样本，因为算法有可能因为运气好把某个东西判断对了。这第二点不是很容易做，所以通常不会这么做。通常不会这么做的原因是，如果你的分类器很准确，那么判断错的次数比判断正确的次数要少得多。总结起来就是训练集中标错的数据影响不大，测试集和验证集中的标错数据如果有较大影响就去修正数据，否则就不用。</p><p>最后的几个建议：</p><ul><li>在构造实际系统时，通常需要更多的人工错误分析，更多的人类见解来架构这些系统，尽管深度学习的研究人员不愿意承认这点。</li><li>一些工程师和研究人员不愿意亲自去看这些样本，也许做这些事情很无聊，但实际上这很重要。花了这几分钟，或者几个小时去亲自统计数据，真的可以帮你找到需要优先处理的任务。</li><li>如果你想搭建全新的机器学习程序，你应该首先快速设立开发集和测试集还有指标，这样就决定了你的目标所在，然后找到训练集，训练一下，看看效果，开始理解你的算法表现如何，从而进行偏差方差分析、误差分析等步骤。初始系统的全部意义在于，有一个学习过的系统，有一个训练过的系统，让你确定偏差方差的范围，就可以知道下一步应该优先做什么，让你能够进行错误分析，可以观察一些错误，然后想出所有能走的方向，哪些是实际上最有希望的方向。</li><li>当出现训练集和测试集数据分布不匹配的时候，你可以通过人工合成数据等办法，向训练集里添加一些和测试集分布相同的数据。</li></ul><h3><span id="迁移学习">迁移学习</span></h3><p>有的时候神经网络可以从一个任务中习得知识，并将这些知识应用到另一个独立的任务中，这就是所谓的迁移学习。</p><p>经验规则是，如果你有一个小数据集，就只训练输出层前的最后一层，或者也许是最后一两层。但是如果你有很多数据，那么也许你可以重新训练网络中的所有参数。如果你重新训练神经网络中的所有参数，那么这个在初期训练阶段，有时称为预训练（<strong>pre-training</strong>）。如果你以后更新所有权重，这个过程叫微调（<strong>fine tuning</strong>）。预训练和微调的权重来源于迁移学习。</p><p>为什么这样做有效果呢？有很多低层次特征可以通过预训练很好的学习出来，而这些特征对很多有共通点的任务都是很有帮助的，所以这些预训练特征可能帮助你的其他任务学习得更快一些，或者需要更少的学习数据。迁移学习起作用的场合是，在<strong>迁移来源问题$A$中你有很多数据，但迁移目标问题$B$你没有那么多数据，并且二者的输入$x$输入相同</strong>。如果$A$的数据没有$B$多，这种情况下增益可能不多。</p><h3><span id="多任务学习">多任务学习</span></h3><p>在迁移学习中，你的步骤是串行的，你从任务里学习只是然后迁移到任务。在多任务学习中，你是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。</p><p><img src="data:image/png;base64,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" data-original="1.png" alt=""></p><p>比如上图中，假设有四个任务，每个任务都期望得到一个值，具体体现在输出层里画了四个圈圈。那么你现在可以做的是训练一个神经网络，来预测这些$y$值，你就得到这样的神经网络，输入$x$，现在输出是一个四维向量$y$。要训练这个神经网络，你现在需要定义神经网络的损失函数，对于一个输出$\hat y$，是个4维向量，对于整个训练集的平均损失：</p>$$\frac{1}{m}\sum_{i = 1}^{m}{\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}}$$<p>$\sum_{j = 1}^{4}{L(\hat y_{j}^{(i)},y_{j}^{(i)})}$这些表示单个任务预测的损失，$L$可以比如说用Logistic损失。</p><p>在这个场合，一张图可以有多个标签。如果你训练了一个神经网络，试图最小化这个成本函数，你做的就是多任务学习。因为你现在做的是建立单个神经网络，然后解决四个问题，然后返回给你四个任务对应的标签。另外你也可以训练四个不同的神经网络，而不是训练一个网络做四件事情。但神经网络一些早期特征，在不同任务中都会用到，然后你发现，训练一个神经网络做四件事情会比训练四个完全独立的神经网络分别做四件事性能要更好，这就是多任务学习的力量。</p><p>那么多任务学习什么时候有意义呢？当三件事为真时，它就是有意义的：</p><ul><li>第一，如果你训练的一组任务，可以共用低层次特征。</li><li>第二，这个准则没有那么绝对，所以不一定是对的。每个任务的数据量要很接近。但我通常会看的是如果你专注于单项任务，如果想要从多任务学习得到很大性能提升，那么其他任务加起来必须要有比单个任务大得多的数据量。如果对于单个任务你已经有1000个样本了，那么对于所有其他任务，你最好有超过1000个样本，这样其他任务的知识才能帮你改善这个任务的性能。</li><li>最后多任务学习往往在以下场合更有意义，当你可以训练一个足够大的神经网络，同时做好所有的工作，所以多任务学习的替代方法是为每个任务训练一个单独的神经网络。多任务学习会降低性能的唯一情况，和训练单个神经网络相比性能更低的情况就是你的神经网络还不够大。但如果你可以训练一个足够大的神经网络，那么多任务学习肯定不会或者很少会降低性能，我们都希望它可以提升性能，比单独训练神经网络来单独完成各个任务性能要更好。</li></ul><p>所以总结一下，多任务学习能让你训练一个神经网络来执行许多任务，这可以给你更高的性能，比单独完成各个任务更高的性能。但要注意，实际上迁移学习比多任务学习使用频率更高。我看到很多任务都是，如果你想解决一个机器学习问题，但你的数据集相对较小，那么迁移学习真的能帮到你，就是如果你找到一个相关问题，其中数据量要大得多，你就能以它为基础训练你的神经网络，然后迁移到这个数据量很少的任务上来。</p><h3><span id="端到端网络">端到端网络</span></h3><p>端到端学习到底是什么呢？简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它，直接从输入到输出映射。</p><p>事实证明，端到端深度学习的挑战之一是，你可能需要大量数据才能让系统表现良好，比如，你只有3000小时数据去训练你的语音识别系统，那么传统的流水线效果真的很好。但当你拥有非常大的数据集时，比如10,000小时数据或者100,000小时数据，这样端到端方法突然开始很厉害了。所以当你的数据集较小的时候，传统流水线方法其实效果也不错，通常做得更好。你需要大数据集才能让端到端方法真正发出耀眼光芒。</p><p>但是通常来说端到端并不是完全的输入到输出，而是需要先对输入进行一些处理，准确获取那些可以利用的部分，然后再利用端到端网络来进行判断。相当于，第一步找到在哪里，第二步判断是什么。为什么两步法更好呢？实际上有两个原因。第一，你解决的两个问题，每个问题实际上都比原来这整个问题要简单得多。第二，两个子任务的训练数据都很多，那么就更容易训练一个更好的网络。比如对于门禁系统，第一步人脸边缘检测，第二步人脸识别。</p><p>应用端到端学习的一些好处，首先端到端学习真的只是<strong>让数据说话</strong>。所以如果你有足够多的$(x,y)$数据，那么不管从$x$到$y$最适合的函数映射是什么，如果你训练一个足够大的神经网络，希望这个神经网络能自己搞清楚，而使用纯机器学习方法，直接从$x$到$y$输入去训练的神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。端到端深度学习的第二个好处就是，<strong>所需手工设计的组件更少</strong>，所以这也许能够简化你的设计工作流程，你不需要花太多时间去手工设计功能，手工设计这些中间表示方式。</p><p>那么缺点呢？这里有一些缺点，首先，它可能<strong>需要大量的数据</strong>。要直接学到这个$x$到$y$的映射，你可能需要大量$(x,y)$数据。其子任务可能有很多训练数据，但是整体起来的训练数据却很少。另一个缺点是，它<strong>排除了可能有用的手工设计组件</strong>。机器学习研究人员一般都很鄙视手工设计的东西，但如果你没有很多数据，你的学习算法就没办法从很小的训练集数据中获得洞察力。所以手工设计组件在这种情况，可能是把人类知识直接注入算法的途径，这总不是一件坏事。我觉得学习算法有两个主要的知识来源，一个是<strong>数据</strong>，另一个是你手工设计的任何东西（<strong>知识</strong>），可能是组件，功能，或者其他东西。所以当你有大量数据时，手工设计的东西就不太重要了，但是当你没有太多的数据时，构造一个精心设计的系统，实际上可以将人类对这个问题的很多认识直接注入到问题里，进入算法里应该挺有帮助的。</p><p>如果你在构建一个新的机器学习系统，而你在尝试决定是否使用端到端深度学习，我认为关键的问题是，你有足够的数据能够直接学到从$x$映射到$y$足够复杂的函数吗？如果你想使用机器学习或者深度学习来学习某些单独的组件，那么当你应用监督学习时，你应该仔细选择要学习的$x$到$y$映射类型，这取决于那些任务你可以收集数据。相比之下，谈论纯端到端深度学习方法是很激动人心的，但是就目前能收集到的数据而言，还有我们今天能够用神经网络学习的数据类型而言，这实际上不是最有希望的方法，或者说这个方法并不是团队想出的最好用的方法。而我认为这种纯粹的端到端深度学习方法，其实前景不如这样更复杂的多步方法。因为目前能收集到的数据，还有我们现在训练神经网络的能力是有局限的。</p><p>这与softmax回归的主要区别在于，softmax将单个标签分配给单个样本，而这里是将多个标签分配给单个样本。</p><p>【原文出自黄海广博士 <strong><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">deeplearning_ai_books</a></strong>】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;涉及 结构化机器学习项目 。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达CS230深度学习笔记（二）</title>
    <link href="http://hhu1506010220.github.io/2020/02/08/%E5%90%B4%E6%81%A9%E8%BE%BECS230%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://hhu1506010220.github.io/2020/02/08/吴恩达CS230深度学习笔记（二）/</id>
    <published>2020-02-08T02:55:45.000Z</published>
    <updated>2020-02-18T02:25:36.871Z</updated>
    
    <content type="html"><![CDATA[<p>涉及 改善深层神经网络：超参数调试、正则化以及优化 。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#深度学习的实践层面">深度学习的实践层面</a><ul><li><a href="#正则化">正则化</a></li><li><a href="#drop正则化">drop正则化</a></li><li><a href="#其他正则化方法">其他正则化方法</a></li><li><a href="#归一化输入">归一化输入</a></li><li><a href="#权重初始化">权重初始化</a></li><li><a href="#梯度检验">梯度检验</a></li></ul></li><li><a href="#优化算法">优化算法</a><ul><li><a href="#mini-batch梯度下降">mini-batch梯度下降</a></li><li><a href="#其他优化算法">其他优化算法</a></li><li><a href="#学习率衰减">学习率衰减</a></li></ul></li><li><a href="#超参数调试-batch正则化和程序框架">超参数调试、Batch正则化和程序框架</a><ul><li><a href="#调参经验">调参经验</a></li><li><a href="#batch-norm归一化">Batch Norm归一化</a></li></ul></li></ul><!-- tocstop --><h2><span id="深度学习的实践层面">深度学习的实践层面</span></h2><p>训练集、验证集、测试集。</p><p>偏差、方差：</p><ul><li><strong>低偏差，低方差</strong>：这是训练的理想模型，数据离散程度小，基本在合理范围内；</li><li><strong>低偏差，高方差</strong>：这是深度学习面临的最大问题，过拟合了。也就是模型太贴合训练数据了，导致其泛化（或通用）能力差，若遇到测试集，则准确度下降的厉害；</li><li><strong>高偏差，低方差</strong>：这往往是训练的初始阶段；</li><li><strong>高偏差，高方差</strong>：这是训练最糟糕的情况，准确度差，数据的离散程度也差。</li></ul><p>减少偏差的方法，比如训练更大的神经网络，或者跑久一点梯度下降。减少方差的方法，正则化。</p><h3><span id="正则化">正则化</span></h3><p>深度学习可能存在过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备足够多的训练数据或者获取更多数据的成本很高，但正则化通常有助于避免过拟合或减少你的网络误差。</p>$\frac{\lambda}{2m}​$乘以$w​$范数的平方，其中 $\left| w \right|*2^2​$ 是$w​$的欧几里德范数的平方，等于$w*{j}​$（$j​$ 值从1到$n_{x}​$）平方的和，也可表示为$w^{T}w​$，也就是向量参数$w​$ 的欧几里德范数（2范数）的平方，此方法称为$L2​$正则化，因为这里用了欧几里德范数，被称为向量参数$w​$的$L2​$范数。<p>为什么只正则化参数$w$？为什么不再加上参数 $b$ 呢？你可以这么做，只是我习惯省略不写，因为$w$通常是一个高维参数矢量，已经可以表达高偏差问题，$w$可能包含有很多参数，我们不可能拟合所有参数，而$b$只是单个数字，所以$w$几乎涵盖所有参数，而不是$b$，如果加了参数$b$，其实也没太大影响，因为$b$只是众多参数中的一个，所以我通常省略不计，如果你想加上这个参数，完全没问题。</p>$L2$正则化是最常见的正则化类型，你们可能听说过$L1$正则化，$L1$正则化，加的不是$L2$范数，而是正则项$\frac{\lambda}{m}$乘以$\sum_{j= 1}^{n_{x}}{|w|}$，$\sum_{j =1}^{n_{x}}{|w|}$也被称为参数$w$向量的$L1$范数，无论分母是$m$还是$2m$，它都是一个比例常量。<p>如果用的是$L1$正则化，$w$最终会是稀疏的，也就是说$w$向量中有很多0，有人说这样有利于压缩模型，因为集合中参数均为0，存储模型所占用的内存更少。实际上，虽然$L1$正则化使模型变得稀疏，却没有降低太多存储内存。人们在训练网络时，越来越倾向于使用$L2$正则化。</p><p>如何在神经网络中实现$L2$正则化呢？</p><p><img src="data:image/png;base64,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" data-original="1.png" alt=""><br>神经网络含有一个成本函数，该函数包含$W^{[1]}$，$b^{[1]}$到$W^{[l]}$，$b^{[l]}$所有参数，字母$L$是神经网络所含的层数，因此成本函数等于$m$个训练样本损失函数的总和乘以$\frac{1}{m}$，正则项为$\frac{\lambda }{2m}{{\sum\nolimits_{1}^{L}{| {{W}^{[l]}}|}}^{2}}$，我们称${||W^{\left[l\right]}||}^{2}$为范数平方，这个矩阵范数${||W^{\left[l\right]}||}^{2}$（即平方范数），被定义为矩阵中所有元素的平方求和（矩阵F范数）。</p><p>从而偏导数变成了下面这个表达式：</p><p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p><p>该正则项说明，不论$W^{[l]}$是什么，我们都试图让它变得更小，实际上，相当于我们给矩阵W乘以$(1 - \alpha\frac{\lambda}{m})$倍的权重，矩阵$W$减去$\alpha\frac{\lambda}{m}$倍的它，也就是用这个系数$(1-\alpha\frac{\lambda}{m})$乘以矩阵$W$，该系数小于1，因此$L2$范数正则化也被称为“权重衰减”。</p><p>直观理解就是$\lambda$增加到足够大，$W$会接近于0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。拿tanh(z)函数举例，如果正则化参数变得很大，参数$W$很小，$z$也会相对变小，此时忽略$b$的影响，$z$会相对变小，实际上，$z$的取值范围很小，这个激活函数，也就是曲线函数$tanh$会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><h3><span id="drop正则化">drop正则化</span></h3><p>假设你在训练下图这样的神经网络，它存在过拟合，这就是<strong>dropout</strong>所要处理的，我们复制这个神经网络，<strong>dropout</strong>会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用<strong>backprop</strong>方法进行训练。这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。</p><p><img src="data:image/png;base64,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" data-original="3.jpg" alt=""></p><p>如何实施<strong>dropout</strong>呢？方法有几种，接下来我要讲的是最常用的方法，即<strong>inverted dropout</strong>（反向随机失活），出于完整性考虑，我们用一个三层（$l=3$）网络来举例说明。编码中会有很多涉及到3的地方。我只举例说明如何在某一层中实施<strong>dropout</strong>。</p><p>定义向量$d$，$d^{[3]}$表示网络第三层的<strong>dropout</strong>向量。然后看它是否小于某数，我们称之为<strong>keep-prob</strong>，它表示保留某个隐藏单元的概率。然后用激活函数a3乘以d3，<code>a3 =np.multiply(a3,d3)</code>，这里是元素相乘，也可写为$a3<em>=d3$，相当于a3中乘以0的部分清零。然后再除以<em>*keep-prob</em></em>（这样做的目的是为了确保a3的期望值不变）。</p><p>据我了解，目前实施<strong>dropout</strong>最常用的方法就是<strong>Inverted dropout</strong>，建议大家动手实践一下。<strong>Dropout</strong>早期的迭代版本都没有除以<strong>keep-prob</strong>，所以在测试阶段，平均值会变得越来越复杂，不过那些版本已经不再使用了。<strong>Inverted dropout</strong>函数在除以<strong>keep-prob</strong>时可以记住上一步的操作，目的是确保即使在测试阶段不执行<strong>dropout</strong>来调整数值范围，激活函数的预期结果也不会发生变化，所以没必要在测试阶段额外添加尺度参数，这与训练阶段不同。（也就是说dropout用在训练阶段，不用在测试阶段）</p><p><strong>Dropout</strong>可以随机删除网络中的神经单元，他为什么可以通过正则化发挥如此大的作用呢？</p><p>直观上理解：不要依赖于任何一个特征，因为该单元的输入可能随时被清除。<strong>我不愿意把所有赌注都放在一个节点上</strong>。因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，<strong>dropout</strong>将产生收缩权重的平方范数的效果，和之前讲的$L2$正则化类似；实施<strong>dropout</strong>的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；$L2$对不同权重的衰减是不同的，它取决于激活函数倍增的大小。因此，<strong>dropout</strong>的功能类似于$L2$正则化，与$L2$正则化不同的是应用方式不同会带来一点点小变化，甚至更适用于不同的输入范围。</p><p>如果你担心某些层比其它层更容易发生过拟合，可以把某些层的<strong>keep-prob</strong>值设置得比其它层更低，缺点是为了使用交叉验证，你要搜索更多的超级参数，另一种方案是在一些层上应用<strong>dropout</strong>，而有些层不用<strong>dropout</strong>，应用<strong>dropout</strong>的层只含有一个超级参数，就是<strong>keep-prob</strong>。要牢记一点，<strong>dropout</strong>是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用<strong>dropout</strong>的。</p><p><strong>dropout</strong>一大缺点就是代价函数$J$不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。定义明确的代价函数$J$每次迭代后都会下降，因为我们所优化的代价函数$J$实际上并没有明确定义，或者说在某种程度上很难计算，所以我们失去了调试工具来绘制这样的图片。我通常会关闭<strong>dropout</strong>函数，将<strong>keep-prob</strong>的值设为1，运行代码，确保J函数单调递减。然后打开<strong>dropout</strong>函数，希望在<strong>dropout</strong>过程中，代码并未引入<strong>bug</strong>。我觉得你也可以尝试其它方法，虽然我们并没有关于这些方法性能的数据统计，但你可以把它们与<strong>dropout</strong>方法一起使用。</p><h3><span id="其他正则化方法">其他正则化方法</span></h3><ul><li><strong>数据扩增</strong>。对已有图片再加工。</li><li><strong>early stopping</strong>。<strong>early stopping</strong>的作用是，你会说，神经网络已经在这个迭代过程中表现得很好了，我们在此停止训练吧，要做就是在中间点停止迭代过程。<strong>early stopping</strong>的主要缺点就是你不能独立地处理<strong>代价函数尽可能小</strong>和<strong>防止过拟合</strong>这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数$J$，因为现在你不再尝试降低代价函数$J$，所以代价函数$J$的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。<strong>Early stopping</strong>的优点是，只运行一次梯度下降，你可以找出$w$的较小值，中间值和较大值，而无需尝试$L2$正则化超级参数$\lambda$的很多值。如果不用<strong>early stopping</strong>，另一种方法就是$L2$正则化，训练神经网络的时间就可能很长。</li></ul><p>虽然$L2$正则化有缺点，可还是有很多人愿意用它。吴恩达老师个人更倾向于使用$L2$正则化，尝试许多不同的$\lambda$值，假设你可以负担大量计算的代价。而使用<strong>early stopping</strong>也能得到相似结果，还不用尝试这么多$\lambda$值。</p><h3><span id="归一化输入">归一化输入</span></h3><p>训练神经网络，其中一个加速训练的方法就是归一化输入。假设一个训练集有两个特征，输入特征为2维，归一化需要两个步骤：</p><ol><li><p>零均值</p></li><li><p>归一化方差；</p><p>我们希望无论是训练集和测试集都是通过相同的$μ$和$σ^2$定义的数据转换，这两个是由训练集得出来的。</p></li></ol><p><img src="data:image/png;base64,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" data-original="4.png" alt=""></p><p>第一步是零均值化，$\mu = \frac{1}{m}\sum_{i =1}^{m}x^{(i)}$，它是一个向量，$x$等于每个训练数据 $x$减去$\mu$，意思是移动训练集，直到它完成零均值化。</p>第二步是归一化方差，注意特征$x_{1}$的方差比特征$x_{2}$的方差要大得多，我们要做的是给$\sigma$赋值，$\sigma^{2}= \frac{1}{m}\sum_{i =1}^{m}{({x^{(i)})}^{2}}$，这是节点$y$ 的平方，$\sigma^{2}$是一个向量，它的每个特征都有方差，注意，我们已经完成零值均化，$({x^{(i)})}^{2}$元素$y^{2}$就是方差，我们把所有数据除以向量$\sigma^{2}$。<p>提示一下，如果你用它来调整训练数据，那么用相同的 $μ$ 和 $\sigma^{2}$来归一化测试集。尤其是，你不希望训练集和测试集的归一化有所不同，不论$μ$的值是什么，也不论$\sigma^{2}$的值是什么，这两个公式中都会用到它们。所以你要用同样的方法调整测试集，而不是在训练集和测试集上分别预估$μ$ 和 $\sigma^{2}$。</p><p>为什么要进行归一化呢？原因是因为非归一化特征的代价函数可能是非常狭长的，非常不均匀，假如$x_{1}$取值范围从1到1000，特征$x_{2}$的取值范围从0到1，结果是参数$w_{1}$和$w_{2}$值的范围或比率将会非常不同。然而如果你归一化特征，代价函数平均起来看更对称，如果你在上图这样的代价函数上运行梯度下降法，你必须使用一个非常小的学习率。因为如果是在这个位置，梯度下降法可能需要多次迭代过程，直到最后找到最小值。但如果函数是一个更圆的球形轮廓，那么不论从哪个位置开始，梯度下降法都能够更直接地找到最小值，你可以在梯度下降法中使用较大步长，而不需要像在左图中那样反复执行。<strong>归一化可以清洗数据分布</strong>。</p><p><img src="data:image/png;base64,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" data-original="5.png" alt=""></p><h3><span id="权重初始化">权重初始化</span></h3><p>训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。如果激活函数的输入特征被零均值和标准方差化，方差是1，$z$也会调整到相似范围，这就没解决问题（梯度消失和爆炸问题）。但它确实降低了梯度消失和爆炸问题，因为它给权重矩阵$w$设置了合理值，你也知道，它不能比1大很多，也不能比1小很多，所以梯度没有爆炸或消失过快。</p><h3><span id="梯度检验">梯度检验</span></h3>首先，不要在训练中使用梯度检验，它只用于调试。我的意思是，计算所有$i$值的$d\theta_{\text{approx}}\left[i\right]$是一个非常漫长的计算过程。第二点，如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出bug，也就是说，如果$d\theta_{\text{approx}}\left[i\right]$与dθ[i]的值相差很大，我们要做的就是查找不同的i值，看看是哪个导致$d\theta_{\text{approx}}\left[i\right]$与$d\theta\left[i\right]$的值相差这么多。可能帮你定位bug的位置，虽然未必能够帮你准确定位bug的位置，但它可以帮助你估测需要在哪些地方追踪bug。<p>第三点，在实施梯度检验时，如果使用正则化，请注意正则项。如果代价函数$J(\theta) = \frac{1}{m}\sum_{}^{}{L(\hat y^{(i)},y^{(i)})} + \frac{\lambda}{2m}\sum_{}^{}{||W^{[l]}||}^{2}$，这就是代价函数$J$的定义，$d\theta$等于与$\theta$相关的$J$函数的梯度，包括这个正则项，记住一定要包括这个正则项。</p><p>第四点，梯度检验不能与<strong>dropout</strong>同时使用，因为每次迭代过程中，<strong>dropout</strong>会随机消除隐藏层单元的不同子集，难以计算<strong>dropout</strong>在梯度下降上的代价函数$J$。因此<strong>dropout</strong>可作为优化代价函数$J$的一种方法，但是代价函数J被定义为对所有指数极大的节点子集求和。而在任何迭代过程中，这些节点都有可能被消除，所以很难计算代价函数$J$。</p><p>俺没用过梯度检验…这里只是看看罢了</p><h2><span id="优化算法">优化算法</span></h2><h3><span id="mini-batch梯度下降">mini-batch梯度下降</span></h3><p>使用<strong>batch</strong>梯度下降法，一次遍历训练集只能让你做一个梯度下降，使用<strong>mini-batch</strong>梯度下降法，一次遍历训练集，能让你做更多的梯度下降。当然正常来说你想要多次遍历训练集，还需要为另一个<strong>while</strong>循环设置另一个<strong>for</strong>循环。所以你可以一直处理遍历训练集，直到最后你能收敛到一个合适的精度。如果你有一个丢失的训练集，<strong>mini-batch</strong>梯度下降法比<strong>batch</strong>梯度下降法运行地更快，所以几乎每个研习深度学习的人在训练巨大的数据集时都会用到。</p>当mini-batch大小为1，就有了新的算法，叫做随机梯度下降法，每个样本都是独立的mini-batch，当你看第一个mini-batch，也就是$X^{{1}}$和$Y^{{1}}$，如果mini-batch大小为1，它就是你的第一个训练样本，这就是你的第一个训练样本。接着再看第二个mini-batch，也就是第二个训练样本，采取梯度下降步骤，然后是第三个训练样本，以此类推，一次只处理一个。随机梯度下降法的一大缺点是，你会失去所有向量化带给你的加速，因为一次性只处理了一个训练样本，这样效率过于低下，所以实践中最好选择不大不小的mini-batch尺寸，实际上学习率达到最快。<p>你会发现两个好处，一方面，你得到了大量向量化，比你一次性处理多个样本快得多。另一方面，你不需要等待整个训练集被处理完就可以开始进行后续工作。</p><p>如果训练集较小，直接使用<strong>batch</strong>梯度下降法，样本集较小就没必要使用<strong>mini-batch</strong>梯度下降法，你可以快速处理整个训练集，所以使用<strong>batch</strong>梯度下降法也很好，这里的少是说小于2000个样本，这样比较适合使用<strong>batch</strong>梯度下降法。不然，样本数目较大的话，一般的<strong>mini-batch</strong>大小为64到512，考虑到电脑内存设置和使用的方式，如果<strong>mini-batch</strong>大小是2的次方，代码会运行地快一些。</p><h3><span id="其他优化算法">其他优化算法</span></h3><p>动量梯度下降（<strong>Momentum</strong>）</p><p>均方根传播（<strong>RMSprop</strong>）</p><p><strong>Adam</strong>优化算法，基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起,被证明能有效适用于不同神经网络，适用于广泛的结构。</p><p><img src="data:image/png;base64,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" data-original="6.png" alt=""></p><p>本算法中有很多超参数，超参数学习率$a$很重要，也经常需要调试，你可以尝试一系列值，然后看哪个有效。$\beta_{1}$常用的缺省值为0.9，这是dW的移动平均数，也就是$dW$的加权平均数，这是<strong>Momentum</strong>涉及的项。至于超参数$\beta_{2}$，<strong>Adam</strong>论文作者，也就是<strong>Adam</strong>算法的发明者，推荐使用0.999，这是在计算${(dW)}^{2}$以及${(db)}^{2}$的移动加权平均值，关于$\varepsilon$的选择其实没那么重要，<strong>Adam</strong>论文的作者建议$\varepsilon$为$10^{-8}$，但你并不需要设置它，因为它并不会影响算法表现。但是在使用<strong>Adam</strong>的时候，人们往往使用缺省值即可，$\beta_{1}$，$\beta_{2}$和$\varepsilon$都是如此，我觉得没人会去调整$\varepsilon$，然后尝试不同的$a$值，看看哪个效果最好。你也可以调整$\beta_{1}$和$\beta_{2}$，但我认识的业内人士很少这么干。</p><h3><span id="学习率衰减">学习率衰减</span></h3><p>加快学习算法的一个办法就是随时间慢慢减少学习率，我们将之称为学习率衰减。</p><p><img src="data:image/png;base64,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" data-original="7.png" alt=""></p><p>要慢慢减少学习率$a$的话，在初期的时候，$a$学习率还较大，你的学习还是相对较快，但随着$a$变小，你的步伐也会变慢变小，所以最后你的曲线（绿色线）会在最小值附近的一小块区域里摆动，而不是在训练过程中，大幅度在最小值附近摆动。</p>一代要遍历一次数据，你可以拆分成不同的mini-batch，第一次遍历训练集叫做第一代。第二次就是第二代，依此类推，你可以将$a$学习率设为$a= \frac{1}{1 + decayrate * \text{epoch}\text{-num}}a_{0}$（decay-rate称为衰减率，epoch-num为代数，$\alpha_{0}$为初始学习率），注意这个衰减率是另一个你需要调整的超参数。还有其他的一些公式，比如，这个叫做指数衰减，其中$a$相当于一个小于1的值，如$a ={0.95}^{\text{epoch-num}} a_{0}$，所以你的学习率呈指数下降。人们用到的其它公式有$a =\frac{k}{\sqrt{\text{epoch-num}}}a_{0}$或者$a =\frac{k}{\sqrt{t}}a_{0}$（$t$为mini-batch的数字）。<p>有时人们也会用一个离散下降的学习率，也就是某个步骤有某个学习率，一会之后，学习率减少了一半，一会儿减少一半，一会儿又一半，这就是离散下降（<strong>discrete stair cease</strong>）的意思。</p><h2><span id="超参数调试-batch正则化和程序框架">超参数调试、Batch正则化和程序框架</span></h2><h3><span id="调参经验">调参经验</span></h3><p>结果证实一些超参数比其它的更为重要，我认为，最为广泛的学习应用是$a$，学习速率是需要调试的最重要的超参数。第二个是<strong>mini-batch</strong>的大小，以确保最优算法运行有效。我还会经常调试<strong>隐藏单元个数</strong>，我用橙色圈住的这些，这三个是我觉得其次比较重要的，相对而言。层数有时会产生很大的影响，学习率衰减也是如此。</p><p><img src="data:image/png;base64,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" data-original="8.png" alt=""></p><p>希望你粗略了解到哪些超参数较为重要，无疑是最重要的，接下来是我用橙色圈住的那些，然后是我用紫色圈住的那些。</p><p>对于调参，在深度学习领域，我们常做的，我推荐你采用下面的做法，随机选择点，不是取每个值，而是取一些有特点的值。另一个惯例是采用由粗糙到精细的策略，先确定一个大致的范围，然后细致搜索。</p><p><img src="data:image/png;base64,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" data-original="9.png" alt=""></p><p>如何调参，主要分两种方式：这是由你拥有的计算资源决定的。如果你拥有足够的计算机去平行试验许多模型，那绝对采用右边这种同时训练好几个模型的方式，尝试许多不同的超参数，看效果怎么样。但在一些应用领域，比如在线广告设置和计算机视觉应用领域，那里的数据太多了，你需要试验大量的模型，所以同时试验大量的模型是很困难的，所以这个时候你就只能逮住一个模型死调。</p><h3><span id="batch-norm归一化">Batch Norm归一化</span></h3>当训练一个模型，比如logistic回归时，你也许会记得，归一化输入特征可以加快学习过程。你计算了平均值，从训练集中减去平均值，计算了方差，接着根据方差归一化你的数据集，在之前的视频中我们看到，这是如何把学习问题的轮廓，从很长的东西，变成更圆的东西，更易于算法优化。所以这是有效的，对logistic回归和神经网络的归一化输入特征值而言。但是对于更深的网络，每层的输入特征不是原始特征了，而是一个个激活值 $a$，那该怎么办呢能不能归一呢？。实践中，经常做的是归一化$z^{[i]}$。Batch归一化的做法是将$z^{[i]}$值进行Batch归一化，简称BN，此过程将由${\beta}^{[i]}$和$\gamma^{[i]}$两参数控制，这一操作（计算均值和方差，减去均值，再除以方差）会给你一个新的规范化的$z^{[i]}$值（${\tilde{z}}^{[i]}$），然后将其输入激活函数中得到$a^{[i]}$，即$a^{[i]} = g^{[i]}({\tilde{z}}^{[ 1]})$。也就是把另一些参数加入到此新网络中${\beta}^{[1]}$，${\beta}^{[2]}$，$\gamma^{[1]}$，$\gamma^{[2]}$等等。<p>实践中，通常可以直接调用框架的封装函数来实现BN。<strong>batch</strong>归一化通常和训练集的<strong>mini-batch</strong>一起使用。</p><p><strong>Batch</strong>归一化的作用是它适用的归一化过程，不只是输入层，甚至同样适用于神经网络中的深度隐藏层。你应用<strong>Batch</strong>归一化了一些隐藏单元值中的平均值和方差，不过训练输入和这些隐藏单元值的一个区别是，你也许不想隐藏单元值必须是平均值0和方差1。</p><p>也许另一个轻微非直观的效果是，BN可以带了一些轻微的正则化作用。如果你应用了较大的<strong>mini-batch</strong>，对，比如说，你用了512而不是64，通过应用较大的<strong>min-batch</strong>，你减少了噪音，因此减少了正则化效果，这是<strong>dropout</strong>的一个奇怪的性质，就是应用较大的<strong>mini-batch</strong>可以减少正则化效果。但是不要把<strong>Batch</strong>归一化当作正则化，把它当作将你归一化隐藏单元激活值并加速学习的方式，我认为正则化几乎是一个意想不到的副作用。另外，<strong>Batch</strong>归一化一次只能处理一个<strong>mini-batch</strong>数据，它在<strong>mini-batch</strong>上计算均值和方差。</p><p>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布；若对神经网络每一层做归一化，会使每一层输出为标准正太分布，会使神经网络完全学习不到特征。</p><p>在测试时咋整呢？你可能需要逐一处理样本，方法是根据你的训练集估算$\mu$和$\sigma^{2}$，估算的方式有很多种，理论上你可以在最终的网络中运行整个训练集来得到$\mu$和$\sigma^{2}$，但在实际操作中，我们通常运用指数加权平均来追踪在训练过程中你看到的$\mu$和$\sigma^{2}$的值。还可以用指数加权平均，有时也叫做流动平均来粗略估算$\mu$和$\sigma^{2}$，然后在测试中使用$\mu$和$\sigma^{2}$的值来进行你所需要的隐藏单元$z$值的调整。在实践中，不管你用什么方式估算$\mu$和$\sigma^{2}$，这套过程都是比较稳健的。而且如果你使用的是某种深度学习框架，通常会有默认的估算$\mu$和$\sigma^{2}$的方式，应该一样会起到比较好的效果。但在实践中，任何合理的估算你的隐藏单元$z$值的均值和方差的方式，在测试中应该都会有效。</p><p>【原文出自黄海广博士 <strong><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">deeplearning_ai_books</a></strong>】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;涉及 改善深层神经网络：超参数调试、正则化以及优化 。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>吴恩达CS230深度学习笔记（一）</title>
    <link href="http://hhu1506010220.github.io/2020/02/07/%E5%90%B4%E6%81%A9%E8%BE%BECS230%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://hhu1506010220.github.io/2020/02/07/吴恩达CS230深度学习笔记（一）/</id>
    <published>2020-02-07T08:17:43.000Z</published>
    <updated>2020-02-17T09:35:53.363Z</updated>
    
    <content type="html"><![CDATA[<p>涉及 基本的神经网络和深度学习的概念。<br><a id="more"></a></p><!-- toc --><ul><li><a href="#深度学习引言">深度学习引言</a></li><li><a href="#神经网络的编程基础">神经网络的编程基础</a><ul><li><a href="#计算图">计算图</a></li><li><a href="#m个样本的梯度下降">m个样本的梯度下降</a></li><li><a href="#广播机制">广播机制</a></li><li><a href="#logistic损失函数的解释">Logistic损失函数的解释</a></li></ul></li><li><a href="#浅层神经网络">浅层神经网络</a><ul><li><a href="#激活函数">激活函数</a></li><li><a href="#激活函数的导数">激活函数的导数</a></li><li><a href="#反向传播">反向传播</a></li></ul></li><li><a href="#深度神经网络">深度神经网络</a></li></ul><!-- tocstop --><h2><span id="深度学习引言">深度学习引言</span></h2><p>神经网络展现出的是，如果你训练一个小型的神经网络，那么这个性能可能会像下图黄色曲线表示那样；如果你训练一个稍微大一点的神经网络，比如说一个中等规模的神经网络（下图蓝色曲线），它在某些数据上面的性能也会更好一些；如果你训练一个非常大的神经网络，它就会变成下图绿色曲线那样，并且保持变得越来越好。因此可以注意到两点：如果你想要获得较高的性能体现，那么你有两个条件要完成，第一个是你需要训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点，另外你需要能画到$x$轴的这个位置，所以你需要很多的数据。因此我们经常说规模一直在推动深度学习的进步，这里的规模指的也同时是神经网络的规模，我们需要一个带有许多隐藏单元的神经网络，也有许多的参数及关联性，就如同需要大规模的数据一样。事实上如今最可靠的方法来在神经网络上获得更好的性能，往往就是<strong>要么训练一个更大的神经网络，要么投入更多的数据</strong>，这只能在一定程度上起作用，因为最终你耗尽了数据，或者最终你的网络是如此大规模导致将要用太久的时间去训练，但是仅仅提升规模的的确确地让我们在深度学习的世界中摸索了很多时间。</p><p><img src="data:image/png;base64,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" data-original="1.png" alt=""></p><p>作为一个具体的例子，神经网络方面的一个巨大突破是从<strong>sigmoid</strong>函数转换到一个<strong>ReLU</strong>函数，这个函数我们在之前的课程里提到过。</p><p>可以知道的一个使用<strong>sigmoid</strong>函数和机器学习问题是，在这个区域，也就是这个<strong>sigmoid</strong>函数的梯度会接近零，所以学习的速度会变得非常缓慢，因为当你实现梯度下降以及梯度接近零的时候，参数会更新的很慢，所以学习的速率也会变的很慢，而通过改变这个被叫做激活函数的东西，神经网络换用这一个函数，叫做<strong>ReLU</strong>的函数（修正线性单元），<strong>ReLU</strong>它的梯度对于所有输入的负值都是零，因此梯度更加不会趋向逐渐减少到零。而这里的梯度，这条线的斜率在这左边是零，仅仅通过将<strong>Sigmod</strong>函数转换成<strong>ReLU</strong>函数，便能够使得一个叫做梯度下降（<strong>gradient descent</strong>）的算法运行的更快，这就是一个或许相对比较简单的算法创新的例子。但是根本上算法创新所带来的影响，实际上是对计算带来的优化，所以有很多像这样的例子，我们通过改变算法，使得代码运行的更快，这也使得我们能够训练规模更大的神经网络，或者是多端口的网络。即使我们从所有的数据中拥有了大规模的神经网络，快速计算显得更加重要的另一个原因是，训练你的神经网络的过程，很多时候是凭借直觉的，往往你对神经网络架构有了一个想法，于是你尝试写代码实现你的想法，然后让你运行一个试验环境来告诉你，你的神经网络效果有多好，通过参考这个结果再返回去修改你的神经网络里面的一些细节，然后你不断的重复上面的操作，当你的神经网络需要很长时间去训练，需要很长时间重复这一循环。</p><h2><span id="神经网络的编程基础">神经网络的编程基础</span></h2><p>二分类、逻辑回归、逻辑回归的代价函数、梯度下降法、导数、更多导数例子，已经在机器学习课程中学过，不再赘述。</p><h3><span id="计算图">计算图</span></h3><p>将计算的过程形式化为一个流程图。<img src="data:image/png;base64,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" data-original="2.png" alt=""></p><p>下面用到的公式：</p><script type="math/tex; mode=display">\frac{dJ}{du}=\frac{dJ}{dv}\frac{dv}{du} ， \frac{dJ}{db}=\frac{dJ}{du}\frac{du}{db} ， \frac{dJ}{da}=\frac{dJ}{du}\frac{du}{da}</script>假设你要计算$\frac{{dJ}}{{dv}}$，那要怎么算呢？好，比如说，我们要把这个$v$值拿过来，改变一下，那么$J$的值会怎么变呢？所以定义上$J = 3v$，现在$v=11$，所以如果你让$v$增加一点点，比如到11.001，那么$J =3v =33.003$，所以我这里$v$增加了0.001，然后最终结果是$J$上升到原来的3倍，所以$\frac{{dJ}}{{dv}}=3$，因为对于任何 $v$ 的增量$J$都会有3倍增量，而且这类似于我们在上一个视频中的例子，我们有$f(a)=3a$，然后我们推导出$\frac{{df}(a)}{{da}}= 3$，所以这里我们有$J=3v$，所以$\frac{{dJ}}{{dv}} =3$，这里$J$扮演了$f$的角色，在之前的视频里的例子。<p>在反向传播算法中的术语，我们看到，如果你想计算最后输出变量的导数，使用你最关心的变量对的导数，那么我们就做完了一步反向传播，在这个流程图中是一个反向步骤。<br>我们来看另一个例子，$\frac{{dJ}}{da}$是多少呢？换句话说，如果我们提高$a$的数值，对$J$的数值有什么影响？</p><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><p>好，我们看看这个例子。变量$a=5$，我们让它增加到5.001，那么对v的影响就是$a+u$，之前$v=11$，现在变成11.001，我们从上面看到现在$J$ 就变成33.003了，所以我们看到的是，如果你让$a$增加0.001，$J$增加0.003。那么增加$a$，我是说如果你把这个5换成某个新值，那么$a$的改变量就会传播到流程图的最右，所以$J$最后是33.003。所以J的增量是3乘以$a$的增量，意味着这个导数是3。要解释这个计算过程，其中一种方式是：如果你改变了$a$，那么也会改变$v$，通过改变$v$，也会改变$J$，所以$J$值的净变化量，当你提升这个值（0.001），当你把$a$值提高一点点，这就是$J$的变化量（0.003）。</p><h3><span id="m个样本的梯度下降">m个样本的梯度下降</span></h3><p>首先，让我们时刻记住有关于损失函数$J(w,b)$的定义。<br>$$J(w,b)=\frac{1}{m}\sum\limits_{i=1}^{m}{L({{a}^{(i)}},{{y}^{(i)}})}$$</p>当你的算法输出关于样本$y$的${{a}^{(i)}}$，${{a}^{(i)}}$是训练样本的预测值，即：$\sigma ( {{z}^{(i)}})=\sigma( {{w}^{T}}{{x}^{\left( i \right)}}+b)$。 所以我们在前面的幻灯中展示的是对于任意单个训练样本，如何计算微分当你只有一个训练样本。因此$d{{w}*{1}}$，$d{{w}*{\text{2}}}$和$db$ 添上上标$i$表示你求得的相应的值。如果你面对的是我们在之前的幻灯中演示的那种情况，但只使用了一个训练样本$({{x}^{(i)}},{{y}^{(i)}})$。 现在你知道带有求和的全局代价函数，实际上是1到$m$项各个损失的平均。 所以它表明全局代价函数对${{w}*{1}}$的微分，对${{w}*{1}}$的微分也同样是各项损失对${{w}_{1}}$微分的平均。<p><img src="data:image/png;base64,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" data-original="4.png" alt=""></p><p>循环的方式 - 代码流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">J=<span class="number">0</span>;dw1=<span class="number">0</span>;dw2=<span class="number">0</span>;db=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(<span class="number">1</span>-y(i)）log(<span class="number">1</span>-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J/= m;</span><br><span class="line">dw1/= m;</span><br><span class="line">dw2/= m;</span><br><span class="line">db/= m;</span><br><span class="line">w=w-alpha*dw</span><br><span class="line">b=b-alpha*db</span><br></pre></td></tr></table></figure>但这种计算中有两个缺点，也就是说应用此方法在逻辑回归上你需要编写两个for循环。第一个for循环是一个小循环遍历$m$个训练样本，第二个for循环是一个遍历所有特征的for循环。这个例子中我们只有2个特征，所以$n$等于2并且${{n}*{x}}$ 等于2。 但如果你有更多特征，你开始编写你的因此$d{{w}*{1}}$，$d{{w}*{2}}$，你有相似的计算从$d{{w}*{3}}$一直下去到$d{{w}_{n}}$。所以看来你需要一个for循环遍历所有$n$个特征。所以这里有一些叫做向量化技术,它可以允许你的代码摆脱这些显式的for循环。<p><strong>向量化</strong>：把for循环改成向量，矩阵运算从而加快计算速度。<strong>CPU</strong>和<strong>GPU</strong>都有并行化的指令，他们有时候会叫做<strong>SIMD</strong>指令，这个代表了一个单独指令多维数据，这个的基础意义是，如果你使用了<strong>built-in</strong>函数,像<code>np.function</code>或者并不要求你实现循环的函数，它可以让<strong>python</strong>的充分利用并行化计算，这是事实在<strong>GPU</strong>和<strong>CPU</strong>上面计算，<strong>GPU</strong>更加擅长<strong>SIMD</strong>计算，但是<strong>CPU</strong>事实上也不是太差，可能没有<strong>GPU</strong>那么擅长吧。</p><p>再使用向量化实现逻辑回归就可以写成：<br>$$Z = w^{T}X + b = np.dot( w.T,X)+b$$$$A = \sigma( Z )$$$$dZ = A - Y$$$${{dw} = \frac{1}{m}*X*dz^{T}\ }$$$$db= \frac{1}{m}*np.sum( dZ)$$$$w: = w - a*dw$$$$b: = b - a*db$$</p><h3><span id="广播机制">广播机制</span></h3><p>$Z=np.dot(w.T,X)+b$的计算在<strong>Python</strong>中有一个巧妙的地方，这里 $b$ 是一个实数，或者你可以说是一个 $1\times 1$ 矩阵，只是一个普通的实数。但是当你将这个向量加上这个实数时，<strong>Python</strong>自动把这个实数 $b$ 扩展成一个 $1\times m$ 的行向量。所以这种情况下的操作似乎有点不可思议，它在<strong>Python</strong>中被称作广播(<strong>brosdcasting</strong>)。<strong>参数axis用来指明将要进行的运算是沿着哪个轴执行，在numpy中，0轴是垂直的，也就是列，而1轴是水平的，也就是行。</strong></p><p>numpy的广播机制中，<strong>如果两个数组的后缘维度（shape[-1]）的轴长度相符或其中一方的轴长度为1，则认为它们是广播兼容的。广播会在缺失维度和轴长度为1的维度上进行。</strong></p><p><img src="data:image/png;base64,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" data-original="5.png" alt=""></p><p>举例：<br>矩阵 $A_{m,n}$ 和矩阵 $B_{1,n}$ 进行四则运算，后缘维度轴长度相符，可以广播，广播沿着轴长度为1的轴进行，即 $B_{1,n}$ 广播成为 ${B_{m,n}}'$ ，之后做逐元素四则运算。</p>矩阵 $A_{m,n}$ 和矩阵 $B_{m,1}$ 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着轴长度为1的轴进行，即 $B_{m,1}$ 广播成为 ${B_{m,n}}'$ ，之后做逐元素四则运算。矩阵 $A_{m,1}$ 和常数$ R$ 进行四则运算，后缘维度轴长度不相符，但其中一方轴长度为1，可以广播，广播沿着缺失维度和轴长度为1的轴进行，缺失维度就是`axis=0`,轴长度为1的轴是`axis=1`，即$R$广播成为 ${B_{m,1}}'$ ，之后做逐元素四则运算。<p><strong>Python</strong>的特性允许你使用广播（<strong>broadcasting</strong>）功能，这是<strong>Python</strong>的<strong>numpy</strong>程序语言库中最灵活的地方。而我认为这是程序语言的优点，也是缺点。优点的原因在于它们创造出语言的表达性，<strong>Python</strong>语言巨大的灵活性使得你仅仅通过一行代码就能做很多事情。但是这也是缺点，由于广播巨大的灵活性，有时候你对于广播的特点以及广播的工作原理这些细节不熟悉的话，你可能会产生很细微或者看起来很奇怪的<strong>bug</strong>。例如，如果你将一个列向量添加到一个行向量中，你会以为它报出维度不匹配或类型错误之类的错误，但是实际上你会得到一个行向量和列向量的求和。</p><p>所以建议你编写神经网络时，<strong>不要使用shape为 <em>(5,)</em>、<em>(n,)</em> 或者其他一维数组的数据结构</strong>。相反，如果你设置 $a$ 为$(5,1)$，那么这就将置于5行1列向量中。在先前的操作里 $a$ 和 $a$ 的转置看起来一样，而现在这样的 $a$ 变成一个新的 $a$ 的转置，并且它是一个行向量。请注意一个细微的差别，在这种数据结构中，当我们输出 $a$ 的转置时有两对方括号，而之前只有一对方括号，所以这就是1行5列的矩阵和一维数组的差别。</p><p>我写代码时还有一件经常做的事，那就是如果我不完全确定一个向量的维度(<strong>dimension</strong>)，我经常会扔进一个断言语句(<strong>assertion statement</strong>)。像这样，去确保在这种情况下是一个$(5,1)$向量，或者说是一个列向量。这些断言语句实际上是要去执行的，并且它们也会有助于为你的代码提供信息。所以不论你要做什么，不要犹豫直接插入断言语句。如果你不小心以一维数组来执行，你也能够重新改变数组维数 $a=reshape$，表明一个$(5,1)$数组或者一个$(1,5)$数组，以致于它表现更像列向量或行向量。</p><p>通过在原先的代码里清除一维数组，我的代码变得更加简洁。而且实际上就我在代码中表现的事情而言，我从来不使用一维数组。因此，要去简化你的代码，而且<strong>不要使用一维数组</strong>。总是使用 $n \times 1$ 维矩阵（基本上是列向量），或者 $1 \times n$ 维矩阵（基本上是行向量），这样你可以减少很多<strong>assert</strong>语句来节省核矩阵和数组的维数的时间。另外，为了确保你的矩阵或向量所需要的维数时，不要羞于 <strong>reshape</strong> 操作。</p><h3><span id="logistic损失函数的解释">Logistic损失函数的解释</span></h3>回想一下，在逻辑回归中，需要预测的结果$\hat{y}$,可以表示为$\hat{y}=\sigma(w^{T}x+b)$，$\sigma$是我们熟悉的$S$型函数 $\sigma(z)=\sigma(w^{T}x+b)=\frac{1}{1+e^{-z}}$ 。我们约定 $\hat{y}=p(y=1|x)$ ，即算法的输出$\hat{y}$ 是给定训练样本 $x$ 条件下 $y$ 等于1的概率。换句话说，如果$y=1$，在给定训练样本 $x$ 条件下$y=\hat{y}$；反过来说，如果$y=0$，在给定训练样本$x$条件下 $y$ 等于1减去$\hat{y}(y=1-\hat{y})$，因此，如果 $\hat{y}$ 代表 $y=1$ 的概率，那么$1-\hat{y}$就是 $y=0$的概率。接下来，我们就来分析这两个条件概率公式。<p>这两个条件概率公式定义形式为 $p(y|x)$并且代表了 $y=0$ 或者 $y=1$ 这两种情况，我们可以将这两个公式合并成一个公式。需要指出的是我们讨论的是二分类问题的损失函数，因此，$y$的取值只能是0或者1。上述的两个条件概率公式可以合并成如下公式：<br>$$p(y|x)={\hat{y}}^{y}{(1-\hat{y})}^{(1-y)}$$</p><p><img src="data:image/png;base64,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" data-original="6.png" alt=""></p><p>接下来我会解释为什么可以合并成这种形式的表达式：$(1-\hat{y})$的$(1-y)$次方这行表达式包含了上面的两个条件概率公式，我来解释一下为什么。</p><p>第一种情况，假设 $y=1$，由于$y=1$，那么${(\hat{y})}^{y}=\hat{y}$，因为 $\hat{y}$的1次方等于$\hat{y}$，$1-{(1-\hat{y})}^{(1-y)}$的指数项$(1-y)$等于0，由于任何数的0次方都是1，$\hat{y}$乘以1等于$\hat{y}$。因此当$y=1$时 $p(y|x)=\hat{y}$（图中绿色部分）。</p><p>第二种情况，当 $y=0$ 时 $p(y|x)$ 等于多少呢? 假设$y=0$，$\hat{y}$的$y$次方就是 <script type="math/tex">\hat{y}</script> 的0次方，任何数的0次方都等于1，因此 $p(y|x)=1×{(1-\hat{y})}^{1-y}$ ，前面假设 $y=0$ 因此$(1-y)$就等于1，因此 $p(y|x)=1×(1-\hat{y})$。因此在这里当$y=0$时，$p(y|x)=1-\hat{y}$。这就是这个公式(第二个公式，图中紫色字体部分)的结果。<br>因此，刚才的推导表明 $p(y|x)={\hat{y}}^{(y)}{(1-\hat{y})}^{(1-y)}$，就是 $p(y|x)$ 的完整定义。由于 log 函数是严格单调递增的函数，最大化 $log(p(y|x))$ 等价于最大化 $p(y|x)$ 并且地计算 $p(y|x)$ 的 log对数，就是计算 $log({\hat{y}}^{(y)}{(1-\hat{y})}^{(1-y)})$ (其实就是将 $p(y|x)$ 代入)，通过对数函数化简为：</p><script type="math/tex; mode=display">ylog\hat{y}+(1-y)log(1-\hat{y})</script><p>而这就是我们前面提到的损失函数的负数 $(-L(\hat{y},y))$ ，前面有一个负号的原因是当你训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数 $log(p(y|x))$ 关联起来了，因此这就是单个训练样本的损失函数表达式。</p><p>在 $m$个训练样本的整个训练集中又该如何表示呢，让我们一起来探讨一下。</p><p>让我们一起来探讨一下，整个训练集中标签的概率，更正式地来写一下。假设所有的训练样本服从同一分布且相互独立，也即独立同分布的，所有这些样本的联合概率就是每个样本概率的乘积:</p><script type="math/tex; mode=display">P\left(\text{labels in training set} \right) = \prod_{i =1}^{m}{P(y^{(i)}|x^{(i)})}</script><p>如果你想做最大似然估计，需要寻找一组参数，使得给定样本的观测值概率最大，但令这个概率最大化等价于令其对数最大化，在等式两边取对数：</p><script type="math/tex; mode=display">logp\left( \text{labels in training set} \right) = log\prod_{i =1}^{m}{P(y^{(i)}|x^{(i)})} = \sum_{i = 1}^{m}{logP(y^{(i)}|x^{(i)})} = \sum_{i =1}^{m}{- L(\hat y^{(i)},y^{(i)})}</script><p>在统计学里面，有一个方法叫做最大似然估计，即求出一组参数，使这个式子取最大值，也就是说，使得这个式子取最大值，$\sum_{i= 1}^{m}{- L(\hat y^{(i)},y^{(i)})}$，可以将负号移到求和符号的外面，$- \sum_{i =1}^{m}{L(\hat y^{(i)},y^{(i)})}$，这样我们就推导出了前面给出的<strong>logistic</strong>回归的成本函数$J(w,b)= \sum_{i = 1}^{m}{L(\hat y^{(i)},y^{\hat( i)})}$。由于训练模型时，目标是让成本函数最小化，所以我们不是直接用最大似然概率，要去掉这里的负号，最后为了方便，可以对成本函数进行适当的缩放，我们就在前面加一个额外的常数因子$\frac{1}{m}$，即:<script type="math/tex">J(w,b)= \frac{1}{m}\sum_{i = 1}^{m}{L(\hat y^{(i)},y^{(i)})}</script></p><h2><span id="浅层神经网络">浅层神经网络</span></h2><p>神经网络的表示、输出、解释 ：略。</p><h3><span id="激活函数">激活函数</span></h3>更通常的情况下，使用不同的函数$g( z^{[1]})$，$g$可以是除了sigmoid函数以外的非线性函数。tanh函数或者双曲正切函数是总体上都优于sigmoid函数的激活函数。公式： $a= tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}$ 。事实上，tanh函数是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了$(0,0)$点，并且值域介于+1和-1之间。在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5。有一点要说明：我基本已经不用sigmoid激活函数了，tanh函数在所有场合都优于sigmoid函数。<p><strong>sigmoid</strong>函数和<strong>tanh</strong>函数两者共同的缺点是，在$z$特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p><p>在机器学习另一个很流行的函数是：修正线性单元的函数（<strong>ReLu</strong>）， 公式： $ a =max( 0,z) $ 所以，只要$z$是正值的情况下，导数恒等于1，当$z$是负值的时候，导数恒等于0。从实际上来说，当使用$z$的导数时，$z$=0的导数是没有定义的。但是当编程实现的时候，$z$的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，$z$是等于0的时候，假设一个导数是1或者0效果都可以。</p><p>这有一些选择激活函数的经验法则：</p><p>如果输出是0、1值（二分类问题），则输出层选择<strong>sigmoid</strong>函数，然后其它的所有单元都选择<strong>Relu</strong>函数。</p><p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用<strong>Relu</strong>激活函数。有时，也会使用<strong>tanh</strong>激活函数，但<strong>Relu</strong>的一个优点是：当$z$是负值的时候，导数等于0。</p><p><img src="data:image/png;base64,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" data-original="7.jpg" alt=""></p><p>两者的优点是：</p><p>第一，在$z$的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个<strong>if-else</strong>语句，而<strong>sigmoid</strong>函数需要进行浮点四则运算，在实践中，使用<strong>ReLu</strong>激活函数神经网络通常会比使用<strong>sigmoid</strong>或者<strong>tanh</strong>激活函数学习的更快。</p><p>第二，<strong>sigmoid</strong>和<strong>tanh</strong>函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而<strong>Relu</strong>和<strong>Leaky ReLu</strong>函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，<strong>Relu</strong>进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而<strong>Leaky ReLu</strong>不会有这问题)</p><p>$z$在<strong>ReLu</strong>的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。</p><p>快速概括一下不同激活函数的过程和结论：</p><p><strong>sigmoid</strong>激活函数：除了输出层是一个二分类问题基本不会用它。</p><p><strong>tanh</strong>激活函数：<strong>tanh</strong>是非常优秀的，几乎适合所有场合。</p><p><strong>ReLu</strong>激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用<strong>ReLu</strong>或者<strong>Leaky ReLu</strong>（公式： $a = max( 0.01z,z)$ 为什么常数是0.01？）当然，可以为学习算法选择不同的参数。</p><p>除此之外，对于使用场合：<strong>不能在隐藏层用线性激活函数</strong>，可以用<strong>ReLU</strong>或者<strong>tanh</strong>或者<strong>leaky ReLU</strong>或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的。</p><h3><span id="激活函数的导数">激活函数的导数</span></h3><p>在神经网络中使用反向传播的时候，你真的需要计算激活函数的斜率或者导数。针对以下四种激活，求其导数如下：</p><p>1）<strong>sigmoid activation function</strong><br>其具体的求导如下： $\frac{d}{dz}g(z) = {\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))$</p><p>注：当$z$ = 10或$z= -10$ ; $\frac{d}{dz}g(z)\approx0$</p><p>当$z $= 0 , $\frac{d}{dz}g(z)\text{=g(z)(1-g(z))=}{1}/{4}$<br>在神经网络中$a= g(z)$; $g{{(z)}^{'}}=\frac{d}{dz}g(z)=a(1-a)$</p><p>2）<strong>Tanh activation function</strong><br>其具体的求导如下： $g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $ ,  $\frac{d}{{d}z}g(z) = 1 - (tanh(z))^{2}$ </p><p>注：当$z$ = 10或$z= -10$ $\frac{d}{dz}g(z)\approx0$</p><p>当$z$ = 0, $\frac{d}{dz}g(z)\text{=1-(0)=}1$</p><p>3）<strong>Rectified Linear Unit (ReLU)</strong></p><p>$g(z) =max (0,z)$</p><p>$ g(z)^{‘}= \begin{cases} 0&amp;    \text{if z &lt; 0} 1&amp;    \text{if z &gt; 0} undefined&amp;    \text{if z = 0} \end{cases} $</p><p>注：通常在$z$= 0的时候给定其导数1,0；当然$z$=0的情况很少</p><p>4）<strong>Leaky linear unit (Leaky ReLU)</strong></p><p>与<strong>ReLU</strong>类似 <script type="math/tex">g(z)=\max(0.01z,z) \ \ \ g(z)^{'}= \begin{cases} 0.01& \text{if z < 0}\ 1&    \text{if z > 0}\ undefined&    \text{if z = 0} \end{cases}</script></p><p>注：通常在$z = 0$的时候给定其导数1,0.01；当然$z=0$的情况很少。</p><h3><span id="反向传播">反向传播</span></h3><p>这部分比较难，但是很重要，通常需要链式求导一堆一堆的手推。自己多推吧。</p><h2><span id="深度神经网络">深度神经网络</span></h2><p>神经网络的层数是这么定义的：<strong>从左到右，由0开始定义</strong>。</p><p>符号约定：</p><p>输入的特征记作$x$，但是$x$同样也是0层的激活函数，所以$x={a}^{[0]}$。</p><p>最后一层的激活函数，所以${a}^{[L]}$是等于这个神经网络所预测的输出结果。</p><p>前向传播：输入${a}^{[l-1]}$，输出是${a}^{[l]}$，缓存为${z}^{[l]}$；从实现的角度来说我们可以缓存下${w}^{[l]}$和${b}^{[l]}$，这样更容易在不同的环节中调用函数。</p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="8.png" alt=""><br>实现过程可以写成： ${z}^{[l]}={W}^{[l]}\cdot{a}^{[l-1]}+{b}^{[l]}$ , ${{a}^{[l]}}={{g}^{[l]}}\left( {{z}^{[l]}}\right)$</p><p>向量化实现过程可以写成： ${z}^{[l]}={W}^{[l]}\cdot {A}^{[l-1]}+{b}^{[l]}$ , ${A}^{[l]}={g}^{[l]}({Z}^{[l]})$<br>前向传播需要喂入${A}^{[0]}$也就是$X$，来初始化；初始化的是第一层的输入值。${a}^{[0]}$对应于一个训练样本的输入特征，而${{A}^{[0]}}$对应于一整个训练样本的输入特征，所以这就是这条链的第一个前向函数的输入，重复这个步骤就可以从左到右计算前向传播。</p>反向传播：输入为${{da}^{[l]}}$，输出为${{da}^{[l-1]}}$，${{dw}^{[l]}}$, ${{db}^{[l]}}$<p><img src="data:image/png;base64,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" data-original="9.png" alt=""></p><p>所以反向传播的步骤可以写成：<br>$$d{{z}^{[l]}}=d{{a}^{[l]}}*{{g}^{[l]}}'( {{z}^{[l]}})  (1)$$$$d{{w}^{[l]}}=d{{z}^{[l]}}\cdot{{a}^{[l-1]}}~  (2)$$$$d{{b}^{[l]}}=d{{z}^{[l]}}~~  (3)$$$$d{{a}^{[l-1]}}={{w}^{\left[ l \right]T}}\cdot {{dz}^{[l]}}  (4)$$$$d{{z}^{[l]}}={{w}^{[l+1]T}}d{{z}^{[l+1]}}\cdot \text{ }{{g}^{[l]}}'( {{z}^{[l]}})~  (5)$$</p><p>式子（5）由式子（4）带入式子（1）得到，前四个式子就可实现反向函数。</p><p>向量化实现过程可以写成：<br>$$d{{Z}^{[l]}}=d{{A}^{[l]}}*{{g}^{\left[ l \right]}}'\left({{Z}^{[l]}} \right)~~  (6)$$$$d{{W}^{[l]}}=\frac{1}{m}\text{}d{{Z}^{[l]}}\cdot {{A}^{\left[ l-1 \right]T}}  (7)$$$$d{{b}^{[l]}}=\frac{1}{m}\text{ }np.sum(d{{z}^{[l]}},axis=1,keepdims=True)  (8)$$$$d{{A}^{[l-1]}}={{W}^{\left[ l \right]T}}.d{{Z}^{[l]}}  (9)$$</p><p>深层的网络隐藏单元数量相对较少，隐藏层数目较多，如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。（举例：用NN实现XOR异或操作）</p><p>神经网络的计算过程会是这样的：</p><p><img src="data:image/png;base64,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" data-original="10.png" alt=""></p><p>把输入特征$a^{[0]}​$，放入第一层并计算第一层的激活函数，用$a^{[1]}​$表示，你需要$W^{[1]}​$和$b^{[1]}​$来计算，之后也缓存$z^{[l]}​$值。之后喂到第二层，第二层里，需要用到$W^{[2]}​$和$b^{[2]}​$，你会需要计算第二层的激活函数$a^{[2]}​$。后面几层以此类推，直到最后你算出了$a^{[L]}​$，第$L​$层的最终输出值$\hat y​$。在这些过程里我们缓存了所有的$z​$值，这就是正向传播的步骤。</p><p>对反向传播的步骤而言，我们需要算一系列的反向迭代，就是这样反向计算梯度，你需要把$da^{[L]}$的值放在这里，然后这个方块会给我们${da}^{[L-1]}$的值，以此类推，直到我们得到${da}^{[2]}$和${da}^{[1]}$，你还可以计算多一个输出值，就是${da}^{[0]}$，但这其实是你的输入特征的导数，并不重要，起码对于训练监督学习的权重不算重要，你可以止步于此。反向传播步骤中也会输出$dW^{[l]}$和$db^{[l]}$，这会输出$dW^{[3]}$和$db^{[3]}$等等。目前为止你算好了所有需要的导数，稍微填一下这个流程图。</p><p>神经网络的一步训练包含了，从$a^{[0]}$开始，也就是 $x$ 然后经过一系列正向传播计算得到$\hat y$，之后再用输出值计算这个（第二行最后方块），再实现反向传播。现在你就有所有的导数项了，$W$也会在每一层被更新为$W=W-αdW$，$b$也一样，$b=b-αdb$，反向传播就都计算完毕，我们有所有的导数值，那么这是神经网络一个梯度下降循环。</p><p>【原文出自黄海广博士 <strong><a href="https://github.com/fengdu78/deeplearning_ai_books" target="_blank" rel="noopener">deeplearning_ai_books</a></strong>】</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;涉及 基本的神经网络和深度学习的概念。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/DeepLearning/"/>
    
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>《矩阵论》课程知识点整理</title>
    <link href="http://hhu1506010220.github.io/2019/12/30/%E3%80%8A%E7%9F%A9%E9%98%B5%E8%AE%BA%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"/>
    <id>http://hhu1506010220.github.io/2019/12/30/《矩阵论》课程知识点整理/</id>
    <published>2019-12-30T10:52:41.000Z</published>
    <updated>2020-10-15T10:29:36.192Z</updated>
    
    <content type="html"><![CDATA[<p>2019年南京大学《矩阵论》课程，个人整理的主要知识点。个人学习使用，请勿转载。<br><a id="more"></a></p><h3><span id="一-线性空间与线性变换">一、线性空间与线性变换</span></h3><p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p><p><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></p><p><img src="data:image/png;base64,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" data-original="3.jpg" alt=""></p><h3><span id="二-内积空间">二、内积空间</span></h3><p><img src="data:image/png;base64,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" data-original="4.jpg" alt=""></p><h3><span id="三-赋范线性空间">三、赋范线性空间</span></h3><p><img src="data:image/png;base64,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" data-original="5.jpg" alt=""></p><h3><span id="四-矩阵的特征值和奇异值分解">四、矩阵的特征值和奇异值分解</span></h3><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="6.jpg" alt=""></p><p><img src="data:image/png;base64,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" data-original="7.jpg" alt=""></p><h3><span id="五-投影分析">五、投影分析</span></h3><p><img src="data:image/png;base64,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" data-original="8.jpg" alt=""></p><h3><span id="六-矩阵分解与广义逆矩阵">六、矩阵分解与广义逆矩阵</span></h3><p><img src="data:image/png;base64,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" data-original="9.jpg" alt=""></p><h3><span id="七-矩阵的特殊乘积">七、矩阵的特殊乘积</span></h3><p><img src="data:image/png;base64,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" data-original="10.jpg" alt=""></p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/12/30/《矩阵论》课程知识点整理/" target="_blank" rel="noopener">https://vodkazy.cn/2019/12/30/《矩阵论》课程知识点整理</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019年南京大学《矩阵论》课程，个人整理的主要知识点。个人学习使用，请勿转载。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数学" scheme="http://hhu1506010220.github.io/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="数学" scheme="http://hhu1506010220.github.io/tags/%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>《自然语言处理》课程知识点整理</title>
    <link href="http://hhu1506010220.github.io/2019/12/27/%E3%80%8A%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E8%AF%BE%E7%A8%8B%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"/>
    <id>http://hhu1506010220.github.io/2019/12/27/《自然语言处理》课程知识点整理/</id>
    <published>2019-12-27T11:15:48.000Z</published>
    <updated>2020-10-15T10:34:57.114Z</updated>
    
    <content type="html"><![CDATA[<p>2019年南京大学《自然语言处理》课程，个人整理的主要知识点。个人学习使用，请勿转载。<br><a id="more"></a></p><h3><span id="nlp-traditional">NLP Traditional</span></h3><p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p><h3><span id="语言模型">语言模型</span></h3><p><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></p><h3><span id="text-classification">Text Classification</span></h3><p><img src="data:image/png;base64,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" data-original="3.jpg" alt=""></p><h3><span id="词性标注与隐马尔可夫模型">词性标注与隐马尔可夫模型</span></h3><p><img src="data:image/png;base64,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" data-original="4.jpg" alt=""></p><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="5.jpg" alt=""></p><h3><span id="statistical-parsing">Statistical Parsing</span></h3><p><img src="data:image/png;base64,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" data-original="6.jpg" alt=""></p><h3><span id="machine-translation">Machine Translation</span></h3><p><img src="data:image/png;base64,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" data-original="7.jpg" alt=""></p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/12/27/《自然语言处理》课程知识点整理" target="_blank" rel="noopener">https://vodkazy.cn/2019/12/27/《自然语言处理》课程知识点整理</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2019年南京大学《自然语言处理》课程，个人整理的主要知识点。个人学习使用，请勿转载。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://hhu1506010220.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>小记：处理LSTM+embedding变长序列</title>
    <link href="http://hhu1506010220.github.io/2019/12/12/%E5%B0%8F%E8%AE%B0%EF%BC%9A%E5%A4%84%E7%90%86LSTM-embedding%E5%8F%98%E9%95%BF%E5%BA%8F%E5%88%97/"/>
    <id>http://hhu1506010220.github.io/2019/12/12/小记：处理LSTM-embedding变长序列/</id>
    <published>2019-12-12T06:42:54.000Z</published>
    <updated>2020-10-15T11:11:57.911Z</updated>
    
    <content type="html"><![CDATA[<p>花了两天的时间学习了一下如何利用torch.nn.utils.rnn来处理变长LSTM，期间遇到了不少bug，特定来记录一下，以后避免踩坑。<br><a id="more"></a></p><h2><span id="前言">前言</span></h2><p>在做NLP作业文章打分任务时，由于输入的文章是不定长的，而转成tensor又要求所有的输入单元要等长，因此通常的做法是利用截断、或者填充的做法统一长度，但是这样有可能会造成信息丢失或者是padding过长最后把前面的记忆都遗忘了的情况。因此学习了一下 <code>torch.nn.utils.rnn</code> 自带的 <code>pack_padded_sequence</code> 方法（压包），该方法返回的是一个带有实际处理长度的PackedSequence对象。对应的 <code>pad_packed_sequence</code> 方法（解包）则是将PackedSequence对象返回成数个mini-batch序列，并返回一个实际处理长度的列表。 </p><p>在处理期间主要遇到了两个问题：</p><p>第一个是在每个batch中，除了最长的那个序列所对应的输出是有变化的，其他的几个序列输出值都是一样的，我观测了输入是没毛病的，输出的前几个值也都是不一样的，但是在某个位置之后就都开始一样了。因为我一开始都是取的最后一个时间步的h作为输出，但是通过debug之后发现，对于每个batch，除了最长的序列的输出是正常的，其余的序列最后一个h都是一样的，我觉得可能是这些padding在前向传播过程中压根就没传过来，最后我采取的措施是分别取最后一个不是padding的时间步的h作为输出就解决了；</p><p>第二个问题是发现虽然我有几百个验证数据，并且他们的分数上到12下到2，但是最后输出的时候却都集中地趋向于8，不管label是2的还是12的，输出都趋向于8，这个我纳闷了好久。最后发现原因竟然是我的训练数据的分布极不均匀，训练集中一共有1070条数据，但是其中的420条的label是8，只有6条的label是2，60多条label是12，所以这也就造成了为什么我的输出会倾向于8左右（因为训练数据的均值是8）。后来我把label的每个种类的数据条数都扩充至一样的，模型的输出就会区分2、8、12了。</p><h3><span id="torchnnutilsrnn">torch.nn.utils.rnn</span></h3><p>首先介绍一下padding部分，这部分主要参考了<a href="https://zhuanlan.zhihu.com/p/59772104" target="_blank" rel="noopener">知乎的一篇文章</a>。</p><p><code>pad_sequence</code> 方法用于返回利用padding的返回一个长度统一的列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_x = [torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]),</span><br><span class="line">           torch.tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]),</span><br><span class="line">           torch.tensor([<span class="number">6</span>, <span class="number">6</span>])]</span><br><span class="line"></span><br><span class="line">x = rnn_utils.pad_sequence(train_x, batch_first=<span class="keyword">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">x = tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>我们发现，这个函数会把长度小于最大长度的 sequences 用 0 填充，并且把 list 中所有的元素拼成一个 tensor。这样做的主要目的是为了让 DataLoader 可以返回 batch，因为 batch 是一个高维的 tensor，其中每个元素的数据必须长度相同。</p><h3><span id="pack_padded_sequence">pack_padded_sequence</span></h3><p>让我们想一下RNN是如何训练的：对于batch为3的数据，首先投进去time_step都为1的数据、加上他们的hidden state，获得输出；然后再读取下一个time_step的所有数据，再加上上一个时间步的输出，以此类推。以上的数据为例，网络读取数据的顺序是：[1, 3, 6]，[1, 3, 6]，[1, 3, 0]，[1, 3, 0]，[1, 3, 0]，[1, 0, 0]，[1, 0, 0]。显然，对于那些用来padding的0，计算它们显然浪费了大量资源没必要，所以就要想办法在计算的时候不让这些0加进去。这就用到了<strong>pack_padded_sequence</strong>方法。</p><p><code>pack_padded_sequence</code> 有三个参数：<code>input, lengths, batch_first</code> 。<code>input</code> 是加过 padding 的数据，<code>lengths</code> 是各个 sequence 的实际长度，<code>batch_first</code>是数据各个 dimension 按照 <code>[batch_size, sequence_length, data_dim]</code>顺序排列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rnn_utils.pack_padded_sequence(batch_x, [<span class="number">7</span>,<span class="number">5</span>,<span class="number">2</span>], batch_first=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">PackedSequence(data=tensor([<span class="number">1.</span>, <span class="number">3.</span>, <span class="number">6.</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">6.</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">1.</span>]),batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>所以说相当于RNN把每个batch又划分为了更小的batch，并且每个batch的大小是不一样的。原理我们懂了，但是如何方便的既获取padding过的序列又获取每个序列的实际长度呢，这就需要我们自己实现一个类似于DataLoader的数据返回迭代器了。</p><h3><span id="dataloader-amp-collate_fn">DataLoader &amp; collate_fn</span></h3><p>Pytorch虽然有已经封装好的DataLoader，返回的是迭代对象，用法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = DataLoader(X, y)</span><br><span class="line">loader = DataLoader(dataset=data, batch_size=<span class="number">128</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> loader:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>对于本文这种需要返回自定义结构的迭代器，需要自己重写一些细节：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyData</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义数据读取迭代器结构</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_seq, data_label)</span>:</span></span><br><span class="line">        self.data_seq = data_seq</span><br><span class="line">        self.data_label = data_label <span class="comment"># 修改传入形参列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data_seq)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data_seq[idx], self.data_label[idx] <span class="comment"># 修改方法的返回值</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义 dataloader 的返回值</span></span><br><span class="line"><span class="string">    :param data: 第0维：data，第1维：label</span></span><br><span class="line"><span class="string">    :return: 序列化的data、记录实际长度的序列、以及label列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: len(x[<span class="number">0</span>]), reverse=<span class="keyword">True</span>) <span class="comment"># pack_padded_sequence 要求要按照序列的长度倒序排列</span></span><br><span class="line">    data_length = [len(sq[<span class="number">0</span>]) <span class="keyword">for</span> sq <span class="keyword">in</span> data]</span><br><span class="line">    x = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    y = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    data = rnn_utils.pad_sequence(x, batch_first=<span class="keyword">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data.unsqueeze(<span class="number">-1</span>), data_length, torch.tensor(y, dtype=torch.float32)</span><br></pre></td></tr></table></figure><p>这样一来，每次返回的就是一个PackedSequence对象、一个记录实际长度的序列、以及标签序列。重新定义collate_fn之后的返回结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">batch_x, batch_x_len = loader.next()</span><br><span class="line">batch_x_pack = rnn_utils.pack_padded_sequence(batch_x, </span><br><span class="line">                                                  batch_x_len, batch_first=<span class="keyword">True</span>)</span><br><span class="line">batch_x = tensor([[[<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>]],</span><br><span class="line">        [[<span class="number">3.</span>],</span><br><span class="line">         [<span class="number">3.</span>],</span><br><span class="line">         [<span class="number">3.</span>],</span><br><span class="line">         [<span class="number">3.</span>],</span><br><span class="line">         [<span class="number">3.</span>],</span><br><span class="line">         [<span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>]],</span><br><span class="line">        [[<span class="number">6.</span>],</span><br><span class="line">         [<span class="number">6.</span>],</span><br><span class="line">         [<span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>]]])</span><br><span class="line">batch_x_len = [<span class="number">7</span>,<span class="number">5</span>,<span class="number">2</span>]</span><br><span class="line">batch_x_pack = PackedSequence(data=tensor([</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>]]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h3><span id="pad_packed_sequence">pad_packed_sequence</span></h3><p><code>pad_packed_sequence</code> 执行的是 <code>pack_padded_sequence</code> 的逆操作。</p><p><strong>pack_padded_sequence</strong> 将padding矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_x = tensor([[[<span class="number">1.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>], [<span class="number">1.</span>]],</span><br><span class="line">        [[<span class="number">3.</span>], [<span class="number">3.</span>], [<span class="number">3.</span>], [<span class="number">3.</span>], [<span class="number">3.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>]],</span><br><span class="line">        [[<span class="number">6.</span>], [<span class="number">6.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>]]])</span><br></pre></td></tr></table></figure><p>加上传入实际的len数组，转化为更小的mini-batch以及对应的len（无padding），输出类型是PackedSequence。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_x_pack = PackedSequence(data=tensor([</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">6.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>]]), batch_sizes=tensor([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p><strong>pad_packed_sequence</strong> 将 PackedSequence 转换为具有 padding 的矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out, _ = net(batch_x_pack)</span><br><span class="line">out_pad, out_len = rnn_utils.pad_packed_sequence(out, batch_first=<span class="keyword">True</span>)</span><br><span class="line">out_pad.shape = torch.Size([<span class="number">3</span>, <span class="number">7</span>, <span class="number">10</span>]) <span class="comment"># batch_size * max_len * hidden_size</span></span><br><span class="line">out.data.shape = torch.Size([<span class="number">14</span>, <span class="number">10</span>])  <span class="comment"># 实际的元素个数 * hidden_size</span></span><br><span class="line">out_len = tensor([<span class="number">7</span>, <span class="number">5</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><h3><span id="embedding">Embedding</span></h3><p>本部分参照<a href="https://mp.weixin.qq.com/s/YueTspxyLQdgvM9iEcQGtA" target="_blank" rel="noopener">THU数据派的一篇文章</a>，讲述如何在网络net中添加embedding层。</p><p>本部分需要注意的一点是，前面部分的 <code>pack_padded_sequence</code> 的结果不能直接传进embedding层，因为embedding层不接受PackedSequence对象作为参数，只接收tensor对象，所以只能先利用 <code>pad_packed_sequence</code> 解包再转化为embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得训练集、验证集和测试集的预料统计</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../../data/token_vocab_pack.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># vocab: OrderedDict，key为词，value为在所有数据集上统计的个数。使用list(token_vocab.keys())可以得到所有词按顺序的列表。</span></span><br><span class="line">    <span class="comment"># stoi: dict, key为词, value为索引下标。</span></span><br><span class="line">    <span class="comment"># itos: dict, key为索引下标，value为词。</span></span><br><span class="line">    json_token_vocab_pack = f.read()</span><br><span class="line">    token_vocab, token_stoi, token_itos, token_num_word = json.loads(json_token_vocab_pack)</span><br></pre></td></tr></table></figure><p>上边的代码可以获得词和索引的下标。之后再利用已有的预训练词表，生成一个embeddings_index索引表。然后利用自身语料，为每个词找到对应的向量表示，并且加个序号。这样的话相当于来一个词就先利用token_stoi获取token的id，然后在embedding层利用这个id获取对应行的embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用Glove，获得现有语料的权重矩阵，并获得[index,vector]的索引列表，矩阵要被加人nn.embedding()</span></span><br><span class="line">EMBEDDING_FILE = <span class="string">'../../data/glove.6B.50d.txt'</span></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(open(EMBEDDING_FILE)):</span><br><span class="line">    val = line.split()</span><br><span class="line">    embeddings_index[val[<span class="number">0</span>]] = np.asarray(val[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_matrix = np.zeros((len(token_itos), vocab_dim))</span><br><span class="line"><span class="keyword">for</span> _index, word <span class="keyword">in</span> token_itos.items():</span><br><span class="line">    embedding_vector = embeddings_index.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        embedding_matrix[int(_index)] = embedding_vector</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        embedding_matrix[int(_index)] = np.asarray(np.zeros(vocab_dim), dtype=<span class="string">'float32'</span>)  <span class="comment"># &lt;unk&gt;</span></span><br><span class="line">embedding_matrix[<span class="number">0</span>] = np.asarray(np.ones(vocab_dim), dtype=<span class="string">'float32'</span>)  <span class="comment"># &lt;pad&gt;</span></span><br></pre></td></tr></table></figure><p>torch.nn包下的Embedding，提供了封装好的可以作为<strong>训练的一层</strong>，参数为词表大小和维度，然后借助于现有的权重矩阵进行初始化权重，并可设置参不参与调参。</p><p><strong>建立词向量层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.nn.Embedding(n_vocabulary,embedding_size)</span></span><br><span class="line">self.embedding = nn.Embedding(len(token_itos), vocab_dim)</span><br><span class="line">self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))</span><br><span class="line">self.embedding.weight.requires_grad = <span class="keyword">False</span> <span class="comment"># 这里设置embedding层不参与训练</span></span><br><span class="line">self.embedding_dropout = nn.Dropout2d(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h3><span id="参考链接">参考链接</span></h3><p><a href="https://zhuanlan.zhihu.com/p/63219625" target="_blank" rel="noopener">使用Keras和Pytorch处理RNN变长序列输入的方法总结</a></p><p><a href="https://blog.csdn.net/baidu_32885165/article/details/102155219" target="_blank" rel="noopener">LSTM在text embedding中的作用(Cross modal retrieval)</a></p><p><a href="https://www.cnblogs.com/duye/p/10590146.html" target="_blank" rel="noopener">【pytorch】关于Embedding和GRU、LSTM的使用详解</a></p><p><a href="https://www.jianshu.com/p/043083d114d4" target="_blank" rel="noopener">pytorch中LSTM笔记</a></p><h3><span id="附录代码">附录代码</span></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">  @ Time     : 2019/12/10 上午10:24</span></span><br><span class="line"><span class="string">  @ Author   : Vodka</span></span><br><span class="line"><span class="string">  @ File     : LSTM .py</span></span><br><span class="line"><span class="string">  @ Software : PyCharm</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.utils.rnn <span class="keyword">as</span> rnn_utils</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">epoch_size = <span class="number">1000</span></span><br><span class="line">softmax_size = <span class="number">20</span></span><br><span class="line">input_size = <span class="number">50</span></span><br><span class="line">hidden_size = <span class="number">32</span></span><br><span class="line">vocab_dim = <span class="number">50</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyData</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义数据读取迭代器结构</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_seq, data_label)</span>:</span></span><br><span class="line">        self.data_seq = data_seq</span><br><span class="line">        self.data_label = data_label</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data_seq)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data_seq[idx], self.data_label[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    定义 dataloader 的返回值</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data.sort(key=<span class="keyword">lambda</span> x: len(x[<span class="number">0</span>]), reverse=<span class="keyword">True</span>)</span><br><span class="line">    data_length = [len(sq[<span class="number">0</span>]) <span class="keyword">for</span> sq <span class="keyword">in</span> data]</span><br><span class="line">    x = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    y = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> data]</span><br><span class="line">    data = rnn_utils.pad_sequence(x, batch_first=<span class="keyword">True</span>, padding_value=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> data.unsqueeze(<span class="number">-1</span>), data_length, torch.tensor(y, dtype=torch.float32).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    自己封装一个RNN</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(len(token_itos), vocab_dim)</span><br><span class="line">        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.embedding_dropout = nn.Dropout2d(<span class="number">0.1</span>)</span><br><span class="line">        self.embedding_dropout.requires_grad = <span class="keyword">False</span></span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        self.linear = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 先解包传进来的x,得到真实的输入序列和真实长度</span></span><br><span class="line">        _pad, _len = rnn_utils.pad_packed_sequence(x, batch_first=<span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 过一个embedding</span></span><br><span class="line">        x_embedding = self.embedding(_pad)</span><br><span class="line">        x_embedding = x_embedding.view(x_embedding.shape[<span class="number">0</span>], x_embedding.shape[<span class="number">1</span>], <span class="number">-1</span>)</span><br><span class="line">        output, _ = self.lstm(x_embedding)</span><br><span class="line">        <span class="comment"># 拿出来最后一个单词对应的时间步的h作为输出</span></span><br><span class="line">        temp = []</span><br><span class="line">        _len = _len - <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(_len)):</span><br><span class="line">            temp.append(output[i, _len[i], :].tolist())</span><br><span class="line">        temp = torch.tensor(temp)</span><br><span class="line">        <span class="comment"># 再过一个线性层</span></span><br><span class="line">        out = self.linear(temp)</span><br><span class="line">        <span class="comment"># print(out)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得训练集、验证集和测试集的预料统计</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'../../data/token_vocab_pack.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="comment"># vocab: OrderedDict，key为词，value为在所有数据集上统计的个数。使用list(token_vocab.keys())可以得到所有词按顺序的列表。</span></span><br><span class="line">    <span class="comment"># stoi: dict, key为词, value为索引下标。</span></span><br><span class="line">    <span class="comment"># itos: dict, key为索引下标，value为词。</span></span><br><span class="line">    json_token_vocab_pack = f.read()</span><br><span class="line">    token_vocab, token_stoi, token_itos, token_num_word = json.loads(json_token_vocab_pack)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用Glove，获得现有语料的权重矩阵，并获得[index,vector]的索引列表,矩阵要被加入结果从</span></span><br><span class="line">EMBEDDING_FILE = <span class="string">'../../data/glove.6B.50d.txt'</span></span><br><span class="line">embeddings_index = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i, line <span class="keyword">in</span> enumerate(open(EMBEDDING_FILE)):</span><br><span class="line">    val = line.split()</span><br><span class="line">    embeddings_index[val[<span class="number">0</span>]] = np.asarray(val[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</span><br><span class="line">embedding_matrix = np.zeros((len(token_itos), vocab_dim))</span><br><span class="line"><span class="keyword">for</span> _index, word <span class="keyword">in</span> token_itos.items():</span><br><span class="line">    embedding_vector = embeddings_index.get(word)</span><br><span class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        embedding_matrix[int(_index)] = embedding_vector</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        embedding_matrix[int(_index)] = np.asarray(np.zeros(vocab_dim), dtype=<span class="string">'float32'</span>)  <span class="comment"># &lt;unk&gt;</span></span><br><span class="line">embedding_matrix[<span class="number">0</span>] = np.asarray(np.ones(vocab_dim), dtype=<span class="string">'float32'</span>)  <span class="comment"># &lt;pad&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> essay_id <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">9</span>):</span><br><span class="line">        <span class="comment"># 加载训练集</span></span><br><span class="line">        data = pd.read_csv(<span class="string">'../../data/train.csv'</span>)</span><br><span class="line">        data = data.loc[data[<span class="string">'essay_set'</span>] == essay_id]</span><br><span class="line">        X_train = data[<span class="string">'tokens'</span>]</span><br><span class="line">        y_train = data[<span class="string">'score'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        X = []</span><br><span class="line">        y = torch.tensor(y_train)</span><br><span class="line">        <span class="keyword">for</span> essay <span class="keyword">in</span> X_train:</span><br><span class="line">            essay_vec = []</span><br><span class="line">            essay_tokens = eval(essay)</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> essay_tokens:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    essay_vec.append(token_stoi[token.lower()])</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    essay_vec.append(<span class="number">1</span>) <span class="comment"># 没见过的词视为&lt;unk&gt;</span></span><br><span class="line">            X.append(torch.tensor(essay_vec))</span><br><span class="line">        train = MyData(X, y)</span><br><span class="line">        trainloader = DataLoader(train, batch_size=batch_size, collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加载验证集</span></span><br><span class="line">        data = pd.read_csv(<span class="string">'../../data/dev.csv'</span>)</span><br><span class="line">        data = data.loc[data[<span class="string">'essay_set'</span>] == essay_id]</span><br><span class="line">        X_dev = data[<span class="string">'tokens'</span>]</span><br><span class="line">        y_dev = data[<span class="string">'score'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        X = []</span><br><span class="line">        y = torch.tensor(y_dev)</span><br><span class="line">        <span class="keyword">for</span> essay <span class="keyword">in</span> X_dev:</span><br><span class="line">            essay_vec = []</span><br><span class="line">            essay_tokens = eval(essay)</span><br><span class="line">            <span class="keyword">for</span> token <span class="keyword">in</span> essay_tokens:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    essay_vec.append(token_stoi[token.lower()])</span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    essay_vec.append(<span class="number">1</span>)</span><br><span class="line">            X.append(torch.tensor(essay_vec))</span><br><span class="line">        valid = MyData(X, y)</span><br><span class="line">        validloader = DataLoader(valid, batch_size=batch_size, collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义网络</span></span><br><span class="line">        net = RNN()</span><br><span class="line">        optimizer = optim.Adam(net.parameters(), lr=learning_rate)</span><br><span class="line">        loss_F = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        best_valid_loss = <span class="number">111111.0</span></span><br><span class="line">        valid_losses = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, epoch_size + <span class="number">1</span>):</span><br><span class="line">            train_loss, valid_loss = [], []</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 训练部分</span></span><br><span class="line">            _index = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> data, length, target <span class="keyword">in</span> trainloader:</span><br><span class="line">                batch_x_pack = rnn_utils.pack_padded_sequence(data, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">                net.zero_grad()</span><br><span class="line">                output = net(batch_x_pack)</span><br><span class="line">                loss = loss_F(output, target)</span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># for i in range(len(output)):</span></span><br><span class="line">                <span class="comment">#     if(_index==1 or _index==27 or _index==59):</span></span><br><span class="line">                <span class="comment">#         print(str(output[i].item()) + "    " + str(target[i].item()) + "    " + str(loss.item()))</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                train_loss.append(loss.item())</span><br><span class="line">                _index += <span class="number">1</span></span><br><span class="line">            print(<span class="string">"*********train*********\nEpoch &#123;&#125; of Essay &#123;&#125;, Loss : &#123;&#125; . "</span>.format(epoch, essay_id,</span><br><span class="line">                                                                                       sum(train_loss) / (</span><br><span class="line">                                                                                           len(train_loss))))</span><br><span class="line">            <span class="comment"># 验证部分</span></span><br><span class="line">            <span class="keyword">for</span> data, length, target <span class="keyword">in</span> validloader:</span><br><span class="line">                batch_x_pack = rnn_utils.pack_padded_sequence(data, length, batch_first=<span class="keyword">True</span>)</span><br><span class="line">                output = net(batch_x_pack)</span><br><span class="line">                loss = loss_F(output, target)</span><br><span class="line">                valid_loss.append(loss.item())</span><br><span class="line">            print(<span class="string">"*********eval*********\nEpoch &#123;&#125; of Essay &#123;&#125;, Loss : &#123;&#125; . "</span>.format(epoch, essay_id,</span><br><span class="line">                                                                                      sum(valid_loss) / (</span><br><span class="line">                                                                                          len(valid_loss))))</span><br><span class="line">            <span class="comment"># print(list(net.named_parameters()))</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># valid_losses.append(sum(valid_loss))</span></span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="comment"># if (len(valid_losses) &gt; 20):</span></span><br><span class="line">            <span class="comment">#     flag = True</span></span><br><span class="line">            <span class="comment">#     for i in range(1, 21):</span></span><br><span class="line">            <span class="comment">#         if (valid_losses[-i] &lt;= best_valid_loss):</span></span><br><span class="line">            <span class="comment">#             best_valid_loss = valid_losses[-i]</span></span><br><span class="line">            <span class="comment">#             flag = False</span></span><br><span class="line">            <span class="comment">#     if flag == True:</span></span><br><span class="line">            <span class="comment">#         break</span></span><br><span class="line"></span><br><span class="line">        torch.save(net, <span class="string">"model_"</span> + str(essay_id) + <span class="string">".pkl"</span>)</span><br></pre></td></tr></table></figure><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/12/12/小记：处理LSTM-embedding变长序列" target="_blank" rel="noopener">https://vodkazy.cn/2019/12/12/小记：处理LSTM-embedding变长序列</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;花了两天的时间学习了一下如何利用torch.nn.utils.rnn来处理变长LSTM，期间遇到了不少bug，特定来记录一下，以后避免踩坑。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Pytorch" scheme="http://hhu1506010220.github.io/categories/Pytorch/"/>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/categories/Pytorch/DeepLearning/"/>
    
    
      <category term="Pytorch" scheme="http://hhu1506010220.github.io/tags/Pytorch/"/>
    
      <category term="DeepLearning" scheme="http://hhu1506010220.github.io/tags/DeepLearning/"/>
    
  </entry>
  
  <entry>
    <title>Go学习笔记</title>
    <link href="http://hhu1506010220.github.io/2019/11/08/Go%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://hhu1506010220.github.io/2019/11/08/Go学习笔记/</id>
    <published>2019-11-08T11:02:38.000Z</published>
    <updated>2020-10-15T10:47:24.906Z</updated>
    
    <content type="html"><![CDATA[<p>由于分布的作业要求用Golang语言实现，所以参照tutorial学习了一下。</p><a id="more"></a><h2><span id="包">包</span></h2><p>每个 Go 程序都是由包构成的。</p><p>程序从 <code>main</code> 包开始运行。</p><p>程序通过导入路径 <code>&quot;fmt&quot;</code> 和 <code>&quot;math/rand&quot;</code> 来使用包。</p><p>按照约定，包名与导入路径的最后一个元素一致。例如，<code>&quot;math/rand&quot;</code> 包中的源码均以 <code>package rand</code> 语句开始。</p><h2><span id="导入">导入</span></h2><p>此代码用圆括号组合了导入，这是“分组”形式的导入语句。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"math"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>当然你也可以编写多个导入语句，例如：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> <span class="string">"fmt"</span></span><br><span class="line"><span class="keyword">import</span> <span class="string">"math"</span></span><br></pre></td></tr></table></figure><p>不过使用分组导入语句是更好的形式。</p><h2><span id="导出名">导出名</span></h2><p>在 Go 中，如果一个名字以大写字母开头，那么它就是已导出的。例如，<code>Pizza</code> 就是个已导出名，<code>Pi</code> 也同样，它导出自 <code>math</code> 包。</p><p><code>pizza</code> 和 <code>pi</code> 并未以大写字母开头，所以它们是未导出的。</p><p>在导入一个包时，你只能引用其中已导出的名字。任何“未导出”的名字在该包外均无法访问。</p><h2><span id="函数">函数</span></h2><p>函数可以没有参数或接受多个参数。</p><p>在本例中，<code>add</code> 接受两个 <code>int</code> 类型的参数。</p><p>注意类型在变量名 <strong>之后</strong>。</p><p>当连续两个或多个函数的已命名形参类型相同时，除最后一个类型以外，其它都可以省略。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">add</span><span class="params">(x <span class="keyword">int</span>, y <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> x + y</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 或者是</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">add</span><span class="params">(x, y <span class="keyword">int</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> x + y</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="多值返回">多值返回</span></h2><p>函数可以返回任意数量的返回值。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">swap</span><span class="params">(x, y <span class="keyword">string</span>)</span> <span class="params">(<span class="keyword">string</span>, <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> y, x</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="命名返回值">命名返回值</span></h2><p>Go 的返回值可被命名，它们会被视作定义在函数顶部的变量。</p><p>返回值的名称应当具有一定的意义，它可以作为文档使用。</p><p>没有参数的 <code>return</code> 语句返回已命名的返回值。也就是 <code>直接</code> 返回。</p><p>直接返回语句应当仅用在下面这样的短函数中。在长的函数中它们会影响代码的可读性。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">split</span><span class="params">(sum <span class="keyword">int</span>)</span> <span class="params">(x, y <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">x = sum * <span class="number">4</span> / <span class="number">9</span></span><br><span class="line">y = sum - x</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="函数值">函数值</span></h2><p>函数也是值。它们可以像其它值一样传递。</p><p>函数值可以用作函数的参数或返回值。</p><h2><span id="函数的闭包">函数的闭包</span></h2><p>Go 函数可以是一个闭包。闭包是一个函数值，它引用了其函数体之外的变量。该函数可以访问并赋予其引用的变量的值，换句话说，该函数被这些变量“绑定”在一起。</p><h2><span id="变量">变量</span></h2><p><code>var</code> 语句用于声明一个变量列表，跟函数的参数列表一样，类型在最后。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> c, python, java <span class="keyword">bool</span></span><br><span class="line"><span class="keyword">var</span> i <span class="keyword">int</span></span><br></pre></td></tr></table></figure><h2><span id="变量的初始化">变量的初始化</span></h2><p>变量声明可以包含初始值，每个变量对应一个。</p><p>如果初始化值已存在，则可以省略类型；变量会从初始值中获得类型。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> i, j <span class="keyword">int</span> = <span class="number">1</span>, <span class="number">2</span></span><br></pre></td></tr></table></figure><h2><span id="短变量声明">短变量声明</span></h2><p>在函数中，简洁赋值语句 <code>:=</code> 可在类型明确的地方代替 <code>var</code> 声明。</p><p>函数外的每个语句都必须以关键字开始（<code>var</code>, <code>func</code> 等等），因此 <code>:=</code> 结构不能在函数外使用。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> i, j <span class="keyword">int</span> = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">k := <span class="number">3</span></span><br><span class="line">c, python, java := <span class="literal">true</span>, <span class="literal">false</span>, <span class="string">"no!"</span></span><br></pre></td></tr></table></figure><h2><span id="基本类型">基本类型</span></h2><p>Go 的基本类型有</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">bool</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">string</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span>  <span class="keyword">int8</span>  <span class="keyword">int16</span>  <span class="keyword">int32</span>  <span class="keyword">int64</span></span><br><span class="line"><span class="keyword">uint</span> <span class="keyword">uint8</span> <span class="keyword">uint16</span> <span class="keyword">uint32</span> <span class="keyword">uint64</span> <span class="keyword">uintptr</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">byte</span> <span class="comment">// uint8 的别名</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">rune</span> <span class="comment">// int32 的别名</span></span><br><span class="line">    <span class="comment">// 表示一个 Unicode 码点</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">float32</span> <span class="keyword">float64</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">complex64</span> <span class="keyword">complex128</span></span><br></pre></td></tr></table></figure><h2><span id="零值">零值</span></h2><p>没有明确初始值的变量声明会被赋予它们的 <strong>零值</strong>。</p><p>零值是：</p><ul><li>数值类型为 <code>0</code>，</li><li>布尔类型为 <code>false</code>，</li><li>字符串为 <code>&quot;&quot;</code>（空字符串）。</li></ul><h2><span id="常量">常量</span></h2><p>常量的声明与变量类似，只不过是使用 <code>const</code> 关键字。</p><p>常量可以是字符、字符串、布尔值或数值。</p><p>常量不能用 <code>:=</code> 语法声明。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> Pi = <span class="number">3.14</span></span><br></pre></td></tr></table></figure><h2><span id="for">for</span></h2><p>Go 只有一种循环结构：<code>for</code> 循环。</p><p>基本的 <code>for</code> 循环由三部分组成，它们用分号隔开：</p><ul><li>初始化语句：在第一次迭代前执行</li><li>条件表达式：在每次迭代前求值</li><li>后置语句：在每次迭代的结尾执行</li></ul><p>初始化语句通常为一句短变量声明，该变量声明仅在 <code>for</code> 语句的作用域中可见。</p><p>一旦条件表达式的布尔值为 <code>false</code>，循环迭代就会终止。</p><p><strong>注意</strong>：和 C、Java、JavaScript 之类的语言不同，Go 的 for 语句后面的三个构成部分外没有小括号， 大括号 <code>{ }</code> 则是必须的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">10</span>; i++ &#123;</span><br><span class="line">sum += i</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 初始化语句和后置语句是可选的。</span></span><br><span class="line"><span class="keyword">for</span> ; sum &lt; <span class="number">1000</span>; &#123;</span><br><span class="line">sum += <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>for 是 Go 中的 “while”：此时你可以去掉分号，因为 C 的 <code>while</code> 在 Go 中叫做 <code>for</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> sum &lt; <span class="number">1000</span> &#123;</span><br><span class="line">sum += sum</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="if">if</span></h2><p>Go 的 <code>if</code> 语句与 <code>for</code> 循环类似，表达式外无需小括号 <code>( )</code> ，而大括号 <code>{ }</code> 则是必须的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> x &lt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span> sqrt(-x) + <span class="string">"i"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="if-的简短语句">if 的简短语句</span></h2><p>同 <code>for</code> 一样， <code>if</code> 语句可以在条件表达式前执行一个简单的语句。在 <code>if</code> 的简短语句中声明的变量同样可以在任何对应的 <code>else</code> 块中使用。</p><p>该语句声明的变量作用域仅在 <code>if</code> 之内。</p> <figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> v := math.Pow(x, n); v &lt; lim &#123;</span><br><span class="line">fmt.Println(<span class="number">1</span>)</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">    fmt.Printf(<span class="string">"%g &gt;= %g\n"</span>, v, lim)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="switch">switch</span></h2><p><code>switch</code> 是编写一连串 <code>if - else</code> 语句的简便方法。它运行第一个值等于条件表达式的 case 语句。</p><p>Go 的 switch 语句类似于 C、C++、Java、JavaScript 和 PHP 中的，不过 Go 只运行选定的 case，而非之后所有的 case。 实际上，Go 自动提供了在这些语言中每个 case 后面所需的 <code>break</code> 语句。 除非以 <code>fallthrough</code> 语句结束，否则分支会自动终止。 Go 的另一点重要的不同在于 switch 的 case 无需为常量，且取值不必为整数。</p><h2><span id="defer">defer</span></h2><p>defer 语句会将函数推迟到外层函数返回之后执行。</p><p>推迟调用的函数其参数会立即求值，但直到外层函数返回前该函数都不会被调用。</p><p>推迟的函数调用会被压入一个栈中。当外层函数返回时，被推迟的函数会按照后进先出的顺序调用。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">defer</span> fmt.Println(<span class="string">"world"</span>)</span><br><span class="line"></span><br><span class="line">fmt.Println(<span class="string">"hello"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="指针">指针</span></h2><p>Go 拥有指针。指针保存了值的内存地址。</p><p>类型 <code>*T</code> 是指向 <code>T</code> 类型值的指针。其零值为 <code>nil</code>。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> p *<span class="keyword">int</span></span><br></pre></td></tr></table></figure><p><code>&amp;</code> 操作符会生成一个指向其操作数的指针。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">i := <span class="number">42</span></span><br><span class="line">p = &amp;i</span><br></pre></td></tr></table></figure><p><code>*</code> 操作符表示指针指向的底层值。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fmt.Println(*p) <span class="comment">// 通过指针 p 读取 i</span></span><br><span class="line">*p = <span class="number">21</span>         <span class="comment">// 通过指针 p 设置 i</span></span><br></pre></td></tr></table></figure><p>这也就是通常所说的“间接引用”或“重定向”。</p><p>与 C 不同，Go 没有指针运算。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">i, j := <span class="number">42</span>, <span class="number">2701</span></span><br><span class="line"></span><br><span class="line">p := &amp;i         <span class="comment">// 指向 i</span></span><br><span class="line">fmt.Println(*p) <span class="comment">// 通过指针读取 i 的值,42</span></span><br><span class="line">*p = <span class="number">21</span>         <span class="comment">// 通过指针设置 i 的值</span></span><br><span class="line">fmt.Println(i)  <span class="comment">// 查看 i 的值,21</span></span><br><span class="line"></span><br><span class="line">p = &amp;j         <span class="comment">// 指向 j</span></span><br><span class="line">*p = *p / <span class="number">37</span>   <span class="comment">// 通过指针对 j 进行除法运算</span></span><br><span class="line">fmt.Println(j) <span class="comment">// 查看 j 的值,73</span></span><br></pre></td></tr></table></figure><h2><span id="结构体">结构体</span></h2><p>一个结构体（<code>struct</code>）就是一组字段（field）。</p><p>结构体字段使用点号来访问。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Vertex <span class="keyword">struct</span> &#123;</span><br><span class="line">X <span class="keyword">int</span></span><br><span class="line">Y <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line">v := Vertex&#123;<span class="number">1</span>, <span class="number">2</span>&#125;</span><br><span class="line">v.X = <span class="number">4</span></span><br></pre></td></tr></table></figure><h2><span id="结构体指针">结构体指针</span></h2><p>结构体字段可以通过结构体指针来访问。</p><p>如果我们有一个指向结构体的指针 <code>p</code>，那么可以通过 <code>(*p).X</code> 来访问其字段 <code>X</code>。不过这么写太啰嗦了，所以语言也允许我们使用隐式间接引用，直接写 <code>p.X</code> 就可以。</p><h2><span id="结构体文法">结构体文法</span></h2><p>结构体文法通过直接列出字段的值来新分配一个结构体。</p><p>使用 <code>Name:</code> 语法可以仅列出部分字段。（字段名的顺序无关。）</p><p>特殊的前缀 <code>&amp;</code> 返回一个指向结构体的指针。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> (</span><br><span class="line">v1 = Vertex&#123;<span class="number">1</span>, <span class="number">2</span>&#125;  <span class="comment">// 创建一个 Vertex 类型的结构体</span></span><br><span class="line">v2 = Vertex&#123;X: <span class="number">1</span>&#125;  <span class="comment">// Y:0 被隐式地赋予</span></span><br><span class="line">v3 = Vertex&#123;&#125;      <span class="comment">// X:0 Y:0</span></span><br><span class="line">p  = &amp;Vertex&#123;<span class="number">1</span>, <span class="number">2</span>&#125; <span class="comment">// 创建一个 *Vertex 类型的结构体（指针）</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2><span id="数组">数组</span></h2><p>类型 <code>[n]T</code> 表示拥有 <code>n</code> 个 <code>T</code> 类型的值的数组。</p><p>表达式</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> a [<span class="number">10</span>]<span class="keyword">int</span></span><br></pre></td></tr></table></figure><p>会将变量 <code>a</code> 声明为拥有 10 个整数的数组。</p><p>数组的长度是其类型的一部分，因此数组不能改变大小。这看起来是个限制，不过没关系，Go 提供了更加便利的方式来使用数组。</p><h2><span id="切片">切片</span></h2><p>每个数组的大小都是固定的。而切片则为数组元素提供动态大小的、灵活的视角。在实践中，切片比数组更常用。</p><p>类型 <code>[]T</code> 表示一个元素类型为 <code>T</code> 的切片。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">primes := [<span class="number">6</span>]<span class="keyword">int</span>&#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>&#125;</span><br><span class="line"><span class="keyword">var</span> s []<span class="keyword">int</span> = primes[<span class="number">1</span>:<span class="number">4</span>]</span><br><span class="line">fmt.Println(s)</span><br></pre></td></tr></table></figure><p>切片通过两个下标来界定，即一个上界和一个下界，二者以冒号分隔：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[low : high]</span><br></pre></td></tr></table></figure><p>它会选择一个半开区间，包括第一个元素，但排除最后一个元素。切片下界的默认值为 <code>0</code>，上界则是该切片的长度。</p><p>以下表达式创建了一个切片，它包含 <code>a</code> 中下标从 1 到 3 的元素：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>:<span class="number">4</span>]</span><br></pre></td></tr></table></figure><p>切片就像数组的引用。切片并不存储任何数据，它只是描述了底层数组中的一段。更改切片的元素会修改其底层数组中对应的元素。与它共享底层数组的切片都会观测到这些修改。切片文法类似于没有长度的数组文法。</p><p>这是一个数组文法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">3</span>]<span class="keyword">bool</span>&#123;<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>&#125;</span><br></pre></td></tr></table></figure><p>下面这样则会创建一个和上面相同的数组，然后构建一个引用了它的切片：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[]<span class="keyword">bool</span>&#123;<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>&#125;</span><br></pre></td></tr></table></figure><p>切片拥有 <strong>长度</strong> 和 <strong>容量</strong>。切片的长度就是它所包含的元素个数。切片的容量是从它的第一个元素开始数，到其底层数组元素末尾的个数。切片 <code>s</code> 的长度和容量可通过表达式 <code>len(s)</code> 和 <code>cap(s)</code> 来获取。你可以通过重新切片来扩展一个切片，给它提供足够的容量。</p><p>切片的零值是 <code>nil</code>。<code>nil</code> 切片的长度和容量为 0 且没有底层数组。</p><h2><span id="用-make-创建切片">用 make 创建切片</span></h2><p>切片可以用内建函数 <code>make</code> 来创建，这也是你创建动态数组的方式。</p><p><code>make</code> 函数会分配一个元素为零值的数组并返回一个引用了它的切片：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">5</span>)  <span class="comment">// len(a)=5</span></span><br></pre></td></tr></table></figure><p>要指定它的容量，需向 <code>make</code> 传入第三个参数：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b := <span class="built_in">make</span>([]<span class="keyword">int</span>, <span class="number">0</span>, <span class="number">5</span>) <span class="comment">// len(b)=0, cap(b)=5</span></span><br><span class="line"></span><br><span class="line">b = b[:<span class="built_in">cap</span>(b)] <span class="comment">// len(b)=5, cap(b)=5</span></span><br><span class="line">b = b[<span class="number">1</span>:]      <span class="comment">// len(b)=4, cap(b)=4</span></span><br></pre></td></tr></table></figure><p>切片可包含任何类型，甚至包括其它的切片。</p><h2><span id="向切片追加元素">向切片追加元素</span></h2><p>为切片追加新的元素是种常用的操作，为此 Go 提供了内建的 <code>append</code> 函数。内建函数的<a href="https://go-zh.org/pkg/builtin/#append" target="_blank" rel="noopener">文档</a>对此函数有详细的介绍。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">append</span><span class="params">(s []T, vs ...T)</span> []<span class="title">T</span></span></span><br><span class="line"><span class="function"><span class="title">s</span> = <span class="title">append</span><span class="params">(s, 2, 3, 4)</span></span></span><br></pre></td></tr></table></figure><p><code>append</code> 的第一个参数 <code>s</code> 是一个元素类型为 <code>T</code> 的切片，其余类型为 <code>T</code> 的值将会追加到该切片的末尾。</p><p><code>append</code> 的结果是一个包含原切片所有元素加上新添加元素的切片。</p><p>当 <code>s</code> 的底层数组太小，不足以容纳所有给定的值时，它就会分配一个更大的数组。返回的切片会指向这个新分配的数组。</p><h2><span id="range">Range</span></h2><p><code>for</code> 循环的 <code>range</code> 形式可遍历切片或映射。</p><p>当使用 <code>for</code> 循环遍历切片时，每次迭代都会返回两个值。第一个值为当前元素的下标，第二个值为该下标所对应元素的一份副本。</p><p>可以将下标或值赋予 <code>_</code> 来忽略它。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, _ := <span class="keyword">range</span> pow</span><br><span class="line"><span class="keyword">for</span> _, value := <span class="keyword">range</span> pow</span><br></pre></td></tr></table></figure><p>若你只需要索引，忽略第二个变量即可。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> pow</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> pow = []<span class="keyword">int</span>&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>&#125;</span><br><span class="line"><span class="keyword">for</span> i, v := <span class="keyword">range</span> pow &#123;</span><br><span class="line">fmt.Printf(<span class="string">"2**%d = %d\n"</span>, i, v)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//输出</span></span><br><span class="line"><span class="number">2</span>**<span class="number">0</span> = <span class="number">1</span></span><br><span class="line"><span class="number">2</span>**<span class="number">1</span> = <span class="number">2</span></span><br><span class="line"><span class="number">2</span>**<span class="number">2</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure><h2><span id="映射">映射</span></h2><p>映射将键映射到值。映射的文法与结构体相似，不过必须有键名。<code>map[键值类型] 值类型</code></p><p>映射的零值为 <code>nil</code> 。<code>nil</code> 映射既没有键，也不能添加键。</p><p><code>make</code> 函数会返回给定类型的映射，并将其初始化备用。若顶级类型只是一个类型名，你可以在文法的元素中省略它。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Vertex <span class="keyword">struct</span> &#123;</span><br><span class="line">Lat, Long <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> m <span class="keyword">map</span>[<span class="keyword">string</span>]Vertex</span><br><span class="line">m = <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]Vertex)</span><br><span class="line">m[<span class="string">"Bell Labs"</span>] = Vertex&#123;</span><br><span class="line">    <span class="number">40.68433</span>, <span class="number">-74.39967</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> m = <span class="keyword">map</span>[<span class="keyword">string</span>]Vertex&#123;</span><br><span class="line"><span class="string">"Bell Labs"</span>: &#123;<span class="number">40.68433</span>, <span class="number">-74.39967</span>&#125;,</span><br><span class="line"><span class="string">"Google"</span>:    &#123;<span class="number">37.42202</span>, <span class="number">-122.08408</span>&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="修改映射">修改映射</span></h2><p>在映射 <code>m</code> 中插入或修改元素：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m[key] = elem</span><br></pre></td></tr></table></figure><p>获取元素：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elem = m[key]</span><br></pre></td></tr></table></figure><p>删除元素：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">delete</span>(m, key)</span><br></pre></td></tr></table></figure><p>通过双赋值检测某个键是否存在：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elem, ok = m[key]</span><br></pre></td></tr></table></figure><p>若 <code>key</code> 在 <code>m</code> 中，<code>ok</code> 为 <code>true</code> ；否则，<code>ok</code> 为 <code>false</code>。</p><p>若 <code>key</code> 不在映射中，那么 <code>elem</code> 是该映射元素类型的零值。</p><p>同样的，当从映射中读取某个不存在的键时，结果是映射的元素类型的零值。</p><p><strong>注</strong> ：若 <code>elem</code> 或 <code>ok</code> 还未声明，你可以使用短变量声明：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elem, ok := m[key]</span><br></pre></td></tr></table></figure><h2><span id="方法">方法</span></h2><p>Go 没有类。不过你可以为结构体类型定义方法。</p><p>方法就是一类带特殊的 <strong>接收者</strong> 参数的函数。</p><p>方法接收者在它自己的参数列表内，位于 <code>func</code> 关键字和方法名之间。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Vertex <span class="keyword">struct</span> &#123;</span><br><span class="line">X, Y <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v Vertex)</span> <span class="title">Abs</span><span class="params">()</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> math.Sqrt(v.X*v.X + v.Y*v.Y)</span><br><span class="line">&#125;</span><br><span class="line">v := Vertex&#123;<span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line">fmt.Println(v.Abs())</span><br></pre></td></tr></table></figure><p><strong>方法即函数</strong>。记住：方法只是个带接收者参数的函数。现在这个 <code>Abs</code> 的写法就是个正常的函数，功能并没有什么变化。</p><h2><span id="指针接收者">指针接收者</span></h2><p>你可以为指针接收者声明方法。</p><p>这意味着对于某类型 <code>T</code>，接收者的类型可以用 <code>*T</code> 的文法。（此外，<code>T</code> 不能是像 <code>*int</code> 这样的指针。）</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"math"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Vertex <span class="keyword">struct</span> &#123;</span><br><span class="line">X, Y <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v Vertex)</span> <span class="title">Abs</span><span class="params">()</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> math.Sqrt(v.X*v.X + v.Y*v.Y)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *Vertex)</span> <span class="title">Scale</span><span class="params">(f <span class="keyword">float64</span>)</span></span> &#123;</span><br><span class="line">v.X = v.X * f</span><br><span class="line">v.Y = v.Y * f</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">v := Vertex&#123;<span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line">v.Scale(<span class="number">10</span>)</span><br><span class="line">fmt.Println(v.Abs())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例如，这里为 <code>*Vertex</code> 定义了 <code>Scale</code> 方法。</p><p>指针接收者的方法可以修改接收者指向的值（就像 <code>Scale</code> 在这做的）。由于方法经常需要修改它的接收者，指针接收者比值接收者更常用。</p><p>若使用值接收者，那么 <code>Scale</code> 方法会对原始 <code>Vertex</code> 值的副本进行操作。（对于函数的其它参数也是如此。）<code>Scale</code> 方法必须用指针接受者来更改 <code>main</code> 函数中声明的 <code>Vertex</code> 的值。</p><h2><span id="方法与指针重定向">方法与指针重定向</span></h2><p>比较前两个程序，你大概会注意到带指针参数的函数必须接受一个指针：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ScaleFunc</span><span class="params">(v *Vertex, f <span class="keyword">float64</span>)</span></span> &#123;</span><br><span class="line">v.X = v.X * f</span><br><span class="line">v.Y = v.Y * f</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> v Vertex</span><br><span class="line">ScaleFunc(v, <span class="number">5</span>)  <span class="comment">// 编译错误！</span></span><br><span class="line">ScaleFunc(&amp;v, <span class="number">5</span>) <span class="comment">// OK</span></span><br></pre></td></tr></table></figure><p>而以指针为接收者的方法被调用时，接收者既能为值又能为指针：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *Vertex)</span> <span class="title">Scale</span><span class="params">(f <span class="keyword">float64</span>)</span></span> &#123;</span><br><span class="line">v.X = v.X * f</span><br><span class="line">v.Y = v.Y * f</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> v Vertex</span><br><span class="line">v.Scale(<span class="number">5</span>)  <span class="comment">// OK</span></span><br><span class="line">p := &amp;v</span><br><span class="line">p.Scale(<span class="number">10</span>) <span class="comment">// OK</span></span><br></pre></td></tr></table></figure><p>对于语句 <code>v.Scale(5)</code>，即便 <code>v</code> 是个值而非指针，带指针接收者的方法也能被直接调用。 也就是说，由于 <code>Scale</code> 方法有一个指针接收者，为方便起见，Go 会将语句 <code>v.Scale(5)</code> 解释为 <code>(&amp;v).Scale(5)</code>。</p><p>同样的事情也发生在相反的方向。</p><p>接受一个值作为参数的函数必须接受一个指定类型的值：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">AbsFunc</span><span class="params">(v Vertex)</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> math.Sqrt(v.X*v.X + v.Y*v.Y)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> v Vertex</span><br><span class="line">fmt.Println(AbsFunc(v))  <span class="comment">// OK</span></span><br><span class="line">fmt.Println(AbsFunc(&amp;v)) <span class="comment">// 编译错误！</span></span><br></pre></td></tr></table></figure><p>而以值为接收者的方法被调用时，接收者既能为值又能为指针：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v Vertex)</span> <span class="title">Abs</span><span class="params">()</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> math.Sqrt(v.X*v.X + v.Y*v.Y)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> v Vertex</span><br><span class="line">fmt.Println(v.Abs()) <span class="comment">// OK</span></span><br><span class="line">p := &amp;v</span><br><span class="line">fmt.Println(p.Abs()) <span class="comment">// OK</span></span><br></pre></td></tr></table></figure><p>这种情况下，方法调用 <code>p.Abs()</code> 会被解释为 <code>(*p).Abs()</code>。</p><h2><span id="选择值或指针作为接收者">选择值或指针作为接收者</span></h2><p>使用指针接收者的原因有二：</p><p>首先，方法能够修改其接收者指向的值。</p><p>其次，这样可以避免在每次调用方法时复制该值。若值的类型为大型结构体时，这样做会更加高效。</p><p>通常来说，所有给定类型的方法都应该有值或指针接收者，但并不应该二者混用。</p><h2><span id="接口">接口</span></h2><p><strong>接口类型</strong> 是由一组方法签名定义的集合。</p><p>接口类型的变量可以保存任何实现了这些方法的值。 也就是说如果接口体内有一个方法名，那么这个方法的实现会自动被接口内的方法捕获到。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"math"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Abser <span class="keyword">interface</span> &#123; <span class="comment">// 接口</span></span><br><span class="line">Abs() <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> a Abser</span><br><span class="line">f := MyFloat(-math.Sqrt2)</span><br><span class="line">v := Vertex&#123;<span class="number">3</span>, <span class="number">4</span>&#125;</span><br><span class="line"></span><br><span class="line">a = f  <span class="comment">// a MyFloat 实现了 Abser</span></span><br><span class="line">a = &amp;v <span class="comment">// a *Vertex 实现了 Abser</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 下面一行，v 是一个 Vertex（而不是 *Vertex），所以没有实现 Abser。</span></span><br><span class="line"><span class="comment">// a = v</span></span><br><span class="line"></span><br><span class="line">fmt.Println(a.Abs())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> MyFloat <span class="keyword">float64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(f MyFloat)</span> <span class="title">Abs</span><span class="params">()</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">float64</span>(-f)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">float64</span>(f)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Vertex <span class="keyword">struct</span> &#123;</span><br><span class="line">X, Y <span class="keyword">float64</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v *Vertex)</span> <span class="title">Abs</span><span class="params">()</span> <span class="title">float64</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> math.Sqrt(v.X*v.X + v.Y*v.Y)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="接口与隐式实现">接口与隐式实现</span></h2><p>类型通过实现一个接口的所有方法来实现该接口。既然无需专门显式声明，也就没有“implements”关键字。</p><p>隐式接口从接口的实现中解耦了定义，这样接口的实现可以出现在任何包中，无需提前准备。</p><p>因此，也就无需在每一个实现上增加新的接口名称，这样同时也鼓励了明确的接口定义。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> I <span class="keyword">interface</span> &#123;</span><br><span class="line">M()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> T <span class="keyword">struct</span> &#123;</span><br><span class="line">S <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 此方法表示类型 T 实现了接口 I，但我们无需显式声明此事。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t T)</span> <span class="title">M</span><span class="params">()</span></span> &#123;</span><br><span class="line">fmt.Println(t.S)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="接口值">接口值</span></h2><p>接口也是值。它们可以像其它值一样传递。接口值可以用作函数的参数或返回值。</p><p>在内部，接口值可以看做包含值和具体类型的元组：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(value, <span class="keyword">type</span>)</span><br></pre></td></tr></table></figure><p>接口值保存了一个具体底层类型的具体值。<strong>接口值调用方法时会执行其底层类型的同名方法。</strong></p><h2><span id="底层值为-nil-的接口值">底层值为 nil 的接口值</span></h2><p>即便接口内的具体值为 nil，方法仍然会被 nil 接收者调用。</p><p>在一些语言中，这会触发一个空指针异常，但在 Go 中通常会写一些方法来优雅地处理它。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(t *T)</span> <span class="title">M</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> t == <span class="literal">nil</span> &#123;</span><br><span class="line">fmt.Println(<span class="string">"&lt;nil&gt;"</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(t.S)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意:</strong> 保存了 nil 具体值的接口其自身并不为 nil。</p><h2><span id="nil-接口值">nil 接口值</span></h2><p>nil 接口值既不保存值也不保存具体类型。</p><p>为 nil 接口调用方法会产生运行时错误，因为接口的元组内并未包含能够指明该调用哪个 <strong>具体</strong> 方法的类型。</p><h2><span id="空接口">空接口</span></h2><p>指定了零个方法的接口值被称为 <em>空接口：</em></p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure><p>空接口可保存任何类型的值。（因为每个类型都至少实现了零个方法。）</p><p>空接口被用来处理未知类型的值。例如，<code>fmt.Print</code> 可接受类型为 <code>interface{}</code> 的任意数量的参数。</p><h2><span id="类型断言">类型断言</span></h2><p><strong>类型断言</strong> 提供了访问接口值底层具体值的方式。</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">t</span> <span class="symbol">:</span>= i.(<span class="built_in">T</span>)</span><br></pre></td></tr></table></figure><p>该语句断言接口值 <code>i</code> 保存了具体类型 <code>T</code>，并将其底层类型为 <code>T</code> 的值赋予变量 <code>t</code>。若 <code>i</code> 并未保存 <code>T</code> 类型的值，该语句就会触发一个恐慌。</p><p>为了 <strong>判断</strong> 一个接口值是否保存了一个特定的类型，类型断言可返回两个值：其底层值以及一个报告断言是否成功的布尔值。</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">t</span>, ok <span class="symbol">:</span>= i.(<span class="built_in">T</span>)</span><br></pre></td></tr></table></figure><p>若 <code>i</code> 保存了一个 <code>T</code>，那么 <code>t</code> 将会是其底层值，而 <code>ok</code> 为 <code>true</code>。</p><p>否则，<code>ok</code> 将为 <code>false</code> 而 <code>t</code> 将为 <code>T</code> 类型的零值，程序并不会产生恐慌。</p><p>请注意这种语法和读取一个映射时的相同之处。</p><h2><span id="类型选择">类型选择</span></h2><p><strong>类型选择</strong> 是一种按顺序从几个类型断言中选择分支的结构。</p><p>类型选择与一般的 switch 语句相似，不过类型选择中的 case 为类型（而非值）， 它们针对给定接口值所存储的值的类型进行比较。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> v := i.(<span class="keyword">type</span>) &#123;</span><br><span class="line"><span class="keyword">case</span> T:</span><br><span class="line">    <span class="comment">// v 的类型为 T</span></span><br><span class="line"><span class="keyword">case</span> S:</span><br><span class="line">    <span class="comment">// v 的类型为 S</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// 没有匹配，v 与 i 的类型相同</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>类型选择中的声明与类型断言 <code>i.(T)</code> 的语法相同，只是具体类型 <code>T</code> 被替换成了关键字 <code>type</code>。</p><p>此选择语句判断接口值 <code>i</code> 保存的值类型是 <code>T</code> 还是 <code>S</code>。在 <code>T</code> 或 <code>S</code> 的情况下，变量 <code>v</code> 会分别按 <code>T</code> 或 <code>S</code> 类型保存 <code>i</code> 拥有的值。在默认（即没有匹配）的情况下，变量 <code>v</code> 与 <code>i</code> 的接口类型和值相同。</p><h2><span id="stringer">Stringer</span></h2><p><a href="https://go-zh.org/pkg/fmt/" target="_blank" rel="noopener"><code>fmt</code></a> 包中定义的 <a href="https://go-zh.org/pkg/fmt/#Stringer" target="_blank" rel="noopener"><code>Stringer</code></a> 是最普遍的接口之一。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Stringer <span class="keyword">interface</span> &#123;</span><br><span class="line">    String() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>Stringer</code> 是一个可以用字符串描述自己的类型。<code>fmt</code> 包（还有很多包）都通过此接口来打印值。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span> &#123;</span><br><span class="line">Name <span class="keyword">string</span></span><br><span class="line">Age  <span class="keyword">int</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p Person)</span> <span class="title">String</span><span class="params">()</span> <span class="title">string</span></span> &#123; </span><br><span class="line">    <span class="comment">// 如果写了该方法，输出为“Arthur Dent (42 years) Zaphod Beeblebrox (9001 years)”，输出的是string内的方法</span></span><br><span class="line"><span class="keyword">return</span> fmt.Sprintf(<span class="string">"%v (%v years)"</span>, p.Name, p.Age)</span><br><span class="line">    <span class="comment">// 如果没写，输出就是“&#123;Arthur Dent 42&#125; &#123;Zaphod Beeblebrox 9001&#125;”，输出的是变量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">a := Person&#123;<span class="string">"Arthur Dent"</span>, <span class="number">42</span>&#125;</span><br><span class="line">z := Person&#123;<span class="string">"Zaphod Beeblebrox"</span>, <span class="number">9001</span>&#125;</span><br><span class="line">fmt.Println(a, z)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="错误">错误</span></h2><p>Go 程序使用 <code>error</code> 值来表示错误状态。</p><p>与 <code>fmt.Stringer</code> 类似，<code>error</code> 类型是一个内建接口：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> error <span class="keyword">interface</span> &#123;</span><br><span class="line">    Error() <span class="keyword">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（与 <code>fmt.Stringer</code> 类似，<code>fmt</code> 包在打印值时也会满足 <code>error</code>。）</p><p>通常函数会返回一个 <code>error</code> 值，调用的它的代码应当判断这个错误是否等于 <code>nil</code> 来进行错误处理。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">i, err := strconv.Atoi(<span class="string">"42"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    fmt.Printf(<span class="string">"couldn't convert number: %v\n"</span>, err)</span><br><span class="line">    <span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(<span class="string">"Converted integer:"</span>, i)</span><br></pre></td></tr></table></figure><p><code>error</code> 为 nil 时表示成功；非 nil 的 <code>error</code> 表示失败。</p><h2><span id="reader">Reader</span></h2><p><code>io</code> 包指定了 <code>io.Reader</code> 接口，它表示从数据流的末尾进行读取。</p><p>Go 标准库包含了该接口的<a href="https://go-zh.org/search?q=Read#Global" target="_blank" rel="noopener">许多实现</a>，包括文件、网络连接、压缩和加密等等。</p><p><code>io.Reader</code> 接口有一个 <code>Read</code> 方法：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(T)</span> <span class="title">Read</span><span class="params">(b []<span class="keyword">byte</span>)</span> <span class="params">(n <span class="keyword">int</span>, err error)</span> //每次<span class="title">n</span>字节读</span></span><br></pre></td></tr></table></figure><p><code>Read</code> 用数据填充给定的字节切片并返回填充的字节数和错误值。在遇到数据流的结尾时，它会返回一个 <code>io.EOF</code> 错误。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"io"</span></span><br><span class="line"><span class="string">"strings"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">r := strings.NewReader(<span class="string">"Hello, Reader!"</span>)</span><br><span class="line"></span><br><span class="line">b := <span class="built_in">make</span>([]<span class="keyword">byte</span>, <span class="number">8</span>)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line">n, err := r.Read(b)</span><br><span class="line">fmt.Printf(<span class="string">"n = %v err = %v b = %v\n"</span>, n, err, b)</span><br><span class="line">fmt.Printf(<span class="string">"b[:n] = %q\n"</span>, b[:n])</span><br><span class="line"><span class="keyword">if</span> err == io.EOF &#123;</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="图像">图像</span></h2><p><a href="https://go-zh.org/pkg/image/#Image" target="_blank" rel="noopener"><code>image</code></a> 包定义了 <code>Image</code> 接口：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> image</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Image <span class="keyword">interface</span> &#123;</span><br><span class="line">    ColorModel() color.Model</span><br><span class="line">    Bounds() Rectangle</span><br><span class="line">    At(x, y <span class="keyword">int</span>) color.Color</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意:</strong> <code>Bounds</code> 方法的返回值 <code>Rectangle</code> 实际上是一个 <a href="https://go-zh.org/pkg/image/#Rectangle" target="_blank" rel="noopener"><code>image.Rectangle</code></a>，它在 <code>image</code> 包中声明。</p><p>（请参阅<a href="https://go-zh.org/pkg/image/#Image" target="_blank" rel="noopener">文档</a>了解全部信息。）</p><p><code>color.Color</code> 和 <code>color.Model</code> 类型也是接口，但是通常因为直接使用预定义的实现 <code>image.RGBA</code> 和 <code>image.RGBAModel</code> 而被忽视了。这些接口和类型由 <a href="https://go-zh.org/pkg/image/color/" target="_blank" rel="noopener"><code>image/color</code></a> 包定义。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"image"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">m := image.NewRGBA(image.Rect(<span class="number">0</span>, <span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">fmt.Println(m.Bounds())</span><br><span class="line">fmt.Println(m.At(<span class="number">0</span>, <span class="number">0</span>).RGBA())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="go-程">Go 程</span></h2><p>Go 程（goroutine）是由 Go 运行时管理的轻量级线程。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">go</span> f(x, y, z)</span><br></pre></td></tr></table></figure><p>会启动一个新的 Go 程并执行</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(x, y, z)</span><br></pre></td></tr></table></figure><p><code>f</code>, <code>x</code>, <code>y</code> 和 <code>z</code> 的求值发生在当前的 Go 程中，而 <code>f</code> 的执行发生在新的 Go 程中。</p><p>Go 程在相同的地址空间中运行，因此在访问共享的内存时必须进行同步。<a href="https://go-zh.org/pkg/sync/" target="_blank" rel="noopener"><code>sync</code></a> 包提供了这种能力，不过在 Go 中并不经常用到，因为还有其它的办法（见下面）。</p><h2><span id="信道">信道</span></h2><p>信道是带有类型的管道，你可以通过它用信道操作符 <code>&lt;-</code> 来发送或者接收值。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ch &lt;- v    <span class="comment">// 将 v 发送至信道 ch。</span></span><br><span class="line">v := &lt;-ch  <span class="comment">// 从 ch 接收值并赋予 v。</span></span><br></pre></td></tr></table></figure><p>（“箭头”就是数据流的方向。）</p><p>和映射与切片一样，信道在使用前必须创建：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br></pre></td></tr></table></figure><p>默认情况下，发送和接收操作在另一端准备好之前都会阻塞。这使得 Go 程可以在没有显式的锁或竞态变量的情况下进行同步。</p><p>以下示例对切片中的数进行求和，将任务分配给两个 Go 程。一旦两个 Go 程完成了它们的计算，它就能算出最终的结果。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">sum</span><span class="params">(s []<span class="keyword">int</span>, c <span class="keyword">chan</span> <span class="keyword">int</span>, t <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">fmt.Println(t)</span><br><span class="line">sum := <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _, v := <span class="keyword">range</span> s &#123;</span><br><span class="line">sum += v</span><br><span class="line">&#125;</span><br><span class="line">c &lt;- sum <span class="comment">// 将和送入 c</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">s := []<span class="keyword">int</span>&#123;<span class="number">7</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">-9</span>, <span class="number">4</span>, <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>)</span><br><span class="line"><span class="keyword">go</span> sum(s[:<span class="built_in">len</span>(s)/<span class="number">2</span>], c ,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">go</span> sum(s[<span class="built_in">len</span>(s)/<span class="number">2</span>:], c ,<span class="number">2</span>)</span><br><span class="line"><span class="keyword">go</span> sum(s[:<span class="built_in">len</span>(s)/<span class="number">2</span><span class="number">-1</span>], c ,<span class="number">3</span>)</span><br><span class="line">x, y, z := &lt;-c, &lt;-c, &lt;-c <span class="comment">// 从 c 中接收</span></span><br><span class="line">    <span class="comment">// 执行的顺序竟然是3 1 2</span></span><br><span class="line">fmt.Println(x, y, z)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="带缓冲的信道">带缓冲的信道</span></h2><p>信道可以是 <em>带缓冲的</em>。将缓冲长度作为第二个参数提供给 <code>make</code> 来初始化一个带缓冲的信道：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">100</span>)</span><br><span class="line">ch &lt;- <span class="number">1</span></span><br></pre></td></tr></table></figure><p>仅当信道的缓冲区填满后，向其发送数据时才会阻塞。当缓冲区为空时，接受方会阻塞。</p><h2><span id="range-和-close">range 和 close</span></h2><p>发送者可通过 <code>close</code> 关闭一个信道来表示没有需要发送的值了。接收者可以通过为接收表达式分配第二个参数来测试信道是否被关闭：若没有值可以接收且信道已被关闭，那么在执行完</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v, ok := &lt;-ch</span><br></pre></td></tr></table></figure><p>之后 <code>ok</code> 会被设置为 <code>false</code>。</p><p>循环 <code>for i := range ch</code> 会不断从信道接收值，直到它被关闭。</p><p><em>注意：</em> 只有发送者才能关闭信道，而接收者不能。向一个已经关闭的信道发送数据会引发程序恐慌（panic）。</p><p><em>还要注意：</em> 信道与文件不同，通常情况下无需关闭它们。只有在必须告诉接收者不再有需要发送的值时才有必要关闭，例如终止一个 <code>range</code> 循环。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">fibonacci</span><span class="params">(n <span class="keyword">int</span>, c <span class="keyword">chan</span> <span class="keyword">int</span>)</span></span> &#123;</span><br><span class="line">x, y := <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; n; i++ &#123;</span><br><span class="line">c &lt;- x</span><br><span class="line">x, y = y, x+y</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">close</span>(c)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">c := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">int</span>, <span class="number">10</span>)</span><br><span class="line"><span class="keyword">go</span> fibonacci(<span class="built_in">cap</span>(c), c)</span><br><span class="line"><span class="keyword">for</span> i := <span class="keyword">range</span> c &#123;</span><br><span class="line">fmt.Println(i)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="select-语句">select 语句</span></h2><p><code>select</code> 语句使一个 Go 程可以等待多个通信操作。</p><p><code>select</code> 会阻塞到某个分支可以继续执行为止，这时就会执行该分支。当多个分支都准备好时会随机选择一个执行。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">tick := time.Tick(<span class="number">100</span> * time.Millisecond)</span><br><span class="line">boom := time.After(<span class="number">500</span> * time.Millisecond)</span><br><span class="line"><span class="keyword">for</span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-tick:</span><br><span class="line">fmt.Println(<span class="string">"tick."</span>)</span><br><span class="line"><span class="keyword">case</span> &lt;-boom:</span><br><span class="line">fmt.Println(<span class="string">"BOOM!"</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">fmt.Println(<span class="string">"    ."</span>)</span><br><span class="line">time.Sleep(<span class="number">50</span> * time.Millisecond)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当 <code>select</code> 中的其它分支都没有准备好时，<code>default</code> 分支就会执行。</p><p>为了在尝试发送或者接收时不发生阻塞，可使用 <code>default</code> 分支：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> i := &lt;-c:</span><br><span class="line">    <span class="comment">// 使用 i</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line">    <span class="comment">// 从 c 中接收会阻塞时执行</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="syncmutex">sync.Mutex</span></h2><p>我们已经看到信道非常适合在各个 Go 程间进行通信。</p><p>但是如果我们并不需要通信呢？比如说，若我们只是想保证每次只有一个 Go 程能够访问一个共享的变量，从而避免冲突？</p><p>这里涉及的概念叫做 <em>互斥（mutual</em>exclusion）<em> ，我们通常使用 </em>互斥锁（Mutex）* 这一数据结构来提供这种机制。</p><p>Go 标准库中提供了 <a href="https://go-zh.org/pkg/sync/#Mutex" target="_blank" rel="noopener"><code>sync.Mutex</code></a> 互斥锁类型及其两个方法：</p><ul><li><code>Lock</code></li><li><code>Unlock</code></li></ul><p>我们可以通过在代码前调用 <code>Lock</code> 方法，在代码后调用 <code>Unlock</code> 方法来保证一段代码的互斥执行。参见 <code>Inc</code> 方法。</p><p>我们也可以用 <code>defer</code> 语句来保证互斥锁一定会被解锁。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line"><span class="string">"fmt"</span></span><br><span class="line"><span class="string">"sync"</span></span><br><span class="line"><span class="string">"time"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SafeCounter 的并发使用是安全的。</span></span><br><span class="line"><span class="keyword">type</span> SafeCounter <span class="keyword">struct</span> &#123;</span><br><span class="line">v   <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span></span><br><span class="line">mux sync.Mutex</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Inc 增加给定 key 的计数器的值。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *SafeCounter)</span> <span class="title">Inc</span><span class="params">(key <span class="keyword">string</span>)</span></span> &#123;</span><br><span class="line">c.mux.Lock()</span><br><span class="line"><span class="comment">// Lock 之后同一时刻只有一个 goroutine 能访问 c.v</span></span><br><span class="line">c.v[key]++</span><br><span class="line">c.mux.Unlock()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Value 返回给定 key 的计数器的当前值。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *SafeCounter)</span> <span class="title">Value</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">int</span></span> &#123;</span><br><span class="line">c.mux.Lock()</span><br><span class="line"><span class="comment">// Lock 之后同一时刻只有一个 goroutine 能访问 c.v</span></span><br><span class="line"><span class="keyword">defer</span> c.mux.Unlock()</span><br><span class="line"><span class="keyword">return</span> c.v[key]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">c := SafeCounter&#123;v: <span class="built_in">make</span>(<span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">int</span>)&#125;</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++ &#123;</span><br><span class="line"><span class="keyword">go</span> c.Inc(<span class="string">"somekey"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">time.Sleep(time.Second)</span><br><span class="line">fmt.Println(c.Value(<span class="string">"somekey"</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/11/08/Go学习笔记" target="_blank" rel="noopener">https://vodkazy.cn/2019/11/08/Go学习笔记</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于分布的作业要求用Golang语言实现，所以参照tutorial学习了一下。&lt;/p&gt;
    
    </summary>
    
      <category term="Go" scheme="http://hhu1506010220.github.io/categories/Go/"/>
    
    
      <category term="Go" scheme="http://hhu1506010220.github.io/tags/Go/"/>
    
  </entry>
  
  <entry>
    <title>利用GParted-liveCD进行Deepin分区大小调整</title>
    <link href="http://hhu1506010220.github.io/2019/09/21/%E5%88%A9%E7%94%A8GParted-liveCD%E8%BF%9B%E8%A1%8CDeepin%E5%88%86%E5%8C%BA%E5%A4%A7%E5%B0%8F%E8%B0%83%E6%95%B4/"/>
    <id>http://hhu1506010220.github.io/2019/09/21/利用GParted-liveCD进行Deepin分区大小调整/</id>
    <published>2019-09-21T01:25:18.000Z</published>
    <updated>2020-10-15T11:07:04.992Z</updated>
    
    <content type="html"><![CDATA[<p>在最初装系统时由于经验的缺乏造成了分区大小分配不合理的情况，最初利用Deepin自带的GParted进行分区发现无法对已挂载的分区进行处理，由此开始了摸索的过程…<br><a id="more"></a><br>在最初装系统时，将/boot、/home以及/根目录分别划分出来，并且分别分配了500M、300G、300G的容量，但随着长时间的使用，发现根目录/下撑死也就几十个G，而/home下则用的较多，因此想试着把根目录的200G移动到/home下。</p><p>最初的办法是采用Deepin自带的GParted进行分区，但是你在处理的时候会发现，这几个分区是无法进行操作的，原因是已经被挂载了。如下图所示：</p><p><img src="data:image/png;base64,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" data-original="1.png" alt=""></p><p>经过谷歌之后，决定在Boot里进行分区操作，也就是在分区还没有被挂载前进行分区操作，用到的软件是Universal USB Installer 以及 GParted-live。由于软件的版本很重要，这里我就直接给出两个网址防止大家重复踩坑。<a href="www.onlinedown.net/soft/247396.htm">UUI下载地址</a> 和 <a href="https://downloads.sourceforge.net/gparted/gparted-live-1.0.0-5-i686.iso" target="_blank" rel="noopener">GParted-live下载地址</a>。</p><p>然后在windows下按照下图制作启动盘即可。</p><p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p><p>之后重启电脑，按F12选择U盘启动。如何选择U盘启动可参考<a href="https://m.toutiao.com/i6659672295650361863/?traffic_source=CS1112&amp;in_ogs=1&amp;utm_source=CZ&amp;source=search_tab&amp;utm_medium=wap_search&amp;original_source=1&amp;in_tfs=CZ&amp;channel=" target="_blank" rel="noopener">这里</a>。</p><p>之后按照下面顺序摁Enter：</p><p><img src="data:image/png;base64,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" data-original="3.png" alt=""><img src="data:image/png;base64,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" data-original="4.png" alt=""><img src="data:image/png;base64,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" data-original="5.png" alt=""><img src="data:image/png;base64,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" data-original="6.png" alt=""></p><p>如果过程中有报错的，或者是一直卡在一个界面进不去的，就说明你的boot设置有问题，应该再重启按F2进行设置。</p><p>进去之后就可以按照一般的分盘软件的操作进行分区大小变换了。但需要注意的是，如果你所操作的分区中包含/boot，请谨慎操作，否则容易gg。</p><p><img src="data:image/png;base64,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" data-original="7.png" alt=""></p><p>我还遇到的一个坑就是gparted-live软件版本太低，造成分区操作报错。如下图所示：</p><p><img src="data:image/png;base64,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" data-original="8.jpg" alt=""></p><p>造成这个错误的原因主要是两个，一个是gparted-live软件版本太低，比如我用0.25.3版本时就会报错，1.0.0版本就成功了。第二点是e2fsck版本太低，可采用以下命令进行更新。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.kernel.org/pub/linux/kernel/people/tytso/e2fsprogs/v1.43/e2fsprogs-1.43.tar.xz</span><br><span class="line">tar -xf e2fsprogs-1.43.tar.xz</span><br><span class="line">cd e2fsprogs-1.43</span><br><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>最后大功告成！</p><p><img src="data:image/png;base64,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" data-original="9.png" alt=""></p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/09/21/利用GParted-liveCD进行Deepin分区大小调整" target="_blank" rel="noopener">https://vodkazy.cn/2019/09/21/利用GParted-liveCD进行Deepin分区大小调整</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在最初装系统时由于经验的缺乏造成了分区大小分配不合理的情况，最初利用Deepin自带的GParted进行分区发现无法对已挂载的分区进行处理，由此开始了摸索的过程…&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux" scheme="http://hhu1506010220.github.io/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://hhu1506010220.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】复杂问题问答的状态转移框架</title>
    <link href="http://hhu1506010220.github.io/2019/07/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E5%A4%8D%E6%9D%82%E9%97%AE%E9%A2%98%E9%97%AE%E7%AD%94%E7%9A%84%E7%8A%B6%E6%80%81%E8%BD%AC%E7%A7%BB%E6%A1%86%E6%9E%B6/"/>
    <id>http://hhu1506010220.github.io/2019/07/03/【论文阅读】复杂问题问答的状态转移框架/</id>
    <published>2019-07-03T03:00:24.000Z</published>
    <updated>2020-10-09T04:41:47.566Z</updated>
    
    <content type="html"><![CDATA[<p>本文针对于复杂问题问答的工作，提出了一个状态转移框架和四种转移操作生成语义查询图SQG。与现有工作相比，本文的方法不依赖于人工定义的模板，针对复杂问题能够灵活的生成查询图，结构上不存在限制，在DBpedia和Freebase知识库上多个QA数据集取得了较好的结果。</p><a id="more"></a><!-- toc --><ul><li><a href="#论文简介">论文简介</a></li><li><a href="#问题介绍">问题介绍</a></li><li><a href="#问题难点">问题难点</a></li><li><a href="#本文模型">本文模型</a><ul><li><a href="#node-recognition">Node Recognition</a></li><li><a href="#sqgs-structure-construction">SQG’s Structure Construction</a><ul><li><a href="#状态转移组合优化">状态转移组合优化</a></li><li><a href="#条件限制减少搜索空间">条件限制（减少搜索空间）</a></li></ul></li><li><a href="#finding-entityrelation-candidates-and-matches-of-sqg">Finding entity/relation candidates and matches of SQG</a><ul><li><a href="#实体抽取">实体抽取</a></li><li><a href="#关系抽取">关系抽取</a></li></ul></li><li><a href="#奖励函数">奖励函数</a></li></ul></li><li><a href="#实验">实验</a><ul><li><a href="#生成训练数据">生成训练数据</a></li><li><a href="#数据集">数据集</a></li><li><a href="#条件比较实验">条件比较实验</a></li><li><a href="#错误分析">错误分析</a></li></ul></li><li><a href="#结论">结论</a></li></ul><!-- tocstop --><p>论文地址：<a href="https://www.aclweb.org/anthology/D18-1234" target="_blank" rel="noopener">A State-transition Framework to Answer Complex Questions over Knowledge Base</a></p><p>论文来源：EMNLP 2018 </p><h2><span id="论文简介">论文简介</span></h2><p>该论文主要研究如何解析回答复杂的自然语言问题。首先分析了复杂问题解析过程中的困难，然后提出了一个状态转移框架和四种转移操作将自然语言问题转化为语义查询图(semantic query graph (SQG) )从而能够使用现有的查询算法找到答案。与现有工作相比，本文的方法不依赖于人工定义的模板，针对复杂问题能够灵活的生成查询图，结构上不存在限制，在DBpedia和Freebase知识库上多个QA数据集取得了较好的结果。  </p><h2><span id="问题介绍">问题介绍</span></h2><p>复杂问题答案，其中问题对应于知识库中的多个三元组。之前的解决办法是理解复杂问题时使用预定义模式或模板，如使用依赖树模式将复杂问题分解为几个简单问题，依靠模板将自然语言句子翻译成预定义的逻辑形式。缺点是使用这些手工制作的模板覆盖所有真实案例是不可能的。</p><p><img src="data:image/png;base64,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" data-original="1.png" alt=""></p><h2><span id="问题难点">问题难点</span></h2><ol><li>多重或隐含的关系</li><li>多个或无实体 </li><li>变量和共同引用</li><li>组合问题 </li></ol><h2><span id="本文模型">本文模型</span></h2><h3><span id="node-recognition">Node Recognition</span></h3><ul><li>节点类型：entity node， type node，literal node， variable node</li><li>识别方法<ul><li>使用序列标注的方法来识别各种类型节点，识别entity/type node效果不好<ul><li>实体node会出现比较复杂和比较长的情况不利于识别</li></ul></li><li>本文采用了两种方法结合<ul><li>针对entity/type nodes，采用entity linking algorithms </li><li>针对variable/literal nodes，采用BLSTM-CRF model</li></ul></li></ul></li></ul><h3><span id="sqgs-structure-construction">SQG’s Structure Construction</span></h3><p>贪心算法：考虑到每一步骤去查找搜索空间的时候，都是指数级别的搜索空间，因此我们无法通过枚举的方式来进行处理，文章采用了一种贪心的算法，一个好的中间状态倾向于有更大可能被访问，同时也会有更多的后续状态。</p><p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p><p>四种操作：</p><ul><li>connect<ul><li>链接两个节点，当两节点之间的关系在知识库中存在这个关系</li></ul></li><li>merge<ul><li>主要是为了解决一些共指的问题</li></ul></li><li>expand<ul><li>解决query中存在隐含信息</li></ul></li><li>fold<ul><li>解决冗余的关系，实现关系合并，从而匹配知识库中的关系</li></ul></li></ul><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><h4><span id="状态转移组合优化">状态转移组合优化</span></h4><p>显然，由于指数搜索空间，不可能枚举每个步骤中的所有可能的转换。因此，我们提出了一种贪婪的搜索算法。目的是一个更好的中间状态应该有更多的机会被访问并产生后续状态。由于每个状态都可以被视为部分SQG，我们使用对数线性模型（参见第3节）提出奖励函数来估计每个状态的可能性。优先级队列用于维持未访问状态的顺序。每回合我们选择具有最大正确性可能性的状态，并尝试每次操作的所有可能的转换。具体地，连接和合并操作尝试当前状态中的每对节点作为两个操作节点，而扩展和折叠操作尝试将每个节点作为操作节点。通过学习模型估计新生成的状态并将其推入优先级队列。请注意，不会添加重复数据状态。当找到完整的SQG QS并且QS不能生成任何更好的后续状态时，算法结束。 </p><h4><span id="条件限制减少搜索空间">条件限制（减少搜索空间）</span></h4><p><strong>条件1（连接条件）</strong>当且仅当在问句 <em>n</em> 依赖关系解析树的 <em>v</em>1 <em>和</em> <em>v</em>2 之间的最短路径中不存在其他节点 <em>v</em> 时，才能连接两个节点 <em>v</em>1 <em>和</em> <em>v</em>2 。（使用Stanford CoreNLP dependency parser and coreference annotator）</p><p><strong>条件2（条件合并）</strong>对于合并操作，两个操作节点中应该至少有一个变量 <em>v</em>。 <em>v</em> 应该是wh-word或代词，可以与其他节点共同引用。</p><p><strong>条件3（条件扩展）</strong>对于扩展操作时，操作节点 <em>u</em> 应该是一个变量，而我们需要的是能够找到 <em>u</em> 在<em>过渡字典</em> 的隐藏信息。</p><p><strong>条件4（条件折叠）</strong>对于折叠操作，我们要求至少一个连接边与操作节点没有关系候选或者对应关系的置信概率小于阈值τ。</p><h3><span id="finding-entityrelation-candidates-and-matches-of-sqg">Finding entity/relation candidates and matches of SQG</span></h3><h4><span id="实体抽取">实体抽取</span></h4><p>S-MART (Yang and Chang, 2015) for Freebase dataset and DBpeida Lookup4 for DBpedia dataset. </p><h4><span id="关系抽取">关系抽取</span></h4><p>本文提出一种新型的改进的关系抽取模型<strong>Multi-Channel Convolutional Neural Network (MCCNN)</strong> ，来抽取两个节点之间的关系。模型分为三个通道：</p><p>first channel：simple path between $v_i$ and $v_j$ in the dependency tree Y (N) as input ，两节点之间依存句法树的信息。</p><p>second channel：the whole question sentence excluded all nodes ，整个句子除去所有节点外的信息，这里包含了所有的relation信息。</p><p>third channel：use $v_i $, $v_j$ and their types as input ，两节点以及其类型信息。</p><p>For $v_i$ = “Chinese” and $v_j$ = “actor”, the input is “Chinese-Country-actor-Actor”.</p><h3><span id="奖励函数">奖励函数</span></h3><p>贪婪地选择具有最大γ(s’)的后续状态s’，其中γ()是获取特征并输出相应状态的奖励的奖励函数。在这项工作中，奖励函数γ()是一个用SVM排名损失训练的线性模型。 </p><p><img src="data:image/png;base64,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" data-original="8.png" alt=""></p><p><strong>节点特征：</strong>所有候选的得分平均值，常量节点数与变量节点数 </p><ul><li>将最高的confidence作为当前节点的confidence</li><li>所有实体的平均confidence</li><li>当前节点中constant nodes和variable nodes 的个数</li></ul><p><strong>边特征：</strong> 所以后续的得分平均值，边总数，有无关系短语数 </p><ul><li>将最高的confidence作为当前边的confidence</li><li>问句中所有的边的个数</li></ul><p><strong>匹配：</strong>查询图与知识图的匹配，考虑到折叠操作，对于不匹配的图给予低分，在所有可能操作都不能提升奖励得分时结束。 </p><ul><li>当所有的后续状态的confidence都比较低的时候，会terminate</li></ul><h2><span id="实验">实验</span></h2><h3><span id="生成训练数据">生成训练数据</span></h3><p>为了生成奖励函数的训练数据，给定问题$N = {w_1，w_2，…，w_n}$，我们首先通过现有实体链接系统检查所有可能的短语wij并获得候选实体列表$v_i = {e_1， e_2，…，e_m}$用于每个实体节点$vi$。如果两个实体节点$v_i$和$v_j$有冲突，即$vi$和$vj$共享一个或多个单词，我们保留较长的单词。我们用特殊标记替换每个实体节点$v_i$以表示实体$e_j$的类型，其中$e_j∈C_{v_i}$并且具有最大的置信可能性。然后，可以通过训练有素的节点识别模型来预测整个节点集V. $Q^S = {V，φ}$是初始状态。此外，我们通过使用BFS算法应用预定义的操作来进行状态转换。同时，如果启用条件机制，则仅考虑满足相应条件的状态。一旦我们得到一个新的$Q^S$，我们调用已经训练号的关系提取模型，以获得每个新边缘$\overline{v_iv_j}∈E$的关系候选列表，同时根据3.1节收集特征，并使用等式1计算排名分数（详见原文）。</p><h3><span id="数据集">数据集</span></h3><p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="4.png" alt=""></p><p><img src="data:image/png;base64,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" data-original="5.png" alt=""></p><h3><span id="条件比较实验">条件比较实验</span></h3><ol><li>减少无效搜索节省SQG时间</li><li>减少搜索空间并避免局部最优</li></ol><p><img src="data:image/png;base64,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" data-original="6.png" alt=""></p><h3><span id="错误分析">错误分析</span></h3><p><img src="data:image/png;base64,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" data-original="7.png" alt=""></p><p>可以看到其实S-MART等实体链接任务的可信度还是可以的，问题主要还是出现在关系映射和问题太复杂上面。有的问题既有count操作又有shortest等限制，针对这种问题还有很大的提升空间。</p><h2><span id="结论">结论</span></h2><p>在本文中，提出了一个状态转换框架，利用神经网络来回答复杂问题，这些问题基于四个原始操作生成语义查询图。我们训练BLSTM-CRF模型来识别包含来自问句的实体和变量的节点。为了提取这些节点之间的关系，提出了一个可以解决显性和隐性关系的MCCNN模型。与现有解决方案相比，作者的框架不依赖于手工制作的模板，并且在给定足够的训练数据的情况下有可能生成各种SQG。对多基准数据集的大量实验证实，该方法具有最先进的技术性能。</p><p>原文转自<a href="https://www.wengbi.com/thread_92292_1.html" target="_blank" rel="noopener">翁笔</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文针对于复杂问题问答的工作，提出了一个状态转移框架和四种转移操作生成语义查询图SQG。与现有工作相比，本文的方法不依赖于人工定义的模板，针对复杂问题能够灵活的生成查询图，结构上不存在限制，在DBpedia和Freebase知识库上多个QA数据集取得了较好的结果。&lt;/p&gt;
    
    </summary>
    
      <category term="KBQA" scheme="http://hhu1506010220.github.io/categories/KBQA/"/>
    
    
      <category term="KBQA" scheme="http://hhu1506010220.github.io/tags/KBQA/"/>
    
  </entry>
  
  <entry>
    <title>【论文阅读】基于复杂查询图编码的知识库问答</title>
    <link href="http://hhu1506010220.github.io/2019/06/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E5%9F%BA%E4%BA%8E%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E5%9B%BE%E7%BC%96%E7%A0%81%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94/"/>
    <id>http://hhu1506010220.github.io/2019/06/03/【论文阅读】基于复杂查询图编码的知识库问答/</id>
    <published>2019-06-03T09:33:21.000Z</published>
    <updated>2020-10-15T10:35:24.355Z</updated>
    
    <content type="html"><![CDATA[<p>大多数现有的KBQA方法只关注简单的问题，在复杂的问题上不能很好地工作，因为它们不能同时表示问题和相应的复杂查询结构。在这项工作中，我们将如此复杂的查询结构编码成一个统一的向量表示，从而成功地捕获复杂问题中各个语义组件之间的交互。这种方法在复杂问题上始终优于现有方法，同时在简单问题上保持竞争力。<br><a id="more"></a></p><p>原文：<a href="https://www.aclweb.org/anthology/D18-1242" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D18-1242</a></p><p>首先介绍几个有关的概念，首先是知识库问答KBQA。KBQA所做的任务是给定自然语言问题，通过对问题进行语义理解和解析，进而利用知识库进行查询、推理得出答案。根据研究对象的不同，目前KBQA可分为面向简单问题的问答和面向复杂问题的问题。简单问题是指问句中只包含一个实体关系的问句，复杂问题就是说问句中包含多个实体或者关系，答案和表层形式之间需要经过比较复杂的计算才能得出结果。针对KBQA，目前的技术流派也分为两种，第一种是基于语义解析的方法，该类方法首先是通过自底而上或者状态查询方法收集候选查询图，然后根据给定的问题根据语义相似度预测出最佳的查询图。另一种办法是基于NN神经网络的办法，这类方法是采用encode-and-compare框架，把问题和谓词序列都压缩到通用的向量空间中，然后通过余弦函数计算相似度，相似度高的就是最佳候选项。<br>为了计算一个问题与一个复杂的查询图的相似度，一个直观的解决方案是像图里一样，把查询图分割成多个语义部分，这样原先的方法变成计算问题与查询图的每个部分的相似度。但是，这样操作面临两个缺陷。第一点，每个语义部件没办法直接跟整个问题进行比较，因为它只学到了整个问题的部分信息。第二点，各个部件是单独编码的，没有学习到整个查询图的表示，因此无法捕捉到一些全局的特征。<br>鉴于此，本文提出了一个轻量有效的神经网络模型去解决复杂的KBQA任务，下面细说一下具体的流程。<br>首先是查询图生成，这里采用Freebase作为目标知识库，然后在生成过程中考虑四种语义约束。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="1.jpg" alt=""><br>第一步要先从问句中提取有效的(mention,focus node)对。对于实体链接，是借助于SMART进行连接。对于类型链接，是利用了问句中的uni-, bi-and tri-gram mention,选取与词向量相似度最高top10个选项。对于时间链接，通过符合year regex提取时间mention。<br><img src="data:image/png;base64,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" data-original="2.jpg" alt=""><br>对于顺序链接，我们利用预定义的最高级词列表。并通过匹配最高级词或“序数+最高级”模式来识别提及的内容。顺序节点是表示序数的整数在mention中。比如对于例句，会将识别出这几个。<br>然后我们通过1跳或者2跳谓词序列将答案节点链接至不同的实体。也就是相当于以entity为终点，去搜索作为锚节点的答案节点。<br><img src="data:image/png;base64,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" data-original="3.jpg" alt=""><br>当有了主路径之后，还要为其添加约束，于是采用深度优先搜索，去查询多元实体的组合，通过添加1跳的节点来约束主路径。对于type限制，主要就是添加isA谓词。除此之外本文还采用谓词的domain去限制答案。<br><img src="data:image/png;base64,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" data-original="4.jpg" alt=""><br>作者造了一个freebase的类型层次表，然后通过这个表去定义一些类型继承关系。比如说这里的government position的domain是一个政治家，isA的类型是美国总统，它在层次表里是属于政治家的子类型那么就保留这个类型限制，如果不是domain的类型的任何子类型或者父类型就抛弃它，所以就相当于进行了一个剪枝操作。时间和序号的限制就是用2跳谓词序列去限制，比如after2002年变成from哪年，并且这一年&gt;2002。同理序列限制也是如此。<br><img src="data:image/png;base64,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" data-original="5.jpg" alt=""><br>当查询图生成之后，接下来的步骤就是如何将问句的语义和查询图的语义做比较，使之挑选出相似度最高的那个查询图作为最终答案。因此本文提出了一种基于神经网络的语义匹配模型。首先，对于语义组件，综合考虑谓词id和谓词名称。谓词id是从路径级别考虑，当给定id序列，我们就将其作为一个整体的单元，然后直接embedding。这么做的原因主要是因为本文方法中id序列的长度不会超过2，并且不同谓词序列的数量和不同谓词的数量是大致相同的。谓词名称则是从单词级别去考虑，把每个谓词拆分成单词序列，对每个单词做embedding之后取平均值。将二者加和就得到了一个语义组件的向量表示。<br><img src="data:image/png;base64,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" data-original="6.jpg" alt=""><br>对于问句的表示，分别从全局和局部级别进行考虑。先将所有实体和时间替换为token，全局信息的话就是把token序列作为输入，然后用同样的单词embedding矩阵去做embedding得到向量序列，然后过一个双向GRU网络，将最后一个前向后向隐藏状态的组合作为最终向量。局部级别的话就是将问句转化成依存路径，然后过另一个双向GRU，得到基于依存级别的向量。还是两个向量做个加法，得到最后的句子表示。<br><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="7.jpg" alt=""><br>那么如何计算问句和语义组件之间的相似度呢？这里采用了最大池化操作，分别从语义组件表示向量和问句表示向量中选出最大的，然后做一个余弦相似度比较。<br><img src="data:image/png;base64,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" data-original="8.jpg" alt=""><br>其他的一些点比如实体链接用到了S-MART做语义丰富，训练时的损失函数，以及计算分数时考虑实体、语义和结构三部分的得分，详见原文。<br><img src="data:image/png;base64,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" data-original="9.jpg" alt=""><br>本文的实验是在三个数据集上进行，分别是<strong>ComplexQuestions</strong>（2100个复杂问题，收集自Bing搜索查询日志）、<strong>WebQuestions</strong>（5810个问题，收集自Google Suggest API）、<strong>SimpleQuestions</strong>（超过100K个问题）。<br><img src="data:image/png;base64,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" data-original="10.jpg" alt=""><br><img src="data:image/png;base64,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" data-original="11.jpg" alt=""><br>利用F1作为衡量指标，结果显示对于复杂问题的处理达到了state of the art水平，同时对简单问题也有不错的表现。</p><p>除此之外，作者还添加了几个额外的实验，比如采取不同的方法做谓词的embedding，结果显示对于谓词名称采用取均值操作，对于谓词id采取路径基本的embedding产生的效果是最好的。对于问句表示，综合考虑依存和句法效果最佳。<br><img src="data:image/png;base64,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" data-original="12.jpg" alt=""><br><img src="data:image/png;base64,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" data-original="13.jpg" alt=""><br>最后有一些自己的思考，本文是第一次尝试用神经网络去显式地编码复杂查询图的完整语义信息。但是对于隐含的时间等复杂的约束并没有进行处理，是否可以用同样的embedding方法进行编码？语义计算采用max-pooling的方法，是否其他的池化方法会更有效？现在的方法是利用pooling来表示各个部件之间的隐藏关系，是否可以用神经网络的办法，比如说把所有的constraint过一个神经网络，用输出的向量来当做语义表示。</p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/06/03/%E3%80%90%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E3%80%91%E5%9F%BA%E4%BA%8E%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E5%9B%BE%E7%BC%96%E7%A0%81%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93%E9%97%AE%E7%AD%94/" target="_blank" rel="noopener">https://vodkazy.cn/2019/06/03/【论文阅读】基于复杂查询图编码的知识库问答</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大多数现有的KBQA方法只关注简单的问题，在复杂的问题上不能很好地工作，因为它们不能同时表示问题和相应的复杂查询结构。在这项工作中，我们将如此复杂的查询结构编码成一个统一的向量表示，从而成功地捕获复杂问题中各个语义组件之间的交互。这种方法在复杂问题上始终优于现有方法，同时在简单问题上保持竞争力。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="KBQA" scheme="http://hhu1506010220.github.io/categories/KBQA/"/>
    
    
      <category term="KBQA" scheme="http://hhu1506010220.github.io/tags/KBQA/"/>
    
  </entry>
  
  <entry>
    <title>Word中利用Aurora编写LateX伪代码</title>
    <link href="http://hhu1506010220.github.io/2019/05/10/Word%E4%B8%AD%E4%BD%BF%E7%94%A8LateX%E7%BC%96%E5%86%99%E4%BC%AA%E4%BB%A3%E7%A0%81/"/>
    <id>http://hhu1506010220.github.io/2019/05/10/Word中使用LateX编写伪代码/</id>
    <published>2019-05-10T04:57:40.000Z</published>
    <updated>2020-10-15T10:58:39.311Z</updated>
    
    <content type="html"><![CDATA[<p>由于很多时候我们要用word而不是LateX来编写论文，所以如果有需要用到伪代码的地步就很不方便。本文主要介绍了如何在word中使用Aurora插入LateX编写的伪代码<br><a id="more"></a></p><h3><span id="安装该软件需要两个东西">安装该软件需要两个东西：</span></h3><ul><li>Aurora本身（链接:<a href="https://pan.baidu.com/s/13xKuAsdaFtno0sETRUPofQ" target="_blank" rel="noopener">https://pan.baidu.com/s/13xKuAsdaFtno0sETRUPofQ</a> 提取码:uk2e）</li><li>Miktex编译程序（<a href="https://miktex.org/download）" target="_blank" rel="noopener">https://miktex.org/download）</a></li></ul><h3><span id="安装步骤">安装步骤</span></h3><ul><li>关闭word。双击安装Aurora，不勾选miktex选项。安装完成（如果一直出现 valid handle 不用管直接一直回车即可）。</li><li>安装Miktex，建议最新版2.9。</li><li>将电脑系统时间调到<strong>2009</strong>年某天，双击下载的破解机，输入任意英文字母进行破解。破解成功。</li><li>打开word。通过插入——对象——Aurora Equation 来插入latex代码。然后开始配置环境，将 <strong>Paths</strong> 改成自己的Miktex下的路径，如图：<br><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></li><li><strong>Properties</strong> 里的 <strong>Rendering method</strong> 改为 <strong>Vector(render for fonts)</strong> 。（Rendering method指的是渲染方式，一共有三种：Raster（位图），Vector（矢量图）和 Vector（render all fonts）。如果有需要在没有安装Aurora插件的电脑的word查看公式，应使用第一种或第三种。但如果生成PDF再查看的话就不需要Aurora插件了。miktex2.9使用第一种方式会报错 problem running latex，因此只能选择后两种。）<br><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></li><li><p>修改常用的Packages，我的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;amssymb&#125;</span><br><span class="line">% \usepackage&#123;euler&#125;</span><br><span class="line">\providecommand&#123;\abs&#125;[1]&#123;\left\lvert#1\right\rvert&#125;</span><br><span class="line">\providecommand&#123;\norm&#125;[1]&#123;\left\lVert#1\right\rVert&#125;</span><br><span class="line">\usepackage&#123;bbm&#125;</span><br><span class="line">\usepackage&#123;CJK&#125;</span><br><span class="line">\usepackage&#123;listings&#125;</span><br><span class="line">\usepackage&#123;xcolor&#125;</span><br><span class="line">\usepackage&#123;listings&#125;</span><br><span class="line">\usepackage&#123;amsmath,bm,graphicx,multirow,bm,bbm,amssymb,psfrag,algorithm,subfigure,color,mdframed,wasysym,subeqnarray,multicol&#125;</span><br><span class="line"></span><br><span class="line">\usepackage&#123;algorithm&#125;</span><br><span class="line">\usepackage&#123;algpseudocode&#125;</span><br><span class="line">\usepackage&#123;amsmath&#125;</span><br><span class="line">\renewcommand&#123;\algorithmicrequire&#125;&#123;\textbf&#123;Input:&#125;&#125;</span><br><span class="line">\renewcommand&#123;\algorithmicensure&#125;&#123;\textbf&#123;Output:&#125;&#125;</span><br></pre></td></tr></table></figure></li><li><p>这时候很可能会继续报错 <code>Problems running LaTex</code>。通常这个错误有三个原因：</p><ul><li>miktex的latex，dvipng和pdflatex路径不对；</li><li>系统时间没有调整至2009年。在一次运行成功后就可以把时间调回来了；</li><li>Rendering method 选择的不对。据说不同版本的miktex所能成功渲染的方法也不同，可以多试试；</li><li>修改了packages选项卡，引入了还没有安装的package。<br>其中第四种应该是最常见的，具体的解决办法就是，利用<code>Miktex</code>自带的编辑器<code>TeXworks</code>来进行安装。具体的，复制一些代码，比如下边给出的这段，然后用<code>TeXworks</code>进行编译，TeXworks会自动安装那些没有安装的包。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[11pt]&#123;ctexart&#125;  </span><br><span class="line">\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]&#123;geometry&#125;  </span><br><span class="line">\usepackage&#123;algorithm&#125;  </span><br><span class="line">\usepackage&#123;algorithmicx&#125;  </span><br><span class="line">\usepackage&#123;algpseudocode&#125;  </span><br><span class="line">\usepackage&#123;amsmath&#125;  </span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;  </span><br><span class="line">\begin&#123;algorithm&#125;[h]  </span><br><span class="line">  \caption&#123;An example for format For \&amp; While Loop in Algorithm&#125;  </span><br><span class="line">  \begin&#123;algorithmic&#125;[1]  </span><br><span class="line">    \For&#123;each $i\in [1,9]$&#125;  </span><br><span class="line">      \State initialize a tree $T_&#123;i&#125;$ with only a leaf (the root);  </span><br><span class="line">      \State $T=T\cup T_&#123;i&#125;;$  </span><br><span class="line">    \EndFor  </span><br><span class="line">    \ForAll &#123;$c$ such that $c\in RecentMBatch(E_&#123;n-1&#125;)$&#125;  </span><br><span class="line">      \label&#123;code:TrainBase:getc&#125;  </span><br><span class="line">      \State $T=T\cup PosSample(c)$;  </span><br><span class="line">      \label&#123;code:TrainBase:pos&#125;  </span><br><span class="line">    \EndFor;  </span><br><span class="line">    \For&#123;$i=1$; $i&lt;n$; $i++$ &#125;  </span><br><span class="line">      \State $//$ Your source here;  </span><br><span class="line">    \EndFor  </span><br><span class="line">    \For&#123;$i=1$ to $n$&#125;  </span><br><span class="line">      \State $//$ Your source here;  </span><br><span class="line">    \EndFor  </span><br><span class="line">    \State $//$ Reusing recent base classifiers.  </span><br><span class="line">    \label&#123;code:recentStart&#125;  </span><br><span class="line">    \While &#123;$(|E_n| \leq L_1 )and( D \neq \phi)$&#125;  </span><br><span class="line">      \State Selecting the most recent classifier $c_i$ from $D$;  </span><br><span class="line">      \State $D=D-c_i$;  </span><br><span class="line">      \State $E_n=E_n+c_i$;  </span><br><span class="line">    \EndWhile  </span><br><span class="line">    \label&#123;code:recentEnd&#125;  </span><br><span class="line">  \end&#123;algorithmic&#125;  </span><br><span class="line">\end&#123;algorithm&#125;  </span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>全部安装好之后就可以在Aurora里用LateX写公式啦！具体的LateX公式的用法请参见另一篇博文<a href="https://vodkazy.cn/2019/04/22/MathJax%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">MathJax使用笔记</a><br>发个示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">\renewcommand&#123;\thealgorithm&#125;&#123;1&#125;</span><br><span class="line">\begin&#123;algorithm&#125;[H]  </span><br><span class="line">  \caption&#123;Location Anonymization&#125;  </span><br><span class="line">  \begin&#123;algorithmic&#125;[1] </span><br><span class="line">  \Require&#123;$U=\&#123;u_1,u_2,...,u_n\&#125;$;k;q;$L=\&#123;L^&#123;\prime&#125;_1,L^&#123;\prime&#125;_2,...,L^&#123;\prime&#125;_n\&#125;$,where $L^&#123;\prime&#125;_i=(&#123;\rho&#125;_i,&#123;\gamma&#125;_i)$&#125;</span><br><span class="line">  \Ensure&#123;$R^&#123;\prime&#125;$&#125;\</span><br><span class="line">    \State Select $k-1$ users from $U$ which are nearest $u_r$ as a set $M$</span><br><span class="line">    \State Define $U_k=&#123;u_r \cup M&#125;$</span><br><span class="line">    \State Define an empty array $A$</span><br><span class="line">    \State Define $R^&#123;\prime&#125;$</span><br><span class="line">    \For&#123;each $u_i\in M$&#125; </span><br><span class="line">      \label&#123;code:TrainBase:getc&#125;  </span><br><span class="line">      \State Calculate the similarity of $u_r$ and $u_i$ as $sim(u_r,u_i)$</span><br><span class="line">      \While &#123;$ sim(u_r,u_i)&gt; &#123;\xi&#125;_s$&#125; $//&#123;\xi&#125;_s$ means the threshold value of similarity</span><br><span class="line">        \State Calculate $Dist(u_r,u_i)$</span><br><span class="line">        \State Put each elements $u_i$ into $A$ in ascending order of the value of $Dist(u_r,u_i)$</span><br><span class="line">      \EndWhile  </span><br><span class="line">      \label&#123;code:TrainBase:pos&#125;  </span><br><span class="line">    \EndFor;  </span><br><span class="line">    \State Set the area as $R^&#123;\prime&#125;$ which is generated by connecting the first $q$ elements in $A$ in turn </span><br><span class="line">    \label&#123;code:recentEnd&#125;  </span><br><span class="line">    \State Return $R^&#123;\prime&#125;$</span><br><span class="line">  \end&#123;algorithmic&#125;  </span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure></p><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p><blockquote><p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p><p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p><p>原文链接：<a href="https://vodkazy.cn/2019/05/10/Word中使用LateX编写伪代码" target="_blank" rel="noopener">https://vodkazy.cn/2019/05/10/Word中使用LateX编写伪代码</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于很多时候我们要用word而不是LateX来编写论文，所以如果有需要用到伪代码的地步就很不方便。本文主要介绍了如何在word中使用Aurora插入LateX编写的伪代码&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="LateX" scheme="http://hhu1506010220.github.io/categories/LateX/"/>
    
    
      <category term="LateX" scheme="http://hhu1506010220.github.io/tags/LateX/"/>
    
  </entry>
  
</feed>
