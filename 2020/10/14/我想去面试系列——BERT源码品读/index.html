<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

	<!-- 百度统计 -->
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "https://hm.baidu.com/hm.js?e31627579358722b9d300535c8206351";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Vodkazy">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="我想去面试系列——BERT源码品读">
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="想飞的小菜鸡">
  <!--Type page-->
  
      <meta property="og:type" content="article">
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- Title -->
  
  <title>我想去面试系列——BERT源码品读 - 想飞的小菜鸡</title>


  <link rel="shortcut icon" href="/../images/icon.ico">
  <!--font-awesome-->
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body>

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="data:image/png;base64,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" data-original="https://cdn2.iconfinder.com/data/icons/weather-color-2/500/weather-01-128.png" alt="想飞的小菜鸡" height="60">
        
      </a>
    </div>
    <a class="header-name" href="/">
            <span>想飞的小菜鸡</span>
            的小窝
        </a>
    <!-- navbar -->
    <nav class="navbar">
      <!--  nav links -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  <i class="fa fa-home "></i>
                
                首页
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  <i class="fa fa-archive "></i>
                
                目录
              </a>
            </li>
          
            <li>
              <a href="/project">
                
                  <i class="fa fa-folder-open "></i>
                
                代码库
              </a>
            </li>
          
            <li>
              <a href="/photo">
                
                  <i class="fa fa-photo "></i>
                
                相册薄
              </a>
            </li>
          
            <li>
              <a href="/lovetree">
                
                  <i class="fa fa-tree "></i>
                
                爱情树
              </a>
            </li>
          
            <li>
              <a href="/guestbook">
                
                  <i class="fa fa-edit "></i>
                
                留言板
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  <i class="fa fa-user "></i>
                
                关于我
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            我想去面试系列——BERT源码品读
            
          </h1>
         
        </div>
         <p class="a-posted-on">
          2020-10-14
          </p>
        <!-- Post Main Content -->
        <div class="entry-content">
          <p>细读BERT-Pytorch源码，附带模块解析和一点思考。<br><a id="more"></a></p>
<!-- toc -->
<ul>
<li><a href="#代码目录说明">代码目录说明</a></li>
<li><a href="#解析参数">解析参数</a></li>
<li><a href="#读词表">读词表</a></li>
<li><a href="#获取训练集和测试集">获取训练集和测试集</a><ul>
<li><a href="#bertdataset">BERTDataset</a></li>
</ul>
</li>
<li><a href="#建模bert">建模BERT</a><ul>
<li><a href="#bertembedding">BERTEmbedding</a></li>
<li><a href="#transformer">Transformer △</a><ul>
<li><a href="#single-attention">Single Attention</a></li>
<li><a href="#multi-head-attention">Multi Head Attention</a></li>
<li><a href="#sublayerconnection">SublayerConnection</a></li>
<li><a href="#feedforwardnetwork">FeedForwardNetwork</a></li>
<li><a href="#transformer集成">Transformer集成</a></li>
</ul>
</li>
<li><a href="#bertmodel">BERTModel</a></li>
</ul>
</li>
<li><a href="#预训练">预训练</a><ul>
<li><a href="#nsp-mlm">NSP &amp; MLM</a></li>
<li><a href="#trainer">Trainer</a></li>
</ul>
</li>
<li><a href="#参考文献">参考文献</a></li>
</ul>
<!-- tocstop -->
<p>源码地址：<a href="https://github.com/codertimo/BERT-pytorch" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch</a>，过了一遍之后感觉这个repo其实bug挺多的..后续作者修了一点bug在 <a href="https://github.com/codertimo/BERT-pytorch/tree/alpha0.0.1a5" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch/tree/alpha0.0.1a5</a>。本文是参照两次的版本综合进行解析。</p>
<h2><span id="代码目录说明">代码目录说明</span></h2><p>我们后续的说明都认为根目录在bert_pytorch目录下。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">├──── bert_pytorch/</span><br><span class="line">│    ├──── __main__.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    ├──── dataset/</span><br><span class="line">│    │    ├──── dataset.<span class="keyword">py</span></span><br><span class="line">│    │    └──── vocab.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    ├──── model/</span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── attention/</span><br><span class="line">│    │    │    ├──── multi_head.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── single.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── embedding/</span><br><span class="line">│    │    │    ├──── bert.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── position.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── segment.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── token.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── utils/</span><br><span class="line">│    │    │    ├──── feed_forward.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── gelu.<span class="keyword">py</span></span><br><span class="line">│    │    │    ├──── layer_norm.<span class="keyword">py</span></span><br><span class="line">│    │    │    └──── sublayer.<span class="keyword">py</span></span><br><span class="line">│    │    │</span><br><span class="line">│    │    ├──── bert.<span class="keyword">py</span></span><br><span class="line">│    │    ├──── language_model.<span class="keyword">py</span></span><br><span class="line">│    │    └──── transformer.<span class="keyword">py</span></span><br><span class="line">│    │</span><br><span class="line">│    └──── trainer/</span><br><span class="line">│         ├──── optim_schedule.<span class="keyword">py</span></span><br><span class="line">│         └──── pretrain.<span class="keyword">py</span></span><br><span class="line">│</span><br><span class="line">├──── LICENSE</span><br><span class="line">├──── Makefile</span><br><span class="line">├──── README.md</span><br><span class="line">├──── requirements.txt</span><br><span class="line">└──── setup.<span class="keyword">py</span></span><br></pre></td></tr></table></figure>
<h2><span id="解析参数">解析参数</span></h2><p><code>__main__.py</code>主入口进来先解析命令行参数，ArgumentParser()，包括default、type、required、help、default等参数设置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 11行～38行</span></span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-c"</span>, <span class="string">"--train_dataset"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"train dataset for train bert"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-t"</span>, <span class="string">"--test_dataset"</span>, type=str, default=<span class="keyword">None</span>, help=<span class="string">"test set for evaluate train set"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-v"</span>, <span class="string">"--vocab_path"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"built vocab model path with bert-vocab"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-o"</span>, <span class="string">"--output_path"</span>, required=<span class="keyword">True</span>, type=str, help=<span class="string">"ex)output/bert.model"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-hs"</span>, <span class="string">"--hidden"</span>, type=int, default=<span class="number">256</span>, help=<span class="string">"hidden size of transformer model"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-l"</span>, <span class="string">"--layers"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"number of layers"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-a"</span>, <span class="string">"--attn_heads"</span>, type=int, default=<span class="number">8</span>, help=<span class="string">"number of attention heads"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-s"</span>, <span class="string">"--seq_len"</span>, type=int, default=<span class="number">20</span>, help=<span class="string">"maximum sequence len"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-d"</span>, <span class="string">"--dropout"</span>, type=float, default=<span class="number">0.1</span>, help=<span class="string">"dropout rate"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"-b"</span>, <span class="string">"--batch_size"</span>, type=int, default=<span class="number">64</span>, help=<span class="string">"number of batch_size"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-e"</span>, <span class="string">"--epochs"</span>, type=int, default=<span class="number">10</span>, help=<span class="string">"number of epochs"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"-w"</span>, <span class="string">"--num_workers"</span>, type=int, default=<span class="number">5</span>, help=<span class="string">"dataloader worker size"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--with_cuda"</span>, type=bool, default=<span class="keyword">True</span>, help=<span class="string">"training with CUDA: true, or false"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--log_freq"</span>, type=int, default=<span class="number">10</span>, help=<span class="string">"printing loss every n iter: setting n"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--corpus_lines"</span>, type=int, default=<span class="keyword">None</span>, help=<span class="string">"total number of lines in corpus"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--cuda_devices"</span>, type=int, nargs=<span class="string">'+'</span>, default=<span class="keyword">None</span>, help=<span class="string">"CUDA device ids"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--on_memory"</span>, type=bool, default=<span class="keyword">False</span>, help=<span class="string">"Loading on memory: true or false"</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">1e-3</span>, help=<span class="string">"learning rate of adam"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_weight_decay"</span>, type=float, default=<span class="number">0.01</span>, help=<span class="string">"weight_decay of adam"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_beta1"</span>, type=float, default=<span class="number">0.9</span>, help=<span class="string">"adam first beta value"</span>)</span><br><span class="line">parser.add_argument(<span class="string">"--adam_beta2"</span>, type=float, default=<span class="number">0.999</span>, help=<span class="string">"adam first beta value"</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<h2><span id="读词表">读词表</span></h2><p><code>vocab.py</code>里有三个类<code>TorchVocab</code>、<code>Vocab</code>、<code>WordVocab</code>，他们从左到右是依次继承的关系。构造词表整体的思路是使用集合中的Counter计数器，按照出现频率进行大到小排序，建立词与索引的映射字典，当字典的大小等于词表的大小，或者当前词的出现的频率小于设定的最小出现频率时，词典建立完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 40行~42行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Vocab"</span>, args.vocab_path)</span><br><span class="line">vocab = WordVocab.load_vocab(args.vocab_path)</span><br><span class="line">print(<span class="string">"Vocab Size: "</span>, len(vocab))</span><br></pre></td></tr></table></figure>
<p><code>TorchVocab</code>类里有三个属性，<code>freqs</code>代表语料库中每个token出现的次数、<code>stoi</code>是一个defaultdict负责将token string映射到数字Interger、<code>itos</code>是一个list里面的每个index对应着一个token string。<code>__init__(self, counter, max_size=None, min_freq=1, specials=[&#39;&lt;pad&gt;&#39;, &#39;&lt;oov&gt;&#39;], vectors=None, unk_init=None, vectors_cache=None)</code>会进行添加特殊token扩展词表specials； 对token按照出现的次数对<code>itos</code>进行排序，出现次数小于min_freq的token将不被添加进词表；此外还有向量初始化、unk初始化设置、向量缓存等设置。<code>vocab_rerank</code>方法可以根据已经排序好的itos对stoi进行重排序。<code>extend</code>方法对字典进行扩展，参数是待扩展的字符串[a,b,c]。</p>
<p><code>Vocab</code>类初始化时添加了五个token，分别是<code>&lt;pad&gt;</code>, <code>&lt;unk&gt;</code>, <code>&lt;eos&gt;</code>, <code>&lt;sos&gt;</code>, <code>&lt;mask&gt;</code>，分别对应index 0~4。这里的<code>eos</code>对应于<code>cls</code>，<code>sos</code>对应于<code>sep</code>。<code>to_seq</code>和<code>from_seq</code>负责stoi和itos之间的转化。<code>load_vocab</code>和<code>save_vocab</code>负责从文件中加载/存储词表。</p>
<p><code>WordVocab</code>初始化时<code>__init__(self, texts, max_size=None, min_freq=1)</code>先搞个<code>Counter</code>统计一下各个token的次数，然后调用父类<code>Vocab</code>的构造函数并把这个counter传参进去进行初始化。同时重写了<code>to_seq</code>和<code>from_seq</code>和<code>load_vocab</code>三个方法，这里的<code>to_seq</code>可以实现为自动加<code>[eos]</code>和<code>[sos]</code>。</p>
<h2><span id="获取训练集和测试集">获取训练集和测试集</span></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># __main_.py 44行～55行</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Train Dataset"</span>, args.train_dataset)</span><br><span class="line">train_dataset = BERTDataset(args.train_dataset, vocab, seq_len=args.seq_len, corpus_lines=args.corpus_lines, on_memory=args.on_memory)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Loading Test Dataset"</span>, args.test_dataset)</span><br><span class="line">test_dataset = BERTDataset(args.test_dataset, vocab, seq_len=args.seq_len, on_memory=args.on_memory) <span class="keyword">if</span> args.test_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"Creating Dataloader"</span>)</span><br><span class="line">train_data_loader = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers)</span><br><span class="line">test_data_loader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers) <span class="keyword">if</span> test_dataset <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure>
<h3><span id="bertdataset">BERTDataset</span></h3><p><code>BERTDataset</code>类继承自<code>Dataset</code>类。实际上BERT的两个训练任务的随机取样（随机mask token和随机采样next sentence都是在dataset读取时完成的）。<code>__init__()</code>负责初始化一些变量如数据集路径corpus_path、词表vocab、最大序列长度seq_len、是否加载至内存on_memory、数据集内数据的条数corpus_lines、编码encoding，<code>__len__</code>和<code>__getitem__</code>就是对<code>Dataset</code>类重写的正常操作，分别返回整个dataset的长度和想要获取的单个元素。</p>
<p><code>random_word(sentence)</code>的作用是随机mask token，输入参数是token序列，输出是token id list和label list。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 63行～90行</span></span><br><span class="line"></span><br><span class="line">tokens = sentence.split()</span><br><span class="line">output_label = []</span><br><span class="line"><span class="keyword">for</span> i, token <span class="keyword">in</span> enumerate(tokens):	<span class="comment"># 这里的15%其实写的不对，应该是语料中15%的token被选择，而不是每个token都有15%的机会被选择，读者需要注意一下。</span></span><br><span class="line">    prob = random.random()</span><br><span class="line">    <span class="keyword">if</span> prob &lt; <span class="number">0.15</span>: </span><br><span class="line">        <span class="comment"># 随机选择15%的token进行mask</span></span><br><span class="line">        prob /= <span class="number">0.15</span></span><br><span class="line">        <span class="keyword">if</span> prob &lt; <span class="number">0.8</span>: </span><br><span class="line">            <span class="comment"># 80%的token转化成&lt;mask&gt;</span></span><br><span class="line">            tokens[i] = self.vocab.mask_index</span><br><span class="line">        <span class="keyword">elif</span> prob &lt; <span class="number">0.9</span>: </span><br><span class="line">            <span class="comment"># 10%的token随机转化成其他token</span></span><br><span class="line">            tokens[i] = random.randrange(len(self.vocab))</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="comment"># 10%的token保持原token，如果token在stoi不存在就返回unk_index</span></span><br><span class="line">            tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)</span><br><span class="line">        output_label.append(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">return</span> tokens, output_label</span><br></pre></td></tr></table></figure>
<p><code>random_sent(index)</code>的作用是随机采样next sentence，输入参数是index行号，输出是sen1、sen2和label。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 92行～99行</span></span><br><span class="line"></span><br><span class="line">t1, t2 = self.get_corpus_line(index)</span><br><span class="line"><span class="comment"># output_text, label(isNotNext:0, isNext:1)</span></span><br><span class="line"><span class="keyword">if</span> random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line">	<span class="keyword">return</span> t1, t2, <span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">return</span> t1, self.get_random_line(), <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>那么给定一个index之后如何<code>__getitem__(index)</code>获取一条数据呢？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dataset/dataset.py 37行～61行</span></span><br><span class="line"></span><br><span class="line">t1, t2, is_next_label = self.random_sent(index) <span class="comment"># 随机选s1 s2</span></span><br><span class="line">t1_random, t1_label = self.random_word(t1)		<span class="comment"># 随机mask单词</span></span><br><span class="line">t2_random, t2_label = self.random_word(t2)		<span class="comment"># 随机mask单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># [CLS] tag = SOS tag, [SEP] tag = EOS tag</span></span><br><span class="line">t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]</span><br><span class="line">t2 = t2_random + [self.vocab.eos_index] 		<span class="comment"># 添加头尾特殊标签</span></span><br><span class="line"></span><br><span class="line">t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]</span><br><span class="line">t2_label = t2_label + [self.vocab.pad_index]	<span class="comment"># label也要对应加上，用于MLM</span></span><br><span class="line"></span><br><span class="line">segment_label = ([<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(t1))] + [<span class="number">2</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(t2))])[:self.seq_len]			<span class="comment"># 添加segment，根据属于s1的token都标1，属于s2的都标2</span></span><br><span class="line">bert_input = (t1 + t2)[:self.seq_len]			<span class="comment"># 两句话拼起来成一个整句</span></span><br><span class="line">bert_label = (t1_label + t2_label)[:self.seq_len]	<span class="comment"># segment label也拼起来</span></span><br><span class="line"></span><br><span class="line">padding = [self.vocab.pad_index <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.seq_len - len(bert_input))]	<span class="comment"># 不够seq_len长的用pad_index补齐</span></span><br><span class="line">bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)</span><br><span class="line"></span><br><span class="line">output = &#123;<span class="string">"bert_input"</span>: bert_input,</span><br><span class="line">          <span class="string">"bert_label"</span>: bert_label,</span><br><span class="line">          <span class="string">"segment_label"</span>: segment_label,</span><br><span class="line">          <span class="string">"is_next"</span>: is_next_label&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &#123;key: torch.tensor(value) <span class="keyword">for</span> key, value <span class="keyword">in</span> output.items()&#125;</span><br></pre></td></tr></table></figure>
<p>除此之外<code>get_corpus_line(item)</code>是为了获取一条预料(s1, s2)，<code>get_random_line()</code>是为了获取单独一句话s2。不再赘述。</p>
<p><code>DataLoader(dataset, batch_size, num_workers)</code>获取dataloader也是正常操作，就不多说了。</p>
<h2><span id="建模bert">建模BERT</span></h2><h3><span id="bertembedding">BERTEmbedding</span></h3><p>BERTEmbedding考虑三种embedding，<code>TokenEmbedding</code>、<code>PositionalEmbedding</code>和<code>SegmentEmbedding</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/embedding/token.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TokenEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(vocab_size, embed_size, padding_idx=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/embedding/segment.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SegmentEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embed_size=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(<span class="number">3</span>, embed_size, padding_idx=<span class="number">0</span>) <span class="comment"># 这里的0、1、2对应于mask、s1、s2</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># model/embedding/position.py(这是原始transformer的写法，positionEmbedding是固定的向量表示)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEmbedding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, max_len=<span class="number">512</span>)</span>:</span> <span class="comment"># d_model实际上就是embed_size</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model).float()</span><br><span class="line">        pe.require_grad = <span class="keyword">False</span></span><br><span class="line">		<span class="comment"># 以下这些代码需要理解，面试会考哦</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).float().unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = (torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).float() * -(math.log(<span class="number">10000.0</span>) / d_model)).exp()</span><br><span class="line">        odd_len = d_model - div_term.size(<span class="number">-1</span>)</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term[:odd_len])	<span class="comment"># 源代码对于奇数的处理不太对，这里做了修改</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/embedding/position.py(现在的写法，直接继承父类了，是让BERT直接学一个0 1 2 3 ... 分别对应的向量表示)</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEmbedding</span><span class="params">(nn.Embedding)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, max_len=<span class="number">512</span>)</span>:</span></span><br><span class="line">        super().__init__(max_len, d_model)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.weight.data[:x.size(<span class="number">1</span>)]</span><br></pre></td></tr></table></figure>
<p>注意一点的是，BERT的positionEmbedding是随机初始化，然后目的是学0 1 2 3…等数字对应的向量表示的，而原始的Transformer的positionEmbedding是用<code>sin</code>和<code>cos</code>特定计算不变的，这其实是Transformer和BERT最大的区别之一。原始Transformer的<code>PositionalEmbedding</code>的计算是有数学理论的，见<a href="https://zhuanlan.zhihu.com/p/166244505" target="_blank" rel="noopener">这里</a>。具体计算公式如下：</p>
<script type="math/tex; mode=display">PE(pos,2i)=\sin(\frac{pos}{1000^{2i/d_{model}}})</script><script type="math/tex; mode=display">PE(pos,2i+1)=\cos(\frac{pos}{1000^{2i/d_{model}}})</script><p>整个<code>BertEmbedding</code>相当于就是把三个embedding直接加起来。至于为什么能加呢？见<a href="https://www.zhihu.com/question/374835153/answer/1069173198" target="_blank" rel="noopener">知乎讨论</a>，实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/embedding/bert.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTEmbedding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT Embedding which is consisted with under features</span></span><br><span class="line"><span class="string">        1. TokenEmbedding : normal embedding matrix</span></span><br><span class="line"><span class="string">        2. PositionalEmbedding : adding positional information</span></span><br><span class="line"><span class="string">        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)</span></span><br><span class="line"><span class="string">        sum of all these features are output of BERTEmbedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size</span></span><br><span class="line"><span class="string">        :param embed_size: embedding size of token embedding</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)</span><br><span class="line">        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)</span><br><span class="line">        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, sequence, segment_label)</span>:</span></span><br><span class="line">        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x) <span class="comment"># 这里的dropout接在了embedding层后面来防止过拟合</span></span><br></pre></td></tr></table></figure>
<h3><span id="transformer">Transformer △</span></h3><p>Transformer的话是BERT的核心，也是面试必考的知识点。由于BERT是Transformer的Encoder，所以我们这里只考虑Encoder部分。它主要由两部分组成：一个是MultiHeadAttention还有一个就是带Residual和LayerNorm的FFN。放个图在这里方便看。</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="1.png" alt=""></p>
<p>我们先来看Attention部分，多头注意力（Multi-headed attention）机制方法，在编码器和解码器中大量的使用了single Attention机制。由于Transformer的输入只有一个，所以里面用到的Attention都是Self-Attention。</p>
<h4><span id="single-attention">Single Attention</span></h4><p>在计算attention时主要分为三步，第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有<strong>点积、拼接、感知机</strong>等；然后第二步一般是使用一个softmax函数对这些权重进行归一化；最后将权重和相应的键值value进行加权求和得到最后的attention。目前在NLP研究中，key和value常常都是同一个，即key=value。</p>
<p>BERT这里的<strong>缩放点积Scaled Dot-Product</strong>就是计算Q与K之间的点乘的时候，为了防止其结果过大，会除以一个尺度标度$\sqrt{d_k}$，其中$\sqrt{d_k}$为一个query和key向量的维度。因为是用的自注意力，所以$d_{query}=d_{key}=d_{value}$，再利用Softmax操作将其结果归一化为概率分布，然后再乘以矩阵V就得到权重求和的表示。</p>
<p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/attention/single.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute 'Scaled Dot Product Attention</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None, dropout=None)</span>:</span>  </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args: query, key, value 同源且 shape 相同</span></span><br><span class="line"><span class="string">            query: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">            key: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">            value: [batch_size, head_num, seq_len, dim]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">                 / math.sqrt(query.size(<span class="number">-1</span>)) </span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">        p_attn = F.softmax(scores, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            p_attn = dropout(p_attn)</span><br><span class="line">        <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<h4><span id="multi-head-attention">Multi Head Attention</span></h4><p>Multi-head Attention其实就是多个single Attention结构的结合，每个attention head学习到在不同表示空间中的特征，搞h个头会比只有1个头能学到更多的信息。具体实现是Query，Key，Value首先分别各自过一个线性变换（这里的变换矩阵是不一样的），然后输入到放缩点积attention，注意这里要做h次，其实也就是所谓的多头，每一次算一个头。而且每次Q，K，V进行线性变换的参数W是不一样的。然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。</p>
<p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/attention/multi_head.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param h: head的个数</span></span><br><span class="line"><span class="string">        :param d_model: hidden_size</span></span><br><span class="line"><span class="string">        """</span>        </span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        self.d_k = d_model // h		<span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.h = h</span><br><span class="line">        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)])</span><br><span class="line">        self.output_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        self.attention = Attention()</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k</span></span><br><span class="line">        query, key, value = [l(x).view(batch_size, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linear_layers, (query, key, value))]</span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch.</span></span><br><span class="line">        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)</span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="comment"># 4) Applying Output Linear Model</span></span><br><span class="line">        x = self.output_linear(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4><span id="sublayerconnection">SublayerConnection</span></h4><p>这部分其实就是实现 <code>Add &amp; Normalize</code>，ResidualConnection和LayerNorm。根据<code>transformers</code>包对于这部分的实现，应该是先Add再Dropout再Norm。注意Attention和FFN的Sublayer是不同的，而非共享参数。我在读这里的时候存在一个疑问是LayerNorm为什么用的是《Attention All you need》里的<code>Annotated Transformer</code>自己实现的LayerNorm，而不用标准的 <code>torch.nn.LayerNorm</code>？最后在issue里我发现这里其实两个东西的差别不大，用哪个都行，只不过<code>Annotated Transformer</code>返回的梯度类型是<code>ThAddBackward</code> 而<code>torch.nn.LayerNorm</code>返回的是 <code>AddcmulBackward</code>，并且值虽然不一样但都还是归一化的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/utils/layer_norm.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># model/utils/sublayer.py</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer, dropout=True)</span>:</span>		<span class="comment"># 这里传参要传一个forward实例化的函数</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">    	<span class="keyword">return</span> self.norm(x + self.dropout(sublayer(x)) <span class="keyword">if</span> dropout <span class="keyword">else</span> sublayer(x)) <span class="comment"># 先dropout再add再norm</span></span><br></pre></td></tr></table></figure>
<h4><span id="feedforwardnetwork">FeedForwardNetwork</span></h4><p>之前的输入要传给FFN，这里的FFN实际上就是两个线性层+dropout+GELU，然后FFN的输出再后续传给SublayerConnection。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/utils/feed_forward.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff)</span>:</span></span><br><span class="line">        super(FeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.activation = GELU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.w_1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line">        x = self.w_2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4><span id="transformer集成">Transformer集成</span></h4><p>最后的实现就是将前面几部分串起来，最后加个dropout即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/transformer.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Bidirectional Encoder = Transformer (self-attention)</span></span><br><span class="line"><span class="string">    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden, attn_heads, feed_forward_hidden, dropout)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: hidden size of transformer</span></span><br><span class="line"><span class="string">        :param attn_heads: head sizes of multi-head attention</span></span><br><span class="line"><span class="string">        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)</span><br><span class="line">        self.feed_forward = FeedForward(d_model=hidden, d_ff=feed_forward_hidden)</span><br><span class="line">        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        x = self.input_sublayer(x, <span class="keyword">lambda</span> _x: self.attention.forward(_x, _x, _x, mask=mask))</span><br><span class="line">        x = self.output_sublayer(x, <span class="keyword">lambda</span> _x: self.feed_forward.forward(_x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3><span id="bertmodel">BERTModel</span></h3><p>以上的东西一拼，就成了BERT模型了。输入是token id sequence和segment label sequence。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/bert.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT model : Bidirectional Encoder Representations from Transformers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden=<span class="number">768</span>, n_layers=<span class="number">12</span>, attn_heads=<span class="number">12</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param vocab_size: vocab_size of total words</span></span><br><span class="line"><span class="string">        :param hidden: BERT model hidden size</span></span><br><span class="line"><span class="string">        :param n_layers: numbers of Transformer blocks(layers)</span></span><br><span class="line"><span class="string">        :param attn_heads: number of attention heads</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = hidden</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">        self.attn_heads = attn_heads</span><br><span class="line">        <span class="comment"># 原论文将FFN的hidden_size设置为普通hidden_size的4倍</span></span><br><span class="line">        self.feed_forward_hidden = hidden * <span class="number">4</span></span><br><span class="line">        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)</span><br><span class="line">        self.transformer_blocks = nn.ModuleList(</span><br><span class="line">            [TransformerBlock(hidden=hidden, attn_heads=attn_heads,</span><br><span class="line">                              feed_forward_hidden=hidden * <span class="number">4</span>, dropout=dropout)</span><br><span class="line">             <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, segment_info)</span>:</span></span><br><span class="line">        <span class="comment"># attention masking for padded token</span></span><br><span class="line">        <span class="comment"># torch.ByteTensor([batch_size, 1, seq_len, seq_len)</span></span><br><span class="line">        mask = (x &gt; <span class="number">0</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, x.size(<span class="number">1</span>), <span class="number">1</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># padding为0，非padding为1</span></span><br><span class="line">        <span class="comment"># embedding the indexed sequence to sequence of vectors</span></span><br><span class="line">        x = self.embedding(x, segment_info)</span><br><span class="line">        <span class="comment"># running over multiple transformer blocks</span></span><br><span class="line">        <span class="keyword">for</span> transformer <span class="keyword">in</span> self.transformer_blocks:</span><br><span class="line">            x = transformer.forward(x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2><span id="预训练">预训练</span></h2><h3><span id="nsp-amp-mlm">NSP &amp; MLM</span></h3><p>主要包含两个训练任务，NSP就是bert后接一个 <code>linear+softmax</code> 做二分类，MLM是BERT后接linear+softmax做大小为<code>vocab_size</code>的多分类（但是transformers包里面的实现是接 <code>linear(hidden\*hidden) + activation + layerNorm + linear(hidden\*vocab_size)</code> 相当于两层MLP多分类）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model/language_model.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NextSentencePrediction</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    2-class classification model : is_next, is_not_next</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: BERT model output size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(hidden, <span class="number">2</span>)</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(x[:, <span class="number">0</span>]).tanh())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedLanguageModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    predicting origin token from masked input sequence</span></span><br><span class="line"><span class="string">    n-class classification problem, n-class = vocab_size</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden, vocab_size, embedding=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param hidden: output size of BERT model</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.linear = nn.Linear(hidden, vocab_size)</span><br><span class="line">        <span class="keyword">if</span> embedding <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self.linear.weight.data = embedding.weight.data</span><br><span class="line">        self.softmax = nn.LogSoftmax(dim=<span class="number">-1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.softmax(self.linear(x))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTLM</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERT Language Model</span></span><br><span class="line"><span class="string">    Next Sentence Prediction Model + Masked Language Model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert: BERT, vocab_size)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param bert: BERT model which should be trained</span></span><br><span class="line"><span class="string">        :param vocab_size: total vocab size for masked_lm</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.bert = bert</span><br><span class="line">        self.next_sentence = NextSentencePrediction(self.bert.hidden)</span><br><span class="line">        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size,</span><br><span class="line">                                           embedding=self.bert.embedding.token)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, segment_label)</span>:</span></span><br><span class="line">        x = self.bert(x, segment_label)</span><br><span class="line">        <span class="keyword">return</span> self.next_sentence(x), self.mask_lm(x)</span><br></pre></td></tr></table></figure>
<h3><span id="trainer">Trainer</span></h3><p>这里需要注意的一点是，两个预训练任务都采用<code>NLL loss</code>，对于MLM任务要忽略0（代表<code>&lt;pad&gt;</code>）所以加了个ignore，但是对于NSP由于句子的标签是0或1，所以这里的0是不可以忽略的，也就是说<code>&lt;pad&gt;</code>要参与进训练。优化器使用了<code>AdamW</code>，这里是原作者自己实现的没用现成的包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trainer/pretrain.py</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BERTTrainer</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    BERTTrainer make the pretrained BERT model with two LM training method.</span></span><br><span class="line"><span class="string">        1. Masked Language Model : 3.3.1 Task #1: Masked LM</span></span><br><span class="line"><span class="string">        2. Next Sentence prediction : 3.3.2 Task #2: Next Sentence Prediction</span></span><br><span class="line"><span class="string">    please check the details on README.md with simple example.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, bert: BERT, vocab_size: int,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lr: float = <span class="number">1e-4</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, weight_decay: float = <span class="number">0.01</span>, warmup_steps=<span class="number">10000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 with_cuda: bool = True, cuda_devices=None, log_freq: int = <span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param bert: BERT模型</span></span><br><span class="line"><span class="string">        :param vocab_size: 词汇表大小</span></span><br><span class="line"><span class="string">        :param train_dataloader: 训练集dataloader</span></span><br><span class="line"><span class="string">        :param test_dataloader: 测试机dataloader[can be None]</span></span><br><span class="line"><span class="string">        :param lr: 学习率</span></span><br><span class="line"><span class="string">        :param betas: 优化器偏差 Adam optimizer betas</span></span><br><span class="line"><span class="string">        :param weight_decay: 权重衰减参数 Adam optimizer weight decay param</span></span><br><span class="line"><span class="string">        :param with_cuda: 是否使用cpu traning with cuda</span></span><br><span class="line"><span class="string">        :param log_freq: 日志频率 logging frequency of the batch iteration</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 设置是否使用gpu, argument -c, --cuda should be true</span></span><br><span class="line">        cuda_condition = torch.cuda.is_available() <span class="keyword">and</span> with_cuda</span><br><span class="line">        self.device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> cuda_condition <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># This BERT model will be saved every epoch</span></span><br><span class="line">        self.bert = bert</span><br><span class="line">        <span class="comment"># Initialize the BERT Language Model, with BERT model</span></span><br><span class="line">        <span class="comment"># 初始化BERT并作为参数初始化BERT Lanuage Model</span></span><br><span class="line">        self.model = BERTLM(bert, vocab_size).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分布式GPU训练 Distributed GPU training if CUDA can detect more than 1 GPU</span></span><br><span class="line">        <span class="keyword">if</span> with_cuda <span class="keyword">and</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">            print(<span class="string">"Using %d GPUS for BERT"</span> % torch.cuda.device_count())</span><br><span class="line">            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the train and test data loader</span></span><br><span class="line">        self.train_data = train_dataloader</span><br><span class="line">        self.test_data = test_dataloader</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Setting the Adam optimizer with hyper-param</span></span><br><span class="line">        self.optim = AdamW(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)</span><br><span class="line">        <span class="comment"># self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 两个任务都采用NLL loss，对于MLM任务0（代表&lt;pad&gt;）要忽略所以加了个ignore，但是对于NSP由于句子的标签是0或1，所以这里的0是不可以忽略的，也就是说&lt;pad&gt;要参与进训练。</span></span><br><span class="line">        <span class="comment"># Using Negative Log Likelihood Loss function for predicting the masked_token</span></span><br><span class="line">        self.masked_criterion = nn.NLLLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">        self.next_criterion = nn.NLLLoss()</span><br><span class="line"></span><br><span class="line">        self.log_freq = log_freq</span><br><span class="line"></span><br><span class="line">        print(<span class="string">"Total Parameters:"</span>, sum([p.nelement() <span class="keyword">for</span> p <span class="keyword">in</span> self.model.parameters()]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        self.iteration(epoch, self.train_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(self, epoch)</span>:</span></span><br><span class="line">        self.iteration(epoch, self.test_data, train=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">iteration</span><span class="params">(self, epoch, data_loader, train=True)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        loop over the data_loader for training or testing</span></span><br><span class="line"><span class="string">        if on train status, backward operation is activated</span></span><br><span class="line"><span class="string">        and also auto save the model every peoch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param epoch: current epoch index</span></span><br><span class="line"><span class="string">        :param data_loader: torch.utils.data.DataLoader for iteration</span></span><br><span class="line"><span class="string">        :param train: boolean value of is train or test</span></span><br><span class="line"><span class="string">        :return: None</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        str_code = <span class="string">"train"</span> <span class="keyword">if</span> train <span class="keyword">else</span> <span class="string">"test"</span></span><br><span class="line"></span><br><span class="line">        avg_loss = <span class="number">0.0</span></span><br><span class="line">        total_correct = <span class="number">0</span></span><br><span class="line">        total_element = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(data_loader):</span><br><span class="line">            <span class="comment"># 0. batch_data will be sent into the device(GPU or cpu)</span></span><br><span class="line">            data = &#123;key: value.to(self.device) <span class="keyword">for</span> key, value <span class="keyword">in</span> data.items()&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 1. forward the next_sentence_prediction and masked_lm model</span></span><br><span class="line">            next_sent_output, mask_lm_output = self.model.forward(data[<span class="string">"bert_input"</span>], data[<span class="string">"segment_label"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-1. NLL(negative log likelihood) loss of is_next classification result</span></span><br><span class="line">            next_loss = self.next_criterion(next_sent_output, data[<span class="string">"is_next"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-2. NLLLoss of predicting masked token word</span></span><br><span class="line">            mask_loss = self.masked_criterion(mask_lm_output.transpose(<span class="number">1</span>, <span class="number">2</span>), data[<span class="string">"bert_label"</span>])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 2-3. Adding next_loss and mask_loss : 3.4 Pre-training Procedure</span></span><br><span class="line">            loss = next_loss + mask_loss</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 3. backward and optimization only in train</span></span><br><span class="line">            <span class="keyword">if</span> train:</span><br><span class="line">                self.optim.zero_grad()</span><br><span class="line">                loss.backward()</span><br><span class="line">                self.optim.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># next sentence prediction accuracy</span></span><br><span class="line">            correct = next_sent_output.argmax(dim=<span class="number">-1</span>).eq(data[<span class="string">"is_next"</span>]).sum().item()</span><br><span class="line">            avg_loss += loss.item()</span><br><span class="line">            total_correct += correct</span><br><span class="line">            total_element += data[<span class="string">"is_next"</span>].nelement()</span><br><span class="line"></span><br><span class="line">            post_fix = &#123;</span><br><span class="line">                <span class="string">"epoch"</span>: epoch,</span><br><span class="line">                <span class="string">"iter"</span>: <span class="string">"[%d/%d]"</span> % (i, len(data_loader)),</span><br><span class="line">                <span class="string">"avg_loss"</span>: avg_loss / (i + <span class="number">1</span>),</span><br><span class="line">                <span class="string">"mask_loss"</span>: mask_loss.item(),</span><br><span class="line">                <span class="string">"next_loss"</span>: next_loss.item(),</span><br><span class="line">                <span class="string">"avg_next_acc"</span>: total_correct / total_element * <span class="number">100</span>,</span><br><span class="line">                <span class="string">"loss"</span>: loss.item()</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % self.log_freq == <span class="number">0</span>:</span><br><span class="line">                print(post_fix)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Logging for PaperSpace matrix monitor</span></span><br><span class="line">                <span class="comment"># index = epoch * len(data_loader) + i</span></span><br><span class="line">                <span class="comment"># for code in ["avg_loss", "mask_loss", "next_loss", "avg_next_acc"]:</span></span><br><span class="line">                <span class="comment">#     print(json.dumps(&#123;"chart": code, "y": post_fix[code], "x": index&#125;))</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"EP%d_%s, avg_loss="</span> % (epoch, str_code), avg_loss / len(data_loader), <span class="string">"total_acc="</span>,</span><br><span class="line">              total_correct * <span class="number">100.0</span> / total_element)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, epoch, file_path=<span class="string">"output/bert_trained.model"</span>)</span>:</span></span><br><span class="line">        <span class="string">""</span></span><br><span class="line">        Saving the current BERT model on file_path</span><br><span class="line">        :param epoch: current epoch number</span><br><span class="line">        :param file_path: model output path which gonna be file_path+<span class="string">"ep%d"</span> % epoch</span><br><span class="line">        :<span class="keyword">return</span>: final_output_path</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        output_path = file_path + ".ep%d" % epoch</span></span><br><span class="line"><span class="string">        torch.save(self.bert.cpu(), output_path)</span></span><br><span class="line"><span class="string">        self.bert.to(self.device)</span></span><br><span class="line"><span class="string">        print("EP:%d Model Saved on:" % epoch, output_path)</span></span><br><span class="line"><span class="string">        return output_path</span></span><br></pre></td></tr></table></figure>
<h2><span id="参考文献">参考文献</span></h2><ol>
<li>BERT-pytorch：<a href="https://github.com/codertimo/BERT-pytorch/" target="_blank" rel="noopener">https://github.com/codertimo/BERT-pytorch/</a></li>
<li>BERT源码-tensorflow：<a href="https://github.com/google-research/bert" target="_blank" rel="noopener">https://github.com/google-research/bert</a></li>
<li>图解什么是Transformer：<a href="https://www.jianshu.com/p/e7d8caa13b21" target="_blank" rel="noopener">https://www.jianshu.com/p/e7d8caa13b21</a></li>
<li>transformers包：<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">https://github.com/huggingface/transformers</a></li>
<li>Attention机制详解（二）——Self-Attention与Transformer：<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47282410</a></li>
</ol>
<blockquote>
<p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p>
<p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p>
<p>原文链接：<a href="https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读</a></p>
</blockquote>

        </div>
      </article>
    </div>

	<!-- 打赏 -->
    <div class="reward">
	<div class="reward-button">赏 <span class="reward-code">
		<span class="alipay-code"> <img class="alipay-img wdp-appear" src="data:image/png;base64,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" data-original="/images/alipay.webp"><b>支付宝打赏</b> </span> 
		<span class="wechat-code"> <img class="wechat-img wdp-appear" src="data:image/png;base64,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" data-original="/images/weixin.webp"><b>微信打赏</b> </span> </span>
	</div>
	<p class="reward-notice">如果文章对你有帮助，欢迎点击上方按钮打赏作者，更多文章请访问<a href="https://vodkazy.cn" style="color:blue">想飞的小菜鸡</a></p>
	    <style>
		*,*:before,*:after {
			-webkit-box-sizing: border-box;
			-moz-box-sizing: border-box;
			-ms-box-sizing: border-box;
			box-sizing: border-box
		}

		.reward {
			padding: 5px 0
		}

		.reward .reward-notice {
			font-size: 14px;
			line-height: 14px;
			margin: 15px auto;
			text-align: center
		}

		.reward .reward-button {
			font-size: 28px;
			line-height: 58px;
			position: relative;
			display: block;
			width: 60px;
			height: 60px;
			margin: 0 auto;
			padding: 0;
			-webkit-user-select: none;
			text-align: center;
			vertical-align: middle;
			color: #fff;
			border: 1px solid #f1b60e;
			border-radius: 50%;
			background: #fccd60;
			background: -webkit-gradient(linear,left top,left bottom,color-stop(0,#fccd60),color-stop(100%,#fbae12),color-stop(100%,#2989d8),color-stop(100%,#207cca));
			background: -webkit-linear-gradient(top,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%);
			background: linear-gradient(to bottom,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%)
		}

		.reward .reward-code {
			position: absolute;
			top: -220px;
			left: 50%;
			display: none;
			width: 350px;
			height: 200px;
			margin-left: -175px;
			padding: 15px;
			border: 1px solid #e6e6e6;
			background: #fff;
			box-shadow: 0 1px 1px 1px #efefef
		}

		.reward .reward-button:hover .reward-code {
			display: block
		}

		.reward .reward-code span {
			display: inline-block;
			width: 150px;
			height: 150px
		}

		.reward .reward-code span.alipay-code {
			float: left
		}

		.reward .reward-code span.alipay-code a {
			padding: 0
		}

		.reward .reward-code span.wechat-code {
			float: right
		}

		.reward .reward-code img {
			display: inline-block;
			float: left;
			width: 150px;
			height: 150px;
			margin: 0 auto;
			border: 0
		}

		.reward .reward-code b {
			font-size: 14px;
			line-height: 26px;
			display: block;
			margin: 0;
			text-align: center;
			color: #666
		}

		.reward .reward-code b.notice {
			line-height: 2rem;
			margin-top: -1rem;
			color: #999
		}

		.reward .reward-code:after,.reward .reward-code:before {
			position: absolute;
			content: '';
			border: 10px solid transparent
		}

		.reward .reward-code:after {
			bottom: -19px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #fff
		}

		.reward .reward-code:before {
			bottom: -20px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #e6e6e6
		}
    </style>


    <!-- Pre or Next -->
    
	<div class="container">
           <ul class="pager">
    	     
      	     <li class="previous">
              <a href="/2020/10/19/我想去面试系列——Word2vec/" rel="prev">上一篇</a>
             </li>
           
           
              <li class="next">
              <a href="/2020/10/10/我想去面试系列——BERT/" rel="prev">下一篇</a>
            </li>
           
          </ul>
       </div>
   

    <!-- Valine无后端评论系统 -->   
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
		    el: '#vcomments' ,
		    appId: 'lH3VkMCd4MHaKtr2n2SRWdoi-MdYXbMMI',
		    appKey: '5aMXSY7b4KwnzfgpzLA0hPLv',
		    notify:true, 
		    verify:false, 
		    placeholder: '填写正确的邮箱和昵称才能收到我的回复哦       ٩( ^o^ )و  ' ,
		    avatar: 'retro'
		});
    </script>
    <!-- Valine无后端评论系统 -->  

  </div>
</div>
</div>

  <!-- Footer -->
  <!-- Footer -->
<footer class="site-info">
  <p>
    <span>想飞的小菜鸡 &copy; 2021</span>
    
      <span class="split">|</span>
      <span>照耀的Blog</span>
    
  </p>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
    本站访客数<span id="busuanzi_value_site_uv"></span>人次
    本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</footer>

  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>


 
  <!-- 使用 aotuload.js 引入看板娘 -->    
  <!-- //<script src="/js/assets/jquery.min.js?v=3.3.1"></script> -->   
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <!-- //<script src="/js/assets/jquery-ui.min.js?v=1.12.1"></script>   --> 
  <script src="https://cdn.jsdelivr.net/npm/jquery-ui-dist@1.12.1/jquery-ui.min.js"></script>
  <script src="/js/assets/autoload.js?v=1.4.2"></script>
  <!-- //<script src="https://live2d-cdn.fghrsh.net/assets/1.4.2/autoload.js></script> -->   
   


<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],e=void 0,0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script><!-- hexo-inject:begin --><!-- hexo-inject:end --></body>

</html>
