<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

	<!-- 百度统计 -->
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "https://hm.baidu.com/hm.js?e31627579358722b9d300535c8206351";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Vodkazy">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="我想去面试系列——BERT">
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="想飞的小菜鸡">
  <!--Type page-->
  
      <meta property="og:type" content="article">
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- Title -->
  
  <title>我想去面试系列——BERT - 想飞的小菜鸡</title>


  <link rel="shortcut icon" href="/../images/icon.ico">
  <!--font-awesome-->
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body>

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="data:image/png;base64,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" data-original="https://cdn2.iconfinder.com/data/icons/weather-color-2/500/weather-01-128.png" alt="想飞的小菜鸡" height="60">
        
      </a>
    </div>
    <a class="header-name" href="/">
            <span>想飞的小菜鸡</span>
            的小窝
        </a>
    <!-- navbar -->
    <nav class="navbar">
      <!--  nav links -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  <i class="fa fa-home "></i>
                
                首页
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  <i class="fa fa-archive "></i>
                
                目录
              </a>
            </li>
          
            <li>
              <a href="/project">
                
                  <i class="fa fa-folder-open "></i>
                
                代码库
              </a>
            </li>
          
            <li>
              <a href="/photo">
                
                  <i class="fa fa-photo "></i>
                
                相册薄
              </a>
            </li>
          
            <li>
              <a href="/lovetree">
                
                  <i class="fa fa-tree "></i>
                
                爱情树
              </a>
            </li>
          
            <li>
              <a href="/guestbook">
                
                  <i class="fa fa-edit "></i>
                
                留言板
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  <i class="fa fa-user "></i>
                
                关于我
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            我想去面试系列——BERT
            
          </h1>
         
        </div>
         <p class="a-posted-on">
          2020-10-10
          </p>
        <!-- Post Main Content -->
        <div class="entry-content">
          <p>最近在准备一些面试的东西，正在以面试的角度去温习一些知识点，本文记录的是Bert相关的内容，主要包括基本原理、模型框架、其他变型、细节解读&amp;面试题。</p>
<a id="more"></a>
<!-- toc -->
<ul>
<li><a href="#基本原理">基本原理</a></li>
<li><a href="#模型框架">模型框架</a></li>
<li><a href="#实现">实现</a></li>
<li><a href="#其他变形">其他变形</a></li>
<li><a href="#细节-面试题搜集">细节 &amp; 面试题搜集</a></li>
<li><a href="#细节-面试题搜集-1">细节 &amp; 面试题搜集</a><ul>
<li><a href="#bert本身">BERT本身</a></li>
<li><a href="#bert变型">BERT变型</a></li>
<li><a href="#场景问题">场景问题</a></li>
</ul>
</li>
<li><a href="#参考文献">参考文献</a></li>
</ul>
<!-- tocstop -->
<blockquote>
<p>Bert的面试重点在transformer架构、multi-head attention、position embedding、<a href="https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读" target="_blank" rel="noopener">源码</a>。</p>
</blockquote>
<h2><span id="基本原理">基本原理</span></h2><p><a href="https://arxiv.org/pdf/1810.04805" target="_blank" rel="noopener">BERT：<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers</a>，整体是一个自编码语言模型（Autoencoder LM）。Bidirectional体现在每一个词向量的产生都同时依赖该单词左侧和右侧的语境信息。</p>
<p><img src="data:image/png;base64,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" data-original="0.png" alt=""></p>
<p>Bert设计了两个任务来联合预训练该模型，MLM+NSP，训练时候的loss是两个任务的loss sum：</p>
<ul>
<li><p>第一个任务是采用 MaskLM 的方式来训练语言模型，形式化为在给定单词上下文序列之后，球当前单词出现的条件概率的乘积。具体的，训练集中选择15%的mask单词，并以特殊标记<code>[MASK]</code>进行替换。为减少微调时对<code>[MASK]</code>标记的过拟合，数据生成器将不是始终用<code>[MASK]</code>替换所选单词，而是80%的时间里将单词替换成<code>[MASK]</code>，10%的时间里用随机单词替换，10%的时间保持单词不变。这样做的目的是使表示偏向实际观察到的单词。这么做的主要原因是：在后续微调任务中语句中并不会出现 [MASK] 标记，而且这么做的另一个好处是：预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。上述提到了这样做的一个缺点，其实这样做还有另外一个缺点，就是每批次数据中只有 15% 的标记被预测，这意味着模型可能需要更多的预训练步骤来收敛。与去噪的自动编码器（<a href="https://www.researchgate.net/publication/221346269_Extracting_and_composing_robust_features_with_denoising_autoencoders" target="_blank" rel="noopener">Vincent et al., 2008</a>）不同的是，MLM任务只是让模型预测被遮蔽的标记，而不是要求模型重建整个输入。</p>
<p><img src="data:image/png;base64,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" data-original="MLM.png" alt=""></p>
</li>
<li><p>第二个任务在双向语言模型的基础上额外增加了一个句子级别的连续性预测任务，即预测输入 BERT 的两段文本是否为连续的文本，引入这个任务可以更好地让模型学到连续的文本片段之间的关系。（RoBerta里说这项任务其实没啥用，起主要作用的还是MLM），这里的masking是静态生成的，每次mask的在训练之前都固定好了。具体的，当为每个预训练选择句子A和B时，50%的时间是选择紧跟着A的实际的下一个句子作为B，而另外50%的时间是随机采样语料库中其他的错误句子。最终的预训练模型在这个任务中达到了 97%-98% 的准确率。</p>
</li>
<li><p>对于预训练语料库，使用 BooksCorpus（800M 单词）（<a href="https://arxiv.org/abs/1506.06724v1" target="_blank" rel="noopener">Zhu et al., 2015</a>）和英语维基百科（2,500M 单词）。对于维基百科，只提取文本段落，而忽略列表、表格和标题。</p>
<script type="math/tex; mode=display">BERT_{base}: L = 12, H = 768, A = 12, TotalParameters = 110M</script><script type="math/tex; mode=display">BERT_{large}: L = 24, H = 1024, A = 16, TotalParameters = 340M</script><p>这里$L$代表Transformer的层数，$H$代表隐藏层大小，$A$代表自注意力的头的个数。</p>
</li>
</ul>
<h2><span id="模型框架">模型框架</span></h2><ul>
<li><p>Encoder：Transformer Encoder。实际上，<strong>Transformer的Encoder（双向Transformer）就是Bert，Transformer的Decoder（单左向Transformer）稍微改了一点就是GPT。</strong>与 Transformer 本身的 Encoder 端相比，BERT 的 Transformer Encoder 端输入的向量表示多了 Segment Embeddings。</p>
</li>
<li><p><img src="data:image/png;base64,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" data-original="3.png" alt=""></p>
</li>
<li><p>每一层的输入是token+seg+pos embedding，记为$e_0$，将$e_0$输入mult-head attention（注意这里的multi-head的输入是QKV，但是里面的每一个self-attention的输入是QW,KW,VW），然后将该输出与原输入<strong>残差求和ResidualConnection</strong>，<strong>层归一化LayerNormalization</strong>，得到$e_{mid}$，再将$e_{mid}$输入FFN再残差求和得到每一层最后的输出。这里残差机制的意义是可以使得模型更深，避免梯度爆炸和梯度消失的问题；所有层的FFN是参数共享的。</p>
<script type="math/tex; mode=display">e_0 = Embedding_{token}(inputs) + Embedding_{seg}(inputs) + Embedding_{pos}(inputs)</script><script type="math/tex; mode=display">e_l = EncoderLayer(e_{l-1}, l \in [1.n])</script><p>其中 $e_i \in \mathbb{R}^{N \times d_{model}}$。EncoderLayer的架构是：</p>
<script type="math/tex; mode=display">e_{mid} = LayerNorm(e_{in} + MultiHeadAttention(e_{in}))</script><script type="math/tex; mode=display">e_{out} = LayerNorm(e_{mid} + FFN(e_{mid}))</script></li>
<li><p>Multi-Head-Attention：输入向量序列$e_{in} = (e_{in1},e_{in2},…,e_{inN}) \in \mathbb{R}^{N \times d_{model}}, Q = e_{in}, K = e_{in}, V = e_{in}$。</p>
<script type="math/tex; mode=display">MultiHeadAttention(e_{in}) = MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O</script><p>其中，多头输出$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$，$Concat(head_1, …, head_h) \in \mathbb{R}^{N \times hd_v}$，可学习的参数矩阵$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$，$W_i^K \in \mathbb{R}^{d_{model} \times d_k}$，$W_i^V \in \mathbb{R}^{d_{model} \times d_v}$，$W_O \in \mathbb{R}^{hd_v \times d_{model}}$。</p>
<p>使用缩放点积作为打分函数的自注意力机制：</p>
<script type="math/tex; mode=display">Attention(QW_i^Q, KW_i^K, VW_i^K) = softmax(\frac{QW_i^Q(KW_i^K)^\top}{\sqrt{d_k}})VW_i^V</script></li>
<li><p>前馈神经网络FFN：这里需要注意的是使用了<strong>GELU</strong>作为激活函数（这里的目的是对于学习率的 warm-up 策略，使用的激活函数不再是普通的 ReLu），这里参数矩阵$W_1 \in \mathbb{R}^{d_{model} \times d_{ff}}$，$W_2 \in \mathbb{R}^{d_{ff} \times d_{model}}$，$b_1 \in \mathbb{R}^{d_{ff}}$，$b_2 \in \mathbb{R}^{d_{model}}$。</p>
<script type="math/tex; mode=display">FFN(e_{mid}) = GELU(e_{mid}W_1 + b_1)W_2 + b_2</script></li>
<li><p>下游任务：句子级别：Sentence Pair Classification（取<code>[CLS]</code>后接全连接层+sigmoid）、Single Sentence Classification（取<code>[CLS]</code>后接全连接层+softmax）；单词级别：Question Answering（取特定区间的token后接其他NN）、Single Sentence Tagging（序列标注多分类，对于每一个token有几个 label 就连接到几个全连接层，再接softmax，然后遍历特定区间的所有token）。分析证实，在QA任务上，使用最高层的<strong>[CLS]</strong>效果更好；在序列标注任务上，使用<strong>Attention</strong>方法集成多层词向量效果最好；在小数据集上BiLSTM要比BERT要好。这里存在个疑问，SQuAD上预测起始/结束位置时，在具体实现的时候是两个独立的条件概率呢，还是联合概率呢？</p>
<p><img src="data:image/png;base64,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" data-original="4.png" alt=""></p>
</li>
</ul>
<h2><span id="实现">实现</span></h2><p>利用huggingface的transformers包。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="comment"># BertTokenizer: 分词工具</span></span><br><span class="line"><span class="comment"># BertModel: BERT模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># inputs = tokenizer("This is sentence 1.", return_tensors="pt")</span></span><br><span class="line">inputs = tokenizer(<span class="string">"This is sentence 1."</span>, <span class="string">"This is sentence 2."</span>, return_tensors=<span class="string">"pt"</span>) <span class="comment"># 返回&#123;'input_ids':tensor([[]]), 'token_type_ids':tensor([[]]), 'attention_mask':tensor([[]])&#125;</span></span><br><span class="line">tokenizer.decode(inputs[<span class="string">"input_ids"</span>].data.cpu().numpy().reshape(<span class="number">-1</span>)) <span class="comment"># 可反向还原句子（OOV单词会被替换为[UNK]）</span></span><br><span class="line">sequence_outputs, pooled_outputs = model(**inputs) <span class="comment">#前者是token embedding（1*len_token*hidden_size）,后者是segment embedding（1*len_token），对[CLS]做了池化之后的结果</span></span><br><span class="line"><span class="comment"># 视频讲师说他基本上不用pooled_outputs来当整个句子的表示，因为不怎么能吸收整个句子的语义</span></span><br><span class="line"><span class="comment"># 他提供的trick是，所有向量取平均</span></span><br><span class="line">sen_vec = sequence_outputs.mean(<span class="number">1</span>)</span><br><span class="line">cos_similarity = (sen_vec * sen_vec).sum(<span class="number">-1</span>) / torch.sqrt((sen_vec * sen_vec).sum(<span class="number">-1</span>)) <span class="comment"># cosine similarity</span></span><br></pre></td></tr></table></figure>
<h2><span id="其他变形">其他变形</span></h2><p><strong><a href="https://arxiv.org/pdf/1906.08237.pdf" target="_blank" rel="noopener">XLNet</a></strong>，是BERT之后的一个自回归（Auto Regression，AR）语言模型。可以理解为BERT是一个自编码（Auto Encoder，AE）模型，将输入句子的某些单词 mask 掉，然后再通过 BERT 还原数据。而XLNet这类自回归语言模型则是不断地使用当前得到的信息预测下一个输出。AR 的方法可以更好地学习 token 之间的依赖关系但是它只能利用单向信息（纯前向或者纯后向），而 AE 的方法可以更好地利用深层的双向信息。因此 XLNet 希望将 AR 和 AE 两种方法的优点结合起来，XLNet 使用了 <strong>Permutation Language Model (PLM)</strong>实现这一目的。目的就是为了解决BERT的[MASK]在训练和推理中不一致的问题。</p>
<ul>
<li>使用排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然。Permutation 指排列组合的意思，XLNet 将句子中的 token 随机排列，然后采用 AR 的方式预测末尾的几个 token。这样一来，在预测 token 的时候就可以同时利用该 token 双向的信息，并且能学到 token 间的依赖。XLNet 中通过 Attention Mask 实现 PLM，而无需真正修改句子 token 的顺序。</li>
<li>采用基于目标感知特征的双流自注意力。无论预测目标的位置在哪里，因式分解后得到的所有情况都是一样的，并且transformer的权重对于不同的情况是一样的，因此无论目标位置怎么变都能得到相同的分布结果。为了解决这个问题，论文中提出来新的分布计算方法Two-Stream Self-Attention，来实现目标位置感知。这个需要看原论文。<ul>
<li>如果目标是预测$x_{z_t}，g_{\theta}(x_{z_{&lt;t}}，z_t)$那么只能有其位置信息$z_t$而不能包含内容信息$x_{z_t}$</li>
<li>如果目标是预测其他tokens即$x_{z_j}， j&gt;t$，那么应该包含$x_{z_t}$的内容信息这样才有完整的上下文信息</li>
</ul>
</li>
<li>作者还将transformer-xl的两个最重要的技术点应用了进来，即<strong>相对位置编码</strong>与<strong>片段循环机制</strong>。transformer-xl的提出主要是为了解决超长序列的依赖问题，对于普通的transformer由于有一个最长序列的超参数控制其长度，对于特别长的序列就会导致丢失一些信息，transformer-xl就能解决这个问题。对于超长文本，如果采用transformer-xl，首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算。该机制不但能保留长依赖关系还能加快训练，因为每一个前置片段都保留了下来，不需要再重新计算，在transformer-xl的论文中，经过试验其速度比transformer快了1800倍。另一方面BERT的position embedding采用的是绝对位置编码，但是绝对位置编码在transformer-xl中有一个致命的问题，因为没法区分到底是哪一个片段里的，这就导致了一些位置信息的损失，这里被替换为了transformer-xl中的相对位置编码（相对距离）。</li>
<li>去除了NSP任务，作者发现该任务对结果的提升并没有太大的影响，主要原因是NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了。</li>
</ul>
<p><strong><a href="https://arxiv.org/pdf/1905.07129.pdf" target="_blank" rel="noopener">Ernie清华</a></strong>，提出用知识图谱来增强预训练模型的能力，其实就是在与训练的时候在BERT的基础上加入一个实体对齐<strong>Entity Alignment</strong>任务。模型有两个encoder，T-encoder和K-encoder，其实这里的K-encoder只有在预训练的时候有作用，在之后的fine-tuning阶段只要使用T-encoder就可以了，所以这里的重要就是引入了实体对齐这个任务而已。本文提出了随机mask tokens-entity中的entity，然后去预测该位置对应的entity，本质上和MLM（mask language model）任务一致，都属于去噪自编码。KG的引入使得在一些和知识图谱相关的上游任务中，该模型的表现要优于BERT。</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="ernie_thu.png" alt=""></p>
<p><strong><a href="https://arxiv.org/pdf/1904.09223v1.pdf" target="_blank" rel="noopener">Ernie百度</a></strong>，也是引入了知识信息， 但是做法与清华的Ernie完全不一样，这里主要的改变是针对bert中的MLM任务做了一些改进。在bert中只是mask了单个token，但是在语言中，很多时候都是以短语或者实体存在的，如果不考虑短语或者实体中词之间的相关性，而将所有的词独立开来，不能很好的表达句法，语义等信息，因此本文引入了三种mask的方式，分别对token，entity，phrase进行mask。除此之外，本论文中还引入了对话语料，丰富语料的来源，并针对对话语料，给出了一个和NSP相似的任务<strong>Dialogue Language Model (DLM)</strong>。</p>
<p><strong><a href="https://arxiv.org/pdf/1907.11692.pdf" target="_blank" rel="noopener">RoBERTa</a></strong>，改进版Bert，是目前最好用的，它在模型层面没有改变原BERT，改变的只是预训练的方法。</p>
<ul>
<li>更大的batch size，加大训练数据16GB-&gt;160GB，训练时间更长。原本的BERTbase 的batch size是256，训练1M个steps。RoBERTa的batch size为8k。为什么要用更大的batch size呢？（除了因为他们有钱玩得起外）作者借鉴了在机器翻译中，用更大的batch size配合更大学习率能提升模型优化速率和模型性能的现象，并且也用实验证明了确实Bert还能用更大的batch size。</li>
<li>不需要Next Sentence Prediction Loss（同时也发现了segment embedding其实作用不大）RoBERTa去除了NSP，每次输入连续的多个句子，直到最大长度512（可以跨文章）。这种训练方式叫做（FULL - SENTENCES），而原来的Bert每次只输入两个句子。实验表明在MNLI这种推断句子关系的任务上RoBERTa也能有更好性能。</li>
<li>使用更长的训练sequence。</li>
<li>dynamic masking。原来的BERT采用的是<strong>static masking</strong>，也就是在dataloader创造预训练数据的时候就已经决定了要mask哪个。整个训练过程，这15%的Tokens一旦被选择就不再改变，也就是说从一开始随机选择了这15%的Tokens，之后的N个epoch里都不再改变了。而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。</li>
</ul>
<p><strong><a href="https://openreview.net/pdf?id=H1eA7AEtvS" target="_blank" rel="noopener">Albert</a></strong>，新的轻量版的BERT，参数比BERT少了18倍。</p>
<ul>
<li>对Embedding因式分解（Factorized embedding parameterization）。ALBERT的作者注意到，对于BERT、XLNet和RoBERTa，word embedding的维度(<code>E</code>)与encoder输出的hidden embedding维度(<code>H</code>)是一样的都是768，<code>H==E</code>，这就意味着二者表达含义的是同等重要的。然而实际上word embedding是用来学习上下文独立表示的，hidden embedding是为了学习上下文依赖表示的。理论上来说hidden embedding的表述包含的信息应该更多一些，因此应该让 <code>H&gt;&gt;E</code>。在NLP任务中，通常词典都会很大，embedding matrix的大小是 <code>E×V</code>，如果和BERT一样让 <code>H==E</code>，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为<code>E</code>，然后再映射到一个高维度的空间，说白了就是先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内，从而把参数量从<code>O(V×H)</code>降低到了<code>O(V×E+E×H)</code>，当<code>E&lt;&lt;H</code>时参数量减少的很明显。</li>
<li>跨层的参数共享（Cross-layer parameter sharing）。Albert的核心思想是共享层与层之间的参数，全连接层与attention层都进行参数共享，也就是说共享encoder内的所有参数，每一层的hidden_size变大，所有24层的transformer的参数都用同一个。虽说看起来模型更小更深了，但是实际inference的时候还是会比bert慢的，因为虽然参数共享了，但推理的时候还是得过相同的N层。实际上是通过参数共享的方式降低了内存，预测阶段还是需要和BERT一样的时间，如果采用了xxlarge版本的ALBERT，那实际上预测速度会更慢。</li>
<li>把NSP任务换成了sentence ordering objective。BERT的NSP任务实际上是一个二分类，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了，并且在MLM任务中其实也有类型的效果。在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 <strong>sentence-order prediction（SOP）</strong>，SOP的正样本和NSP的获取方式是一样的，负样本把正样本的顺序反转即可。SOP因为实在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务。</li>
<li>ALBERT的作者还发现一个很有意思的点，ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</li>
</ul>
<p><strong><a href="https://openreview.net/pdf?id=r1xMH1BtvB" target="_blank" rel="noopener">Electra</a></strong>，最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。</p>
<ul>
<li>提出了 <strong>Replaced Token Detection (RTD)</strong> 预训练任务，判断每个词是否是被替换过的词。</li>
<li>ELECTRA 由两个部分组成，第一部分是生成器 (Generator)，生成器将句子中的部分单词进行替换。第二部分是判别器 (Discriminator)，判别器用于判断一个句子中每一个单词是否被替换了，训练的过程会预测所有的单词，比 BERT 更高效。</li>
<li>使用GAN的训练思路，对于一段文本，ELECTRA 使用了 MLM 对生成器进行训练，也是随机 [mask] 部分单词，然后用Generator预测的结果替换该单词；Discriminator的任务是预测每个位置的单词是来自于原文还是来自Generator的文本。该模型的主观思想是，生成mask的时候有些位置很好学，但是有些位置的东西很难学，那么模型更有机会学到一些很难的场景，能力更强。</li>
<li>因为由于这种对抗生成的方式不同于GAN可以梯度连续从Generator传到Discriminator，Electra的梯度不能从Generator到Discriminator，所以只能综合两者的损失值对Generator进行损失传递。ELECTRA 总体的损失函数由生成器的损失函数 LMLM 和判别器的损失函数 LDisc 组成。生成器的训练损失函数仍然是 MLM 损失函数，主要原因是生成器将单词进行替换，而单词是离散的，导致判别器到生成器的梯度中断了。具体来说是利用Generator loss对Generator进行传导，用Generator loss + Discriminator loss对Discriminator进行传导。</li>
</ul>
<p><strong><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">OpenAI-GPT</a></strong>、<strong><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">ELMo</a></strong>的架构和Bert几乎是一样的，都是输入之后用特征提取器提取特征，然后输出。</p>
<p>ELMo和OpenAI GPT的思想其实非常非常简单，就是用海量的无标注数据学习语言模型，在学习语言模型的过程中自然而然的就学到了上下文的语义关系。它们都是来学习一个语言模型，前者使用的是LSTM而后者使用Transformer，在进行下游任务处理的时候也有所不同，ELMo是把它当成特征。拿分类任务来说，输入一个句子，ELMo用LSTM把它扫一次，这样就可以得到每个词的表示，这个表示是考虑上下文的，因此”He deposited his money in this bank”和”His soldiers were arrayed along the river bank”中的两个bank的向量是不同的。下游任务用这些向量来做分类，它会增加一些网络层，但是ELMo语言模型的参数是固定的。而OpenAI GPT不同，它直接用特定任务来Fine-Tuning Transformer的参数。因为用特定任务的数据来调整Transformer的参数，这样它更可能学习到与这个任务特定的上下文语义关系，因此效果也更好。</p>
<p>差别是，GPT只用了前序序列没用后续序列（Bert发现了这点于是用了MLM或者是CBOW），ELMo用的是BiLSTM（Bert采用了更强的特征提取器Transformer）。Bert相对于其之前工作word2vec、GPT、ELMo的优势在于：</p>
<ul>
<li>相比于<strong>word2vec</strong>包含了语境信息；</li>
<li>相比于<strong>ELMo</strong>速度更快，并行程度更高，ELMo 使用独立训练的从左到右和从右到左的 LSTM 的连接来为下游任务生成特征；</li>
<li>相比于<strong>GPT</strong>包含了双向的语境信息。BERT Transformer 使用的是双向的自注意力，而 GPT Transformer 使用的是受限的自注意力，每个标记只能关注其左边的语境。</li>
<li>更多的训练语料，GPT 是在 BooksCorpus 上训练出来的然而 BERT 是在 BooksCorpus和 Wikipedia上训练出来的。GPT 仅在微调时使用<code>[SEP]</code>和<code>[CLS]</code> 而BERT 在预训练时使用 <code>[SEP]</code>， <code>[CLS]</code> 和 <code>Segment embedding</code>。</li>
</ul>
<p><img src="data:image/png;base64,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" data-original="2.png" alt=""></p>
<p>总体来说，基于Transformer的模型统治了NLP，主要原因有：①更大规模的预训练数据搭配更大的模型和更强大的算力②一些局部的小技巧（数据预处理、masking、训练任务等）③模型的压缩与蒸馏、加速与并行。缺点就是在少训练数据时容易过拟合。</p>
<p><img src="data:image/png;base64,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" data-original="summary.png" alt=""></p>
<h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><p>后续更新…</p>
<h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><h3><span id="bert本身">BERT本身</span></h3><ul>
<li><p>BERT 的基本原理是什么？（说说BERT的原理实现，输入，中间过程，输出？）答：见第一部分。BERT可以调用<code>get_pooled_ouput</code>获得全局的语义表征向量；<code>get_sequence_output</code>则是获得各个词的表征向量。</p>
</li>
<li><p>BERT的训练过程是怎么样的？是如何Mask的？两个任务分别的输入输出是什么？为什么要加这两个训练任务？80％等数值的含义？答：具体细节见第一部分。在训练BERT的时候，这两个任务是同时训练的。所以，BERT的损失函数是把这两个任务的损失函数加起来的，是一个多任务训练。</p>
</li>
<li><p>BERT 是怎么用 Transformer 的？（Transformer encoder模块包含了哪些？BERT和Transformer什么关系？BERT的位置编码和Transformer有什么不同）答：见第二部分。</p>
</li>
<li><p>输入的意义？</p>
<p>答：$([CLS],s_1,s_2…,s_m,[SEP],p_1,p_2,…,p_n,[SEP])$子句序列，用[SEP]分割。包含两个Level的信息，一个是句子先后信息（对应于后边的Next Sentence Prediction训练任务），另一个是每个单句内各个单词之间的先后信息（对应于后边的Masked Language Model训练任务）。</p>
</li>
<li><p>输入包括哪些Embedding，各自的作用是什么？如何拼接的？</p>
<p>答：<strong>Token Embedding</strong>对应于每个token的语义表示、<strong>Segment Embedding</strong>对应于句子的划分（如从<code>[CLS]</code>到第一个<code>[SEP]</code>全都是$E_A$，$p_1$到第二个<code>[SEP]</code>全都是$E_B$）、<strong>Position Embedding</strong>对应于每个位置的编码，不同的位置不同的编码，这样才能加入单词的先后顺序的影响。在预训练时三种embedding都是随机生成的（而在Transformer中position embedding是由 sin/cos 函数生成的固定的值）。<strong>三部分直接做加法</strong>是可以保留原有信息的。</p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/1.png" alt=""></p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/embedding.png" alt=""></p>
</li>
<li><p>句子的长度不一样怎么办（BERT的输入长度不定怎么处理）？</p>
<p>答：需要设置定长，原论文里面是512，如果过长就截断，如果过短就0padding+mask(0的位置为True，否则为False)。</p>
</li>
<li><p>BERT的优缺点？</p>
<p>答：优点的话主要有以下几个：</p>
<ul>
<li>Transformer Encoder因为有Self-attention机制，因此BERT自带双向功能；</li>
<li>BERT加入了Next Sentence Prediction来和Masked-LM一起做联合训练，可以获取比词更高级别的句子级别的语义表征，embedding的含义更加丰富；</li>
<li>为了适配多任务下的迁移学习，BERT设计了更通用的输入层和输出层；</li>
<li>引入大规模、高质量的文本数据，微调成本小。</li>
</ul>
<p>缺点XLNet 论文中有提到：</p>
<ul>
<li>BERT 的在预训练时会出现特殊的[MASK]，但是它在下游的 fine-tune 中不会出现，这就出现了预训练阶段和 fine-tune 阶段不一致的问题；</li>
<li>预训练过程和生成过程的不一致，这导致了BERT对生成式任务NLG处理效果不好（自回归语言模型Pre-train模式的原因）；</li>
<li>采取独立性假设：没有考虑预测[MASK]之间的相关性，是对语言模型联合概率的有偏估计。BERT 在分词后做[MASK]会产生的一个问题，为了解决 OOV 的问题，我们通常会把一个词切分成更细粒度的 WordPiece。BERT 在 Pretraining 的时候是随机 Mask 这些 WordPiece 的，这就可能出现只 Mask 一个词的一部分的情况。这样它只需要记住一些词(WordPiece 的序列)就可以完成这个任务，而不是根据上下文的语义关系来预测出来的。类似的中文的词”模型”也可能被 Mask 部分(其实用”琵琶”的例子可能更好，因为这两个字只能一起出现而不能单独出现)，这也会让预测变得容易。针对这点的改进是将词作为一个整体要么都 Mask 要么都不 Mask，这就是所谓的 Whole Word Masking。这是一个很简单的想法，对于 BERT 的代码修改也非常少，只是修改一些 Mask 的那段代码；</li>
<li>BERT对超长文本效果不理想，无法文档级别的NLP任务，只适合于句子和段落级别的任务（参考XLNet处理方法，state reuse）；</li>
<li>NSP任务优化（QA，Sentence Order）</li>
</ul>
</li>
<li><p>BERT擅长处理哪些下游NLP任务？</p>
<p>答：</p>
<ul>
<li>适合句子和段落级别的任务，不适用于文档级别的任务；</li>
<li>适合处理高层语义信息提取的任务，对浅层语义信息提取的任务的提升效果不大（如一些简单的文本分类任务）；</li>
<li>适合处理句子/段落的匹配任务；因此，在一些任务中可以构造辅助句（类似匹配任务）实现效果提升（如关系抽取/情感挖掘等任务）；</li>
<li>不适合处理NLG任务。</li>
</ul>
</li>
<li><p>BERT 模型为什么要用 mask？它是如何做 mask 的？（Mask会带来什么缺点吗？）</p>
<p>答：MLM任务典型的 Denosing Autoencoder 的思路，那些被 Mask 掉的单词就是<strong>在输入侧加入噪音</strong>，可以防止信息泄露。这种方法优点是它能比较自然地融入双向语言模型，同时看到被预测单词的上文和下文，然而缺点也很明显，主要在输入侧引入<code>[Mask]</code>标记，导致预训练阶段和 Fine-tuning 阶段不一致的问题。</p>
</li>
<li><p>BERT 的两个预训练任务对应的损失函数是什么(用公式形式展示)？</p>
<p>答：定义 $\theta$ 为BERT中Encoder部分的参数， $\theta_1$ 是 Mask-LM 任务中在 Encoder 上所接的输出层中的参数，  $\theta_2$ 则是句子预测任务中在 Encoder 接上的分类器参数。因此，在第一部分的损失函数中，如果被 mask 的词集合为 $M$，因为它是一个词典大小 $|V|$ 上的多分类问题，那么具体说来有：</p>
<script type="math/tex; mode=display">L_1(\theta,\theta_1) = -\displaystyle\sum_{i=1}^{M}\log{p(m=m_i|\theta,\theta_1), m_i \in [1,2,...,|V|]}</script><p>在句子预测任务中，也是一个分类问题的损失函数：</p>
<script type="math/tex; mode=display">L_2(\theta,\theta_2) = -\displaystyle\sum_{j=1}^{N}\log{p(n=n_j|\theta,\theta_2), n_j \in [IsNext, NotNext]}</script><p>最终的损失函数为两个任务联合学习的损失之和：</p>
<script type="math/tex; mode=display">L(\theta,\theta_1,\theta_2) = L_1(\theta,\theta_1) + L_2(\theta,\theta_2) = -\displaystyle\sum_{i=1}^{M}\log{p(m=m_i|\theta,\theta_1)}  -\displaystyle\sum_{j=1}^{N}\log{p(n=n_j|\theta,\theta_2)}</script></li>
<li><p>为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？</p>
<p>答：BERT采用的Masked LM，会选取语料中所有词的15%进行随机mask，论文中表示是受到完形填空任务的启发，但其实<strong>与CBOW也有异曲同工之妙</strong>。从CBOW的角度，这里$p=15\%$有一个比较好的解释是：在一个大小为$1/p = 100 /15 \approx 7$的窗口中随机选一个词，类似CBOW中滑动窗口的中心词，区别是这里的滑动窗口是非重叠的。</p>
</li>
<li><p>使用BERT预训练模型为什么最多只能输入512个词，最多只能两个句子合成一句？</p>
<p>答：这是Google BERT预训练模型初始设置的原因，前者对应Position Embeddings，后者对应Segment Embeddings。在BERT中，Token，Position，Segment Embeddings <strong>都是通过学习来得到的</strong>。所以在一开始训练的时候就按照512和2训练的，如果你能修改之后再重训那也是可以的。</p>
</li>
<li><p>为什么BERT在第一句前会加一个[CLS]标志?</p>
<p>答：BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。为什么选它呢，因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。</p>
</li>
<li><p>BERT非线性的来源在哪里？</p>
<p>答：前馈层的gelu激活函数和self-attention。</p>
</li>
<li><p>BERT的三个Embedding直接相加会对语义有影响吗？</p>
<p>答：因为三个embedding相加等价于三个原始one-hot的拼接再经过一个全连接网络。知乎上有个话题专门对这个问题进行了讨论。<a href="https://www.zhihu.com/question/374835153" target="_blank" rel="noopener">传送门</a></p>
</li>
<li><p>Transformer的点积模型做缩放的原因是什么？（BERT 为什么scale product？）</p>
<p>答：论文中的解释为，向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。至于具体该怎么解释，似乎需要牵扯到数学的证明。</p>
</li>
<li><p>Positional Embedding是怎么实现的？手写一个。（position embedding是什么，哪种效果好？）</p>
<p>答：见《<a href="https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/14/我想去面试系列——BERT源码品读</a>》。区别主要在于Transformer是用sin、cos算出来的固定值，而BERT是随机初始化去学习的参数。</p>
</li>
<li><p>BERT的position embedding为什么是通过学习出来的而不像transformer那样通过sinusoidal函数生成？（Transformer 和 BERT 的位置编码有什么区别？）</p>
<p>答：</p>
<ul>
<li>对于翻译任务，encoder的核⼼任务是提取完整的句⼦语义信息，无需特别关注某个词的具体位置。而BERT在做下游的序列标注类任务时需要确切的位置信息，模型需要给出每个位置的预测结果，因此BERT在预训练过程中需要建模完整的词序信息。还有一点transofmer语料相对不多。</li>
<li>在Transformer里，Positional与Token相加之前，要对Token做一个放大。可能的意义在于在最终的词向量表示中，Token代表的语义信息占据绝对主导，远远超过Position代表的位置信息。而bert中的Token 和Positional 都是trainable。并且三者相加时并没有对Token作放大。</li>
</ul>
</li>
<li><p>BERT里add&amp;norm是什么以及作用？BERT的残差网络在哪用到的？</p>
<p>答：可以使得模型更深，避免梯度爆炸和梯度消失的问题。在Transformer里的MultiHeadAttention和FFN后边都有用到。</p>
</li>
<li><p>为什么BERT可以做到很深，关键点在哪里？</p>
<p>答：网络结构：完全双向，层数深，可表征的函数空间足够大，self-attention 克服了长距离依赖问题，残差结构可以缓解过拟合；预训练任务设计好：无监督任务，除了LM任务之外， 加了句子级别的任务，有利于句子语义的表示和学习；训练语料大，训练充分。多层Transformer+巨大的数据量+同时利用左右词语信息是关键。</p>
</li>
<li><p>BERT对于输入长度不一致的文本预测所花的时间一样吗？</p>
<p>答：一样的，都是相当于统一处理成定长（这样才能统一放进一个batch里）。具体的实现可以利用<code>torch.nn.utils.rnn</code> 自带的 <code>pad_packed_sequence</code>和<code>pack_packed_sequence</code>分别对序列进行填充打包和解包。只不过训练的时候反向传播的时候乘以0给消除掉这些padding的影响使得它们不对梯度产生影响而已。</p>
</li>
<li><p>BERT的激活函数用了什么？（GELU）和RELU的区别 ？</p>
<p>答：GELU的核心思想就是，<strong>将非线性与随机正则化结合</strong>。具体的，将Input $x$ 乘以一个服从伯努利分布的$\Phi(x)$，而$\Phi(x)$又是依赖于输入Input $x$的。含义就是，$x$乘以一个它大于其他值的概率，也就是当输入值inpuits较小时，inputs被drop的可能性更大。GELU通过这种方式加mask，既保持了不确定性，又建立了与input的依赖关系。Gelu则在relu的基础上加入了统计的特性。</p>
<ul>
<li><p>GELU的表达式为$f(x)=xP(X \leq x) = x \Phi(x) = 0.5x(1+\tanh(\sqrt{2/{\pi}}(x+0.044715x^3)))$，RELU的表达式为$ f(x) = max(0,x) $。</p>
</li>
<li><p>ReLU：为Inputs乘以一个0或者1，所乘的值是确定的，是依据Sign(x)来确定的；GELU：也会为Inputs乘以0或者1，<strong>GELU所加的0-1mask的值是随机的，同时是依赖于inputs的分布的</strong>。</p>
<p>还有个说法是说RELU缺乏数据的统计特性，而GELU则在RELU的基础上加入了统计的特性。</p>
</li>
</ul>
</li>
<li><p>BERT的attention机制是什么？介绍self-attention，优点是什么？BERT的attention和普通的attention的区别？为什么还要self-attention后边加个FFN？</p>
<p>答：<a href="https://zhuanlan.zhihu.com/p/63928317" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/63928317</a>。加FFN是因为由于self-attention中的计算都是线性了，为了提高模型的非线性拟合能力，需要在其后接上前馈网络。</p>
</li>
<li><p>其实我们可以用加入位置嵌入的方式来改进这个无法并行的问题那为什么非得用bert呢？</p>
<p>答：确实之前基友pos+textcnn来实现并行训练的，但我认为BERT实际上是一个集大成者，很多技术比如说用位置嵌入、加attention等方法都是在它之前的文章里都用过了，只不过他这里提出了个预训练任务MLM来做预训练，然后用了之前很多技术的精髓集中起来，重点，集大成。</p>
</li>
<li><p>手写一个BERT里的Transformer？</p>
<p>答：分别写三个类，single_attention、MultiHeadAttention、TransformerBlock。</p>
</li>
<li><p>Bert的Multi-Head Attention原理和结构是什么？BERT为什么需要多头？如果是QA问题，你知道该如何调整encoder的层数吗？</p>
<p>答：</p>
<ul>
<li>MultiHead就是把n组$softmax(\frac{QK^T}{\sqrt{d_k}})V$的输出向量做拼接，每组随机初始化，能学到n种不同的attention组合。</li>
<li>多头的原因是单个attention获取到的信息有限，而我们需要提取多重意义的含义（比如在不同场景下表达不同意思）。其实就是相当于一个集成作用，和CNN使用多个通道卷积效果类似。论文中说到这样的好处是可以允许模型在不同的表示子空间里学习到相关的信息，后面还会根据attention可视化来验证。</li>
<li>QA问题应该大幅降低encoder层数，因为在BERT模型的应用中，我们一般取第12层的hidden向量用于下游任务。而低层向量基本上包含了基础信息，我们可以取低层的输出向量接到任务层，进行微调。（来源自文章<a href="https://cloud.tencent.com/developer/article/1542897" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1542897</a>）</li>
</ul>
</li>
<li><p>如果BERT中去掉self-attention层，还可以拿到词嵌入么，为什么？</p>
<p>答：去掉attention仍然有全连接层，还是可以拿到，类似于word2vec。</p>
</li>
<li><p>BERT的wordpiece原理是什么？对于BERT未登录词OOV是怎么处理的？</p>
<p>答：WordPiece字面理解是把word拆成piece一片一片。Subword粒度在word与character之间，避免细粒度过粗或者过细。</p>
<ul>
<li>WordPiece的一种主要的实现方式叫做BPE（Byte-Pair Encoding）双字节编码。BPE的过程可以理解为把一个单词再拆分，使得我们的此表会变得精简，并且寓意更加清晰。BPE的大概训练过程：首先将词分成一个一个的字符，然后在词的范围内统计字符对出现的次数，每次将次数最多的字符对保存起来，直到循环次数结束。比如将”loved”,”loving”,”loves”拆成”lov”,”ed”,”ing”,”es”几部分。<strong>BPE</strong>：找出频率最高相邻序列，并把它合并，依次循环。</li>
<li>但是<strong>wordpiece在中文中不是很适用</strong>。首先我们的中文不像英文或者其他欧洲的语言一样通过空格分开，我们是连续的。其次我们的中文一个字就是一个最小的单元，无法在拆分的更小了。在中文中一般的处理方式是两中，分词和分字。理论上分词要比分字好，因为分词更加细致，语义分的更加开。分字简单，效率高，词表也很小，常用字就3000左右。</li>
<li>对于OOV单词，数据归一化，数字替换为0，重复标点符号归一化等等；直接将新词映射为UNK；同义词表查询；拆分成字符。</li>
</ul>
</li>
<li><p>说一下BERT的分词是怎么做的，tokenizer的输出是什么？这种分词叫什么？</p>
<p>答：</p>
<ul>
<li>bert包括三个tokenizer：<code>FullTokenizer</code>，<code>BasicTokenizer</code>（转成 unicode、清理特殊字符、处理中文、空格分词、去除多余字符和标点分词、再次空格分词），<code>WordpieceTokenizer</code>（贪婪最长优先匹配，寻找最长token序列），其中<code>FullTokenizer</code>就是调用后面两个，调用<code>BasicTokenizer</code>得到初步的token列表，对于token再分别调用<code>WordpieceTokenizer</code>，产生更细的token。</li>
<li>bert构建的词典包括：FullTokenizer产生的token、训练使用的特殊字符，如<code>[PAD] [CLS]</code>等、预留的为使用的99个占位符，<code>[unused1]~[unused99]</code>，新增词使用这99个占位符，可以不破坏预训练模型的embedding。</li>
</ul>
</li>
<li><p>BERT全称是什么？为什么名字里有双向？</p>
<p>答：<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers。双向主要体现在self-attention，同时与上下文进行attention。</p>
</li>
<li><p>估计一下 BERT 的参数量？一层 BERT 大概有多少参数量？</p>
<p>答：以BERT base为例，参数量约有110M，隐藏层大小768，头12个，层数12层。</p>
<ul>
<li>word embedding参数：词向量包括三个部分的编码：词向量参数，位置向量参数，句子类型参数，Bert采用的<code>vocab_size=30522</code>，<code>hidden_size=768</code>，<code>max_position_embeddings=512</code>，<code>token_type_embeddings=2</code>。这就很显然了，embedding参数 = <code>(30522+512+2)*768</code>。</li>
<li>multi-heads参数：从transformer结构看，Q，K，V就是我们输入的三个句子词向量，从之前的词向量分析可知，输出向量大小为<code>len*768</code>。如果是self-attention，Q=K=V，如果是普通的attention，Q !=K=V。但是，不管用的是self-attention还是普通的attention，参数计算并不影响。因为在输入单头head时，对QKV的向量均进行了不同的线性变换，引入了三个参数，<code>W1</code>，<code>W2</code>，<code>W3</code>，其维度均为：<code>768*64</code>（这里的64是指$d_k=d_v=d_q=d_{model}/h$）。那么单head的参数：<code>768*768/12*3</code>。 而头的数量为<code>h=12</code>。multi-heads的参数：<code>768*768/12*3*12</code> 之后将12个头concat后又进行了线性变换，用到了参数<code>W_o</code>，大小为<code>768 * 768</code>。 那么最后multi-heads的参数：<code>768*768/12*3*12+768*768</code>。</li>
<li>FFN参数：公式$FFN(x)=\max(0,xW_1+b_1)W_2+b_2$，用到了两个参数<code>W1</code>和<code>W2</code>，BERT沿用了惯用的全连接层大小设置，即$4<em>d_{model}$，为3072，因此，<code>W1</code>，<code>W2</code>大小为`768 </em> 3072<code>，2个为</code> 2<em>768</em>3072`。</li>
<li>LayerNorm层参数：gamma和beta，出现在了三个地方，embedding处、multi-head处、FFN处。但是参数都很少，维度均为768，因此总参数为<code>768*2+768*2*2*12</code>。</li>
</ul>
</li>
<li><p>BERT哪些方面可以并行？</p>
<p>答：</p>
<ul>
<li>由于BERT使用了Transformer，所以在计算multi-head attention的时候是可以并行处理的，因为每一个head计算的时候是和其他head不相关的。</li>
<li>不同于RNN计算当前词的特征要依赖于前文计算，有时序这个概念，是按照时序计算的，而BERT的Transformer-encoder中的self-attention计算当前词的特征时候，没有时序这个概念，是同时利用上下文信息来计算的，一句话的token特征是通过矩阵并行‘瞬间’完成运算的，故self-attention也体现了并行。</li>
<li>除此之外计算FFN也是可以并行的。</li>
</ul>
</li>
<li><p>BERT为什么只用Transformer的Encoder而不用Decoder？</p>
<p>答：因为Decoder不能获取要预测的信息，获取到的是句子的部分信息（比如类似于Transformer Decoder的GPT，每个token只能关注左侧的语境），而Encoder里面有完整句子的信息（双向的attention）。</p>
</li>
<li><p>LayerNorm、BatchNorm的区别？为什么不用BN而是用LN？</p>
<p>答：作用都是为了使得数据独立同分布。</p>
<ul>
<li>BatchNorm：为每一个小batch计算每一层的平均值和方差（对小batchsize效果不好）。Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize = 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”；</li>
<li>LayerNorm：独立计算每一层每一个样本的均值和方差（主要对RNN作用明显）。它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。layer normalization 对所有的特征进行缩放，这显得很没道理，但是它在NLP中很适用；</li>
<li>如果我们将一批文本组成一个batch，那么BN的操作方向是，对每句话的第一个词进行操作。但语言文本的复杂性是很高的，任何一个词都有可能放在初始位置，且词序可能并不影响我们对句子的理解。而BN是针对每个位置进行缩放，这不符合NLP的规律。而LN则是针对一句话进行缩放的，且<strong>LN一般用在第三维度</strong>，如[batchsize, seq_len, dims]中的dims，一般为词向量的维度，或者是RNN的输出维度等等，这一维度各个特征的量纲应该相同。因此也不会遇到上面因为特征的量纲不同而导致的缩放问题。从LayerNorm的优点来看，它对于batch大小是健壮的，并且在样本级别而不是batch级别工作得更好。</li>
</ul>
</li>
<li><p>BERT中的warmup有什么作用？</p>
<p>答：warmup 的意思是需要在训练最初使用较小的学习率来启动，并很快切换到大学习率而后进行常见的 decay。这样做的原因主要如下：</p>
<ul>
<li>训练开始阶段，模型权重的改变是非常迅速的。如果初期学习率较大，会容易直接崩溃，训练震荡，或者是过拟合，使得后续需要多训练很多轮才能拉回来。</li>
<li>在训练的过程中，如果有mini-batch内的数据分布方差特别大，这就会导致模型学习剧烈波动，使其学得的权重很不稳定，这在训练初期最为明显，最后期较为缓解（所以我们要对数据进行scale也是这个道理）。所以warmup有助于减缓模型在初始阶段对mini-batch的提前过拟合现象，保持分布的平稳。</li>
</ul>
</li>
<li><p>BERT为什么不适合用于生成式模型？</p>
<p>答：BERT本质上是一种自编码的语言模型，Pre-train的时候使用到了上下文特征，而生成式任务属于finetune阶段的下游任务，生成式任务都是从左至右的，这就导致了Pre-train和finetune两阶段不一致，所以效果不太好。自回归模型在生成式任务上会优于自编码模型，因为其Pre-train和Fine-tune阶段是一致的。</p>
</li>
<li><p>源码中Attention后实际的流程是如何的？</p>
<p>答：Transform模块中，在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output；所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output。</p>
</li>
<li><p>基础细节：pooling的反向传播，卷积的反向传播，adam，self-attention计算？</p>
<p>答：略。</p>
</li>
</ul>
<h3><span id="bert变型">BERT变型</span></h3><ul>
<li><p>为什么 BERT 比 ELMo 效果好？（BERT相比起其他模型为什么有效？）</p>
<p>答：①LSTM 抽取特征的能力远弱于 Transformer；②拼接方式双向融合的特征融合能力偏弱(没有具体实验验证，只是推测)；③BERT 的训练数据以及模型参数均多于 ELMo；④预训练任务MLM中的mask机制起到了最大的改进作用。</p>
</li>
<li><p>ELMo 和 BERT 的区别是什么？（ELMo为什么不用attention？）</p>
<p>答：ELMo 模型是通过语言模型任务得到句子中单词的 embedding 表示，以此作为补充的新特征给下游任务使用。因为 ELMo给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”。而 BERT 模型是“基于 Fine-tuning 的模式”，这种做法和图像领域基于 Fine-tuning 的方式基本一致，下游任务需要将模型改造成 BERT 模型，才可利用 BERT 模型预训练好的参数。</p>
</li>
<li><p>为什么ELMo、CBOW这样的双向语言模型不用mask？</p>
<p>答：众所周知，在 Elmo 出来之前， bi-rnn、bi-lstm 这些双向模型已被广泛用在分类、序列标注等任务上，用来提取一个词的上下文特征，所以将这种结构用在语言模型上也是自然而然，只不过 label 从分类标签、序列标签等，变成了词本身。</p>
<p>但如 Jacob 所说， Elmo 严格意义上确实不太算他所认为的双向语言模型，因为 bi-rnn、bi-lstm 的『双向』，并不是像 bert 一样，将上下文用同一套模型参数进行编码，而是用两个分开的单向rnn去处理，只不过共享了输入，以及输出时进行拼接。</p>
<p>所以像 Elmo 这么设计的话，就不会出现 Bert 的『上帝视角』问题，因为用的两套模型参数，互不干涉，不会出现下一个词被提前编码到当前词所对应的模型的情形。</p>
<p>而在 CBOW 看来 Bert 的预测模式完全借鉴的它的——用一个词周围的词，去预测其本身。 所以从这个意义上讲， CBOW 也应该算是个『双向』语言模型吧。 但为何 CBOW 不用 mask 呢？ 想必看到这里大家应该都明白了， mask 的提出是为了应对 transformer 的全局 self-attention， 而非『双向』，而 CBOW 这种上古模型，只是简单的单层神经网络，根本不可能出现『see themselves』 的问题。</p>
</li>
<li><p>BERT的mask相比于CBOW有什么异同？</p>
<p>答：</p>
<ul>
<li>相同点：都是给定上下文去预测一个词，只不过他们的上下文不一样。CBOW 的核心思想是给定上下文，根据它的上文 Context-Before 和下文 Context-after 去预测 input word。而 BERT 的做法是给定一个句子，会随机 Mask 15%的词，然后让 BERT 来预测这些 Mask 的词。</li>
<li>不同点：①CBOW 中，每个单词都会成为 input word，而 BERT 如果这么做的话训练数据就太大了，所以它只训练了其词表只有<strong>30522个wordpiece</strong>词根/单词，其他的单词用word piece去组合，除此之外的没见过的就<code>[UNK]</code>。②对于输入数据部分，CBOW 中的输入数据只有待预测单词的上下文，而 BERT 的输入是带有[MASK] token 的“完整”句子，也就是说 BERT 在输入端将待预测的 input word 用[MASK] token 代替了。③通过 CBOW 模型训练后，每个单词的 word embedding 是唯一的，因此并不能很好的处理一词多义的问题，而 BERT 模型得到的 word embedding(token embedding)融合了上下文的信息，就算是同一个单词，在不同的上下文环境下，得到的 word embedding 是不一样的。</li>
</ul>
</li>
<li><p>词袋模型-&gt;word2vec-&gt;BERT的演化历史？</p>
<p>答：</p>
<ul>
<li>词袋模型(Bag-of-words model)是将一段文本（比如一个句子或是一个文档）用一个“装着这些词的袋子”来表示，这种表示方式不考虑文法以及词的顺序。<strong>「而在用词袋模型时，文档的向量表示直接将各词的词频向量表示加和」</strong>。通过上述描述，可以得出词袋模型的两个缺点：① 词向量化后，词与词之间是有权重大小关系的，不一定词出现的越多，权重越大；② 词与词之间是没有顺序关系的。</li>
<li>而 word2vec 是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射成一个低维稠密向量，通过求余弦的方式，可以判断两个词语之间的关系，word2vec 其底层主要采用基于 CBOW 和 Skip-Gram 算法的神经网络模型。相比于BagWords的改进在于：① 考虑了词与词之间的顺序，引入了上下文的信息；② 得到了词更加准确的表示，其表达的信息更为丰富。</li>
<li>BERT则是在CBOW的思想上前进了一小步。从准确率上说改进的话，BERT 利用更深的模型，以及海量的语料，得到的 embedding 表示，来做下游任务时的准确率是要比 word2vec 高不少的。实际上，这也离不开模型的“加码”以及数据的“巨大加码”。再从方法的意义角度来说，BERT 的重要意义在于给大量的 NLP 任务提供了一个泛化能力很强的预训练模型，而仅仅使用 word2vec 产生的词向量表示，不仅能够完成的任务比 BERT 少了很多，而且很多时候直接利用 word2vec 产生的词向量表示给下游任务提供信息，下游任务的表现不一定会很好，甚至会比较差。</li>
</ul>
</li>
<li><p>Pre-trained Model都了解哪些，BERT与GPT区别？</p>
<p>答：见第四部分。 GPT不是双向的，没有masking的概念；BERT在训练中加入了下一个句子预测任务，所以它也有 segment嵌入。但是二者都用了transformer作为特征提取器。</p>
</li>
<li><p>Albert，具体原理是什么？相对做了哪些改进？对计算量有影响么，为什么？</p>
<p>答：①矩阵分解(有助于减少参数数量)； ②权重共享(有助于减少参数数量并进行正则化)；③nsp任务改进SOP（句间连贯性损失）；④去掉了Dropout（v1有但是v2就没有了）。但是对计算量没有影响，对训练和推理时间也没影响，因为只是参数减少了，但是同一套参数会被多次使用，该算的还是得算。</p>
</li>
<li><p>Roberta为什么使用动态mask？Roberta和BERT在预训练时候有什么不同？</p>
<p>答：RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。这就叫做动态Masking。<strong>这可能是因为更换了数据被MASK的方式，提高了模型输入的数据的随机性，使得模型可以学习到更多的pattern。</strong></p>
<p>预训练的不同有三点：①更大的batch size和更多的训练数据；②去除了NSP更改为FULL-SENTENCES（每次输入连续的多个句子，直到最大长度512）；③对数据的动态mask。</p>
</li>
<li><p>BERT与LSTM的比较？优劣势分别是啥？</p>
<p>答：①BERT里的transformer可以处理更长的序列，但是LSTM会随着距离的增加信息衰减的越多；②BERT可以考虑双向的信息，但是单LSTM只能考虑单向的；③LSTM是序列模型，无法并行计算，但是BERT可以（利用positional embedding来考虑时序）。</p>
</li>
<li><p>BERT的变种们的具体区别？相对BERT，Albert，Roberta， Electra做了什么改进？（介绍一下其他的Masked Language Model？）</p>
<p>答：见第四部分。</p>
</li>
<li><p>XLNet和BERT的不同？为什么要取消NSP任务？自编码AE、自回归AR的概念和区别？</p>
<p>答：</p>
<ul>
<li>BERT是AE模型，XLNet的思路采用的是自回归语言模型，根据上文来预测下一个单词，但是在上文中添加了下文信息，这样就既解决了[mask]带来的两阶段不一致问题和无法同时引入上下文信息的问题（取名为Permutation Language Model，意思是重建序列顺序）。XLNet在输入阶段不改变序列顺序，但是会在transformer中利用attention mask去随机掩盖掉序列中的一些单词，并将未掩盖掉的单词作为预测单词的上文。详情见第四部分。</li>
<li><strong>AE语言模型</strong>的目的是<strong>从损坏的输入中重建原始数据</strong>（预测mask）。用一个神经网络把输入映射为一个低维特征（通常输入会加一些噪声），这就是编码部分，之后再尝试将这个特征进行还原。BERT就是自编码模型。优点是能够比较自然的融入双向语言模型；缺点是引入了噪声，存在Pre-train和Fine-tune两阶段不一致的问题。</li>
<li><strong>AR语言模型</strong>的目的则是<strong>根据上文内容预测下文，或者根据下文内容预测上文</strong>。优点是和下游NLP任务相关，比如生成式NLP任务，文本摘要，机器翻译等，实际生成内容时，就是从左到右的，这样Pre-train和finetune阶段任务一致；缺点是只能利用单向信息，不能同时利用双向信息融合。</li>
</ul>
</li>
<li><p>XLNet和BERT位置编码的不同？</p>
<p>答：BERT中的位置编码是绝对位置编码，XLNet中基本结构是transformer-XL，位置编码为相对位置编码。XLNet中使用正余弦函数相对位置编码。主要有三个改动：①将绝对位置编码改进成相对位置编码；②将原来Q、K中与<code>W</code>参数的乘积，替换成了两个可学习的参数；③将原来单独的<code>W</code>矩阵，改成了两个矩阵参数<code>embedding的参数</code>，<code>位置编码的参数</code>。</p>
</li>
<li><p>XLNet为何有效？</p>
<p>答：参考 <a href="https://zhuanlan.zhihu.com/p/110204573" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110204573</a> 作答。</p>
<ul>
<li><p>首先要明确的一点你是，XLNet是一种自回归模型，传统的自回归模型只能考虑单向信息，所以为了能够利用双向信息，XLNet引入了<strong>Permutation LM排列语言模型</strong>，非常巧妙地通过 AR 的单向方式来习得双向信息。具体实现方式是，通过随机取一句话的一种排列，然后将末尾一定量的词给“遮掩”（和 BERT 里的直接替换 “[MASK]” 有些不同）掉，最后用 AR 的方式来按照这种排列依次预测被“遮掩”掉的词。相当于随机排列，排列的顺序决定了当前位置能看到哪些上下文。<a href="https://zhuanlan.zhihu.com/p/110204573" target="_blank" rel="noopener">这篇文章</a>可以说讲的是很形象了。</p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/xlnet.jpg" alt=""></p>
</li>
<li><p>为了实现 Permutation 加上 AR 预测过程，首先我们会发现，XLNet 打乱了句子的顺序，这时在预测的时候 token 的位置信息会非常重要，同时在预测的时候也必须将 token 的内容信息遮掩起来 (否则输入包含了要预测的内容信息，模型就无法学到知识)。也就是说 XLNet 需要看到 token 的位置信息，但是又不能看到 token 的内容信息。所以说就要把位置信息和内容信息割裂开来。于是就有了<strong>Two-Stream双流注意力</strong>：</p>
<ul>
<li><p>Query Stream，对于每一个 token，其对应的 Query Stream 只包含了该 token 的位置信息，只能参考之前的历史信息以及当前要预测的位置信息。注意是 token 在原始句子的位置信息，不是重新排列的位置信息。用$g$表示，该流用于计算得到后续Self-attention的输入Q。</p>
</li>
<li><p>Content Stream，对于每一个 token，其对应的 Content Stream 包含了该 token 的内容信息，编码到当前时刻的所有内容。用$h$表示，该流用于计算得到后续Self-attention的输入K、V。</p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/two_stream_1.png" alt=""></p>
</li>
<li><p>具体如何计算可以参考 <a href="https://www.jianshu.com/p/2b5b368cbaa0" target="_blank" rel="noopener">https://www.jianshu.com/p/2b5b368cbaa0</a>。简而言之就是XLNet的每一层里，每个位置的query stream是以前一层的query stream作为Q、其他位置的content stream作为K和V；每个位置的content stream都是以content stream作为QKV。二者组合分裂开来，就形成了双流注意力机制。</p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/xlnet.png" alt=""></p>
</li>
</ul>
</li>
<li><p>融合Transformer-XL的优点，处理过长文本。引入<strong>相对位置编码</strong>和<strong>片段循环机制</strong>。</p>
</li>
</ul>
</li>
<li><p>BERT如何压缩？知识蒸馏是什么，有哪几种方式？</p>
<p>答：压缩方法：</p>
<ul>
<li><strong>低秩因式分解</strong>：该方法基本思想是将原始的大矩阵分解为两个或多个低秩矩阵的乘积。就模型压缩技术而言主要用于全连接层和卷积层。举例有Albert。</li>
<li><strong>量化</strong>：量化技术通过减少用于表示每个权重值的精度来压缩模型。例如模型使用float32标准定义参数的精度进行训练，然后我们可以使用量化技术选择float16，甚至int8表示参数的精度用于压缩模型。</li>
<li><strong>OP重建</strong>：合并底层操作，加速矩阵运算。</li>
<li><strong>剪枝</strong>：不改变模型结构，减小模型的维度、层数、头数等超参数，以减小模型量级。</li>
<li><strong>知识蒸馏</strong>：基于知识蒸馏的模型压缩的基本思想是将知识从大型的，经过预训练的教师模型转移到通常较小的学生模型中，常见的学生模型根据教师模型的输出以及分类标签进行训练。在使用模型压缩技术的Bert变体中以知识蒸馏为主要技术的论文众多，例如DistilBERT、TinyBERT、MobileBERT。</li>
</ul>
</li>
<li><p>BERT的几种掩码方式？例如wwm？</p>
<p>答：Ernie（掩码基本单元（中：字，英：word）、掩码短语、掩码实体）、SpanBERT（掩盖span tokens，用其他左右边界之外的词和待预测token的位置向量来预测token）、BERT_WWM（中文分词级别的掩码）等。更多分析见<a href="https://zhuanlan.zhihu.com/p/90813015" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/90813015</a>。</p>
</li>
<li><p>后续的工作是如何【改进生成任务】、【引入知识】、【引入多任务学习】、【改进mask】、【精细调参】的？</p>
<p>答：<a href="https://www.jiqizhixin.com/articles/2019-08-26-16" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-08-26-16</a></p>
</li>
</ul>
<h3><span id="场景问题">场景问题</span></h3><ul>
<li><p>BERT 应用于有空格丢失或者单词拼写错误等数据是否还是有效，以及改进方法？</p>
<p>答：</p>
<ul>
<li>对于空格丢失，按照常理推断可能会无效了，因为空格都没有的话，那么便成为了一长段文本，但是具体还是有待验证。而对于有空格丢失的数据要如何处理呢？一种方式是利用 Bi-LSTM + CRF 做分词处理，待其处理成正常文本之后，再将其输入 BERT 做下游任务。</li>
<li>对于单词拼写错误，少量的话应该影响不大，因为预训练的预料本身就有噪音；但是如果有比较多的话，就需要通过人工特征工程的方式，以中文中的同义词替换为例，将不同的错字/别字都替换成同样的词语，这样减少错别字带来的影响。例如花被、花珼、花呗、花呗、花钡均替换成花呗。</li>
</ul>
</li>
<li><p>BERT适用的场景有哪些？</p>
<p>答：</p>
<ul>
<li>如果NLP任务偏向在语言本身中就包含答案，而不特别依赖文本外的其它特征，往往应用Bert能够极大提升应用效果，典型的任务比如QA和阅读理解。如果其他非文本特征也非常关键，那BERT对这些特征无法处理。</li>
<li>相对单文本分类任务和序列标注等其它典型NLP任务，Bert特别适合用来解决判断句子关系的这类问题。一个原因是因为BERT的预训练任务中NSP使得它擅长挖掘句子间的关系；另一个原因是Self Attention机制自带句子A中单词和句子B中任意单词的Attention效果，而这种细粒度的匹配对于句子匹配类的任务尤其重要，所以Transformer的本质特性也决定了它特别适合解决这类任务。</li>
<li>与NLP任务对深层语义特征的需求程度有关。越是需要深层语义特征的任务，越适合利用Bert来解决。一些浅层特征工程型任务比如POS、Tagging、Tokenize，可能不太需要用到NER。这是由于Transformer层深比较深，更适合挖深度的语义。</li>
<li>Bert比较适合解决输入长度不太长的NLP任务，而输入比较长的任务，典型的比如文档级别的任务，Bert解决起来可能就不太好。主要原因在于：Transformer的self attention机制因为要对任意两个单词做attention计算，所以时间复杂度是n平方，n是输入的长度。如果输入长度比较长，Transformer的训练和推理速度掉得比较厉害，于是，这点约束了Bert的输入长度不能太长。</li>
</ul>
</li>
<li><p>在BERT应用中，如何解决长文本问题？</p>
<p>答：目前似乎有很多trick，花样很多，知乎上也有讨论。<a href="https://www.zhihu.com/question/327450789" target="_blank" rel="noopener">传送门</a></p>
<ul>
<li>经典方法：直接截断，截取前510个或后510个或前128+后382；</li>
<li>滑动窗口。Sliding Window即把文档分成有重叠的若干段，然后每一段都当作独立的文档送入BERT进行处理，最后再对于这些独立文档得到的结果进行整合；</li>
<li>EMNLP2019的<a href="https://arxiv.org/pdf/1908.08167.pdf" target="_blank" rel="noopener">这篇文章</a>给出的解决方案是global norm + passage rank + sliding window；</li>
<li>更加花哨一点，Arxiv上<a href="https://arxiv.org/pdf/2008.09093" target="_blank" rel="noopener">这篇文章</a>给的思路是长文本用sliding window切片，独立放进BERT得到很多<code>[ClS]</code>，然后再把所有的<code>[CLS]</code>进行融合（实验证明再套个Transformer效果最好），相当于transformer套transformer，里面的负责得到各个子句的表示，外面的负责融合所有子句；</li>
<li>曲线救国，换成没有长度限制的XLNet。</li>
</ul>
</li>
<li><p>BERT的finetune有什么技巧？</p>
<p>答：从网上道听途说看了一些：</p>
<ul>
<li>一般来说BERT的fine-tune epochs范围为[2, 3, 4]。</li>
<li>层数一般是第12层表达的含义最好。</li>
<li>BERT的fine-tune学习率：<strong>[5e-5, 3e-5, 2e-5]</strong> ，下接结构BiLSTM + CRF学习率：<strong>1e-4</strong>。</li>
<li>对长文本的处理，截断head+tail的方式最好。</li>
</ul>
</li>
<li><p>BERT结合KG怎么做？结合多模态怎么做？</p>
<p>答：结合KG的：清华的Ernie、北大的K-BERT；结合多模态的：VisualBERT、VL-BERT、ViLBERT等到。自行看论文去。</p>
</li>
<li><p>说说针对中文，BERT有什么可以改进的地方？怎么对预训练任务的改进？针对中文应该如何mask？</p>
<p>答：最初BERT对中文任务是以汉字为单位实现训练的。于是就产生了一个问题，既然是以汉字为单位训练的，其训练出的就是孤零零的汉字向量，而在现代汉语中，<strong>单个汉字是远不足以表达像词语或者短语那样丰富的语义的</strong>，这就造成了BERT在很多中文任务上表现并不理想的情况。改进方法就是<strong>使用词粒度</strong>。如百度Ernie，在masked训练阶段，将包含有某一个知识实体的汉字全都Mask掉，让模型来猜测这个词语，从而学习到知识实体。后来哈工大推出BERT-WWM，WWM的意思是Whole Word Masking，其实就是Ernie模型中的短语级别遮盖（phrase-level masking）训练。</p>
</li>
<li><p>给你一堆语料，你怎么训练一个类似于BERT这样的模型，但是又比BERT轻量级很多，准确率下降也不大，工业界可用的那种？</p>
<p>答：见模型压缩部分。</p>
</li>
<li><p>为什么BERT中输入数据的<code>[mask]</code>标记为什么不能直接留空或者直接输入原始数据，在self-attention的Q K V计算中，不与待预测的单词做Q K V交互计算？</p>
<p>答：乍一看，感觉这个idea确实有可能可行，而且也没有看到什么不合理之处，但是需要注意的是，这样做的话，需要每预测一个单词，就要计算一套Q、K、V。就算不每次都计算，那么保存每次得到的Q、K、V也需要耗费大量的空间。总而言之，这种做法确实可能也是可行，但是实际操作难度却很大，从计算量来说，就是预训练BERT模型的好几倍(至少)，而且要保存中间状态也并非易事。</p>
</li>
</ul>
<h2><span id="参考文献">参考文献</span></h2><ol>
<li>Pre-training of Deep Bidirectional Transformers for Language Understanding：<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></li>
<li>BERT 模型详解：<a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/bert-theory/</a></li>
<li>七月在线NLP褚博士：BERT模型深度修炼指南：<a href="http://www.julyedu.com/video/play/264/8448" target="_blank" rel="noopener">http://www.julyedu.com/video/play/264/8448</a></li>
<li>NewBeeNLP关于BERT，面试官们都怎么问：<a href="http://mp.weixin.qq.com/s?__biz=MzIxMzkwNjM2NQ==&amp;mid=2247484380&amp;idx=1&amp;sn=55fccbb2565520bf747fa35359271673&amp;chksm=97aee50ea0d96c180ccf9177c653e78097ee7a9f18a3fb02811c128548cb585b427887d0dc8f&amp;mpshare=1&amp;scene=23&amp;srcid=1010Pr5bZpA0VDQksaXrjOif&amp;sharer_sharetime=1602343574372&amp;sharer_shareid=ac5afd09812a03f906a0b77a8c423be9#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com</a></li>
<li>一文读懂BERT（原理篇）：<a href="https://blog.csdn.net/jiaowoshouzi/article/details/89073944" target="_blank" rel="noopener">https://blog.csdn.net/jiaowoshouzi/article/details/89073944</a></li>
<li>The Illustrated Transformer：<a href="http://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">http://jalammar.github.io/illustrated-transformer/</a></li>
<li>A Visual Guide to Using BERT for the First Time：<a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" target="_blank" rel="noopener">http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a></li>
<li>XLNet详解：<a href="https://zhuanlan.zhihu.com/p/110204573" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/110204573</a></li>
<li>XLNet详解：<a href="https://www.jianshu.com/p/2b5b368cbaa0" target="_blank" rel="noopener">https://www.jianshu.com/p/2b5b368cbaa0</a></li>
<li>超细节的BERT/Transformer知识点：<a href="https://zhuanlan.zhihu.com/p/132554155" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/132554155</a></li>
<li>一文揭开ALBERT的神秘面纱：<a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">https://blog.csdn.net/u012526436/article/details/101924049</a></li>
<li>BERT知识点总结：<a href="https://blog.csdn.net/XiangJiaoJun_/article/details/107129808" target="_blank" rel="noopener">https://blog.csdn.net/XiangJiaoJun_/article/details/107129808</a></li>
<li>后BERT时代：15个预训练模型对比分析与关键点探索：<a href="https://www.jiqizhixin.com/articles/2019-08-26-16" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-08-26-16</a></li>
<li>BERT模型详解：<a href="http://fancyerii.github.io/2019/03/09/bert-theory/" target="_blank" rel="noopener">http://fancyerii.github.io/2019/03/09/bert-theory/</a></li>
</ol>
<blockquote>
<p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p>
<p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p>
<p>原文链接：<a href="https://vodkazy.cn/2020/10/10/我想去面试系列——BERT" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/10/我想去面试系列——BERT</a></p>
</blockquote>

        </div>
      </article>
    </div>

	<!-- 打赏 -->
    <div class="reward">
	<div class="reward-button">赏 <span class="reward-code">
		<span class="alipay-code"> <img class="alipay-img wdp-appear" src="data:image/png;base64,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" data-original="/images/alipay.webp"><b>支付宝打赏</b> </span> 
		<span class="wechat-code"> <img class="wechat-img wdp-appear" src="data:image/png;base64,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" data-original="/images/weixin.webp"><b>微信打赏</b> </span> </span>
	</div>
	<p class="reward-notice">如果文章对你有帮助，欢迎点击上方按钮打赏作者，更多文章请访问<a href="https://vodkazy.cn" style="color:blue">想飞的小菜鸡</a></p>
	    <style>
		*,*:before,*:after {
			-webkit-box-sizing: border-box;
			-moz-box-sizing: border-box;
			-ms-box-sizing: border-box;
			box-sizing: border-box
		}

		.reward {
			padding: 5px 0
		}

		.reward .reward-notice {
			font-size: 14px;
			line-height: 14px;
			margin: 15px auto;
			text-align: center
		}

		.reward .reward-button {
			font-size: 28px;
			line-height: 58px;
			position: relative;
			display: block;
			width: 60px;
			height: 60px;
			margin: 0 auto;
			padding: 0;
			-webkit-user-select: none;
			text-align: center;
			vertical-align: middle;
			color: #fff;
			border: 1px solid #f1b60e;
			border-radius: 50%;
			background: #fccd60;
			background: -webkit-gradient(linear,left top,left bottom,color-stop(0,#fccd60),color-stop(100%,#fbae12),color-stop(100%,#2989d8),color-stop(100%,#207cca));
			background: -webkit-linear-gradient(top,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%);
			background: linear-gradient(to bottom,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%)
		}

		.reward .reward-code {
			position: absolute;
			top: -220px;
			left: 50%;
			display: none;
			width: 350px;
			height: 200px;
			margin-left: -175px;
			padding: 15px;
			border: 1px solid #e6e6e6;
			background: #fff;
			box-shadow: 0 1px 1px 1px #efefef
		}

		.reward .reward-button:hover .reward-code {
			display: block
		}

		.reward .reward-code span {
			display: inline-block;
			width: 150px;
			height: 150px
		}

		.reward .reward-code span.alipay-code {
			float: left
		}

		.reward .reward-code span.alipay-code a {
			padding: 0
		}

		.reward .reward-code span.wechat-code {
			float: right
		}

		.reward .reward-code img {
			display: inline-block;
			float: left;
			width: 150px;
			height: 150px;
			margin: 0 auto;
			border: 0
		}

		.reward .reward-code b {
			font-size: 14px;
			line-height: 26px;
			display: block;
			margin: 0;
			text-align: center;
			color: #666
		}

		.reward .reward-code b.notice {
			line-height: 2rem;
			margin-top: -1rem;
			color: #999
		}

		.reward .reward-code:after,.reward .reward-code:before {
			position: absolute;
			content: '';
			border: 10px solid transparent
		}

		.reward .reward-code:after {
			bottom: -19px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #fff
		}

		.reward .reward-code:before {
			bottom: -20px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #e6e6e6
		}
    </style>


    <!-- Pre or Next -->
    
	<div class="container">
           <ul class="pager">
    	     
      	     <li class="previous">
              <a href="/2020/10/14/我想去面试系列——BERT源码品读/" rel="prev">上一篇</a>
             </li>
           
           
              <li class="next">
              <a href="/2020/10/06/《百面机器学习》阅读笔记/" rel="prev">下一篇</a>
            </li>
           
          </ul>
       </div>
   

    <!-- Valine无后端评论系统 -->   
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
		    el: '#vcomments' ,
		    appId: 'lH3VkMCd4MHaKtr2n2SRWdoi-MdYXbMMI',
		    appKey: '5aMXSY7b4KwnzfgpzLA0hPLv',
		    notify:true, 
		    verify:false, 
		    placeholder: '填写正确的邮箱和昵称才能收到我的回复哦       ٩( ^o^ )و  ' ,
		    avatar: 'retro'
		});
    </script>
    <!-- Valine无后端评论系统 -->  

  </div>
</div>
</div>

  <!-- Footer -->
  <!-- Footer -->
<footer class="site-info">
  <p>
    <span>想飞的小菜鸡 &copy; 2021</span>
    
      <span class="split">|</span>
      <span>照耀的Blog</span>
    
  </p>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
    本站访客数<span id="busuanzi_value_site_uv"></span>人次
    本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</footer>

  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>


 
  <!-- 使用 aotuload.js 引入看板娘 -->    
  <!-- //<script src="/js/assets/jquery.min.js?v=3.3.1"></script> -->   
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <!-- //<script src="/js/assets/jquery-ui.min.js?v=1.12.1"></script>   --> 
  <script src="https://cdn.jsdelivr.net/npm/jquery-ui-dist@1.12.1/jquery-ui.min.js"></script>
  <script src="/js/assets/autoload.js?v=1.4.2"></script>
  <!-- //<script src="https://live2d-cdn.fghrsh.net/assets/1.4.2/autoload.js></script> -->   
   


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],e=void 0,0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
