<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

	<!-- 百度统计 -->
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "https://hm.baidu.com/hm.js?e31627579358722b9d300535c8206351";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Vodkazy">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="模型训练与测试常用模板代码">
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="想飞的小菜鸡">
  <!--Type page-->
  
      <meta property="og:type" content="article">
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- Title -->
  
  <title>模型训练与测试常用模板代码 - 想飞的小菜鸡</title>


  <link rel="shortcut icon" href="/../images/icon.ico">
  <!--font-awesome-->
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body>

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="data:image/png;base64,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" data-original="https://cdn2.iconfinder.com/data/icons/weather-color-2/500/weather-01-128.png" alt="想飞的小菜鸡" height="60">
        
      </a>
    </div>
    <a class="header-name" href="/">
            <span>想飞的小菜鸡</span>
            的小窝
        </a>
    <!-- navbar -->
    <nav class="navbar">
      <!--  nav links -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  <i class="fa fa-home "></i>
                
                首页
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  <i class="fa fa-archive "></i>
                
                目录
              </a>
            </li>
          
            <li>
              <a href="/project">
                
                  <i class="fa fa-folder-open "></i>
                
                代码库
              </a>
            </li>
          
            <li>
              <a href="/photo">
                
                  <i class="fa fa-photo "></i>
                
                相册薄
              </a>
            </li>
          
            <li>
              <a href="/lovetree">
                
                  <i class="fa fa-tree "></i>
                
                爱情树
              </a>
            </li>
          
            <li>
              <a href="/guestbook">
                
                  <i class="fa fa-edit "></i>
                
                留言板
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  <i class="fa fa-user "></i>
                
                关于我
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            模型训练与测试常用模板代码
            
          </h1>
         
        </div>
         <p class="a-posted-on">
          2021-05-06
          </p>
        <!-- Post Main Content -->
        <div class="entry-content">
          <p>无意中发现了一份模型训练的模板，觉得很不错，刻意码下来备用。<br><a id="more"></a></p>
<p>原文转自：<a href="https://cloud.tencent.com/developer/article/1686256" target="_blank" rel="noopener">【colab pytorch】训练和测试常用模板代码</a></p>
<p><strong>1、分类模型训练代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss and optimizer</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">total_step = len(train_loader)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i ,(images, labels) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward pass</span></span><br><span class="line">        outputs = model(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward and optimizer</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;'</span></span><br><span class="line">                  .format(epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, total_step, loss.item()))</span><br></pre></td></tr></table></figure>
<p><strong>2、分类模型测试代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test the model</span></span><br><span class="line">model.eval()  <span class="comment"># eval mode(batch norm uses moving mean/variance </span></span><br><span class="line">              <span class="comment">#instead of mini-batch mean/variance)</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        outputs = model(images)</span><br><span class="line">        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Test accuracy of the model on the 10000 test images: &#123;&#125; %'</span></span><br><span class="line">          .format(<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<p><strong>3、自定义损失函数</strong></p>
<p>继承torch.nn.Module类写自己的loss。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLoss</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        loss = torch.mean((x - y) ** <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p><strong>4、标签平滑</strong></p>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, e=<span class="number">0.1</span>, reduction=<span class="string">'mean'</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">        self.e = e</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one_hot</span><span class="params">(self, labels, classes, value=<span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">            Convert labels to one hot vectors</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            labels: torch tensor in format [label1, label2, label3, ...]</span></span><br><span class="line"><span class="string">            classes: int, number of classes</span></span><br><span class="line"><span class="string">            value: label value in one hot vector, default to 1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            return one hot format labels in shape [batchsize, classes]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        one_hot = torch.zeros(labels.size(<span class="number">0</span>), classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#labels and value_added  size must match</span></span><br><span class="line">        labels = labels.view(labels.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        value_added = torch.Tensor(labels.size(<span class="number">0</span>), <span class="number">1</span>).fill_(value)</span><br><span class="line"></span><br><span class="line">        value_added = value_added.to(labels.device)</span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line"></span><br><span class="line">        one_hot.scatter_add_(<span class="number">1</span>, labels, value_added)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span><span class="params">(self, target, length, smooth_factor)</span>:</span></span><br><span class="line">        <span class="string">"""convert targets to one-hot format, and smooth</span></span><br><span class="line"><span class="string">        them.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            target: target in form with [label1, label2, label_batchsize]</span></span><br><span class="line"><span class="string">            length: length of one-hot format(number of classes)</span></span><br><span class="line"><span class="string">            smooth_factor: smooth factor for label smooth</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            smoothed labels in one hot format</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        one_hot = self._one_hot(target, length, value=<span class="number">1</span> - smooth_factor)</span><br><span class="line">        one_hot += smooth_factor / (length - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> one_hot.to(target.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.size(<span class="number">0</span>) != target.size(<span class="number">0</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)'</span></span><br><span class="line">                    .format(x.size(<span class="number">0</span>), target.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Expected input tensor to have least 2 dimensions(got &#123;&#125;)'</span></span><br><span class="line">                    .format(x.size(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> x.dim() != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Only 2 dimension tensor are implemented, (got &#123;&#125;)'</span></span><br><span class="line">                    .format(x.size()))</span><br><span class="line"></span><br><span class="line">        smoothed_target = self._smooth_label(target, x.size(<span class="number">1</span>), self.e)</span><br><span class="line">        x = self.log_softmax(x)</span><br><span class="line">        loss = torch.sum(- x * smoothed_target, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">'none'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.sum(loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> torch.mean(loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'unrecognized option, expect reduction to be one of none, mean, sum'</span>)</span><br></pre></td></tr></table></figure>
<p>或者直接在训练文件里做label smoothing</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line">    N = labels.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># C is the number of classes.</span></span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=<span class="number">0.1</span> / (C - <span class="number">1</span>)).cuda()</span><br><span class="line">    smoothed_labels.scatter_(dim=<span class="number">1</span>, index=torch.unsqueeze(labels, dim=<span class="number">1</span>), value=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    score = model(images)</span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=<span class="number">1</span>)</span><br><span class="line">    loss = -torch.sum(log_prob * smoothed_labels) / N</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>5、mixup训练</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line"><span class="keyword">for</span> images, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">    images, labels = images.cuda(), labels.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup images and labels.</span></span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()</span><br><span class="line">    index = torch.randperm(images.size(<span class="number">0</span>)).cuda()</span><br><span class="line">    mixed_images = lambda_ * images + (<span class="number">1</span> - lambda_) * images[index, :]</span><br><span class="line">    label_a, label_b = labels, labels[index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mixup loss.</span></span><br><span class="line">    scores = model(mixed_images)</span><br><span class="line">    loss = (lambda_ * loss_function(scores, label_a)</span><br><span class="line">            + (<span class="number">1</span> - lambda_) * loss_function(scores, label_b))</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p><strong>6、L1正则化</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=<span class="string">'sum'</span>)</span><br><span class="line">loss = ...  <span class="comment"># Standard cross-entropy loss</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
<p><strong>7、不对偏置进行权重衰减</strong></p>
<p>pytorch里的weight decay相当于l2正则</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] == <span class="string">'bias'</span>)</span><br><span class="line">others_list = (param <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> name[<span class="number">-4</span>:] != <span class="string">'bias'</span>)</span><br><span class="line">parameters = [&#123;<span class="string">'parameters'</span>: bias_list, <span class="string">'weight_decay'</span>: <span class="number">0</span>&#125;,                </span><br><span class="line">              &#123;<span class="string">'parameters'</span>: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>
<p><strong>8、梯度裁剪</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<p><strong>9、得到当前学习率</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If there is one global learning rate (which is the common case).</span></span><br><span class="line">lr = next(iter(optimizer.param_groups))[<span class="string">'lr'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># If there are multiple learning rates for different layers.</span></span><br><span class="line">all_lr = []</span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    all_lr.append(param_group[<span class="string">'lr'</span>])</span><br></pre></td></tr></table></figure>
<p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p>
<p><strong>10、学习率衰减</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reduce learning rate when validation accuarcy plateau.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="string">'max'</span>, patience=<span class="number">5</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cosine annealing learning rate.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=<span class="number">80</span>)</span><br><span class="line"><span class="comment"># Reduce learning rate by 10 at given epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[<span class="number">50</span>, <span class="number">70</span>], gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">80</span>):</span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning rate warmup by 10 epochs.</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=<span class="keyword">lambda</span> t: t / <span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">10</span>):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    train(...)</span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure>
<p><strong>11、优化器链式更新</strong></p>
<p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> ExponentialLR, StepLR</span><br><span class="line">model = [torch.nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>))]</span><br><span class="line">optimizer = SGD(model, <span class="number">0.1</span>)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=<span class="number">0.9</span>)</span><br><span class="line">scheduler2 = StepLR(optimizer, step_size=<span class="number">3</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    print(epoch, scheduler2.get_last_lr()[<span class="number">0</span>])</span><br><span class="line">    optimizer.step()</span><br><span class="line">    scheduler1.step()</span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure>
<p><strong>12、模型训练可视化</strong></p>
<p>pip install tensorboard</p>
<p>tensorboard —logdir=runs</p>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n_iter <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(<span class="string">'Loss/train'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Loss/test'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Accuracy/train'</span>, np.random.random(), n_iter)</span><br><span class="line">    writer.add_scalar(<span class="string">'Accuracy/test'</span>, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure>
<p><strong>13、保存和加载断点</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">tart_epoch = <span class="number">0</span></span><br><span class="line"><span class="comment"># Load checkpoint.</span></span><br><span class="line"><span class="keyword">if</span> resume: <span class="comment"># resume为参数，第一次训练时设为0，中断再训练时设为1</span></span><br><span class="line">    model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'best_checkpoint.pth.tar'</span>)</span><br><span class="line">    <span class="keyword">assert</span> os.path.isfile(model_path)</span><br><span class="line">    checkpoint = torch.load(model_path)</span><br><span class="line">    best_acc = checkpoint[<span class="string">'best_acc'</span>]</span><br><span class="line">    start_epoch = checkpoint[<span class="string">'epoch'</span>]</span><br><span class="line">    model.load_state_dict(checkpoint[<span class="string">'model'</span>])</span><br><span class="line">    optimizer.load_state_dict(checkpoint[<span class="string">'optimizer'</span>])</span><br><span class="line">    print(<span class="string">'Load checkpoint at epoch &#123;&#125;.'</span>.format(start_epoch))</span><br><span class="line">    print(<span class="string">'Best accuracy so far &#123;&#125;.'</span>.format(best_acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, num_epochs): </span><br><span class="line">    ... </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Test the model</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    <span class="comment"># save checkpoint</span></span><br><span class="line">    is_best = current_acc &gt; best_acc</span><br><span class="line">    best_acc = max(current_acc, best_acc)</span><br><span class="line">    checkpoint = &#123;</span><br><span class="line">        <span class="string">'best_acc'</span>: best_acc,</span><br><span class="line">        <span class="string">'epoch'</span>: epoch + <span class="number">1</span>,</span><br><span class="line">        <span class="string">'model'</span>: model.state_dict(),</span><br><span class="line">        <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'checkpoint.pth.tar'</span>)</span><br><span class="line">    best_model_path = os.path.join(<span class="string">'model'</span>, <span class="string">'best_checkpoint.pth.tar'</span>)</span><br><span class="line">    torch.save(checkpoint, model_path)</span><br><span class="line">    <span class="keyword">if</span> is_best:</span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure>
<p><strong>14、提取Imagenet预训练模型某层的特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VGG-16 relu5-3 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>).features[:<span class="number">-1</span>]</span><br><span class="line"><span class="comment"># VGG-16 pool5 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>).features</span><br><span class="line"><span class="comment"># VGG-16 fc7 feature.</span></span><br><span class="line">model = torchvision.models.vgg16(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:<span class="number">-3</span>])</span><br><span class="line"><span class="comment"># ResNet GAP feature.</span></span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">model = torch.nn.Sequential(collections.OrderedDict(</span><br><span class="line">    list(model.named_children())[:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    model.eval()</span><br><span class="line">    conv_representation = model(image)</span><br></pre></td></tr></table></figure>
<p><strong>15、提取imagenet预训练模型多层卷积特征</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeatureExtractor</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Helper class to extract several convolution features from the given</span></span><br><span class="line"><span class="string">    pre-trained model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes:</span></span><br><span class="line"><span class="string">        _model, torch.nn.Module.</span></span><br><span class="line"><span class="string">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(</span></span><br><span class="line"><span class="string">                list(model.named_children())[:-1]))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; conv_representation = FeatureExtractor(</span></span><br><span class="line"><span class="string">                pretrained_model=model,</span></span><br><span class="line"><span class="string">                layers_to_extract=&#123;'layer1', 'layer2', 'layer3', 'layer4'&#125;)(image)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, pretrained_model, layers_to_extract)</span>:</span></span><br><span class="line">        torch.nn.Module.__init__(self)</span><br><span class="line">        self._model = pretrained_model</span><br><span class="line">        self._model.eval()</span><br><span class="line">        self._layers_to_extract = set(layers_to_extract)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            conv_representation = []</span><br><span class="line">            <span class="keyword">for</span> name, layer <span class="keyword">in</span> self._model.named_children():</span><br><span class="line">                x = layer(x)</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">in</span> self._layers_to_extract:</span><br><span class="line">                    conv_representation.append(x)</span><br><span class="line">            <span class="keyword">return</span> conv_representation</span><br></pre></td></tr></table></figure>
<p><strong>16、微调全连接层</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="keyword">False</span></span><br><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">100</span>)  <span class="comment"># Replace the last fc layer</span></span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span></span><br></pre></td></tr></table></figure>
<p><strong>17、以较大学习率微调全连接层，较小学习率微调卷积层</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=<span class="keyword">True</span>)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> id(p) <span class="keyword">not</span> <span class="keyword">in</span> finetuned_parameters)</span><br><span class="line">parameters = [&#123;<span class="string">'params'</span>: conv_parameters, <span class="string">'lr'</span>: <span class="number">1e-3</span>&#125;, </span><br><span class="line">              &#123;<span class="string">'params'</span>: model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p>
<p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p>
<p>原文链接：<a href="https://vodkazy.cn/2021/05/06/模型训练与测试常用模板代码" target="_blank" rel="noopener">https://vodkazy.cn/2021/05/06/模型训练与测试常用模板代码</a></p>
</blockquote>

        </div>
      </article>
    </div>

	<!-- 打赏 -->
    <div class="reward">
	<div class="reward-button">赏 <span class="reward-code">
		<span class="alipay-code"> <img class="alipay-img wdp-appear" src="data:image/png;base64,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" data-original="/images/alipay.webp"><b>支付宝打赏</b> </span> 
		<span class="wechat-code"> <img class="wechat-img wdp-appear" src="data:image/png;base64,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" data-original="/images/weixin.webp"><b>微信打赏</b> </span> </span>
	</div>
	<p class="reward-notice">如果文章对你有帮助，欢迎点击上方按钮打赏作者，更多文章请访问<a href="https://vodkazy.cn" style="color:blue">想飞的小菜鸡</a></p>
	    <style>
		*,*:before,*:after {
			-webkit-box-sizing: border-box;
			-moz-box-sizing: border-box;
			-ms-box-sizing: border-box;
			box-sizing: border-box
		}

		.reward {
			padding: 5px 0
		}

		.reward .reward-notice {
			font-size: 14px;
			line-height: 14px;
			margin: 15px auto;
			text-align: center
		}

		.reward .reward-button {
			font-size: 28px;
			line-height: 58px;
			position: relative;
			display: block;
			width: 60px;
			height: 60px;
			margin: 0 auto;
			padding: 0;
			-webkit-user-select: none;
			text-align: center;
			vertical-align: middle;
			color: #fff;
			border: 1px solid #f1b60e;
			border-radius: 50%;
			background: #fccd60;
			background: -webkit-gradient(linear,left top,left bottom,color-stop(0,#fccd60),color-stop(100%,#fbae12),color-stop(100%,#2989d8),color-stop(100%,#207cca));
			background: -webkit-linear-gradient(top,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%);
			background: linear-gradient(to bottom,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%)
		}

		.reward .reward-code {
			position: absolute;
			top: -220px;
			left: 50%;
			display: none;
			width: 350px;
			height: 200px;
			margin-left: -175px;
			padding: 15px;
			border: 1px solid #e6e6e6;
			background: #fff;
			box-shadow: 0 1px 1px 1px #efefef
		}

		.reward .reward-button:hover .reward-code {
			display: block
		}

		.reward .reward-code span {
			display: inline-block;
			width: 150px;
			height: 150px
		}

		.reward .reward-code span.alipay-code {
			float: left
		}

		.reward .reward-code span.alipay-code a {
			padding: 0
		}

		.reward .reward-code span.wechat-code {
			float: right
		}

		.reward .reward-code img {
			display: inline-block;
			float: left;
			width: 150px;
			height: 150px;
			margin: 0 auto;
			border: 0
		}

		.reward .reward-code b {
			font-size: 14px;
			line-height: 26px;
			display: block;
			margin: 0;
			text-align: center;
			color: #666
		}

		.reward .reward-code b.notice {
			line-height: 2rem;
			margin-top: -1rem;
			color: #999
		}

		.reward .reward-code:after,.reward .reward-code:before {
			position: absolute;
			content: '';
			border: 10px solid transparent
		}

		.reward .reward-code:after {
			bottom: -19px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #fff
		}

		.reward .reward-code:before {
			bottom: -20px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #e6e6e6
		}
    </style>


    <!-- Pre or Next -->
    
	<div class="container">
           <ul class="pager">
    	     
           
              <li class="next">
              <a href="/2021/01/04/关于SQL的一些非SELECT知识点/" rel="prev">下一篇</a>
            </li>
           
          </ul>
       </div>
   

    <!-- Valine无后端评论系统 -->   
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
		    el: '#vcomments' ,
		    appId: 'lH3VkMCd4MHaKtr2n2SRWdoi-MdYXbMMI',
		    appKey: '5aMXSY7b4KwnzfgpzLA0hPLv',
		    notify:true, 
		    verify:false, 
		    placeholder: '填写正确的邮箱和昵称才能收到我的回复哦       ٩( ^o^ )و  ' ,
		    avatar: 'retro'
		});
    </script>
    <!-- Valine无后端评论系统 -->  

  </div>
</div>
</div>

  <!-- Footer -->
  <!-- Footer -->
<footer class="site-info">
  <p>
    <span>想飞的小菜鸡 &copy; 2021</span>
    
      <span class="split">|</span>
      <span>照耀的Blog</span>
    
  </p>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
    本站访客数<span id="busuanzi_value_site_uv"></span>人次
    本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</footer>

  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>


 
  <!-- 使用 aotuload.js 引入看板娘 -->    
  <!-- //<script src="/js/assets/jquery.min.js?v=3.3.1"></script> -->   
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <!-- //<script src="/js/assets/jquery-ui.min.js?v=1.12.1"></script>   --> 
  <script src="https://cdn.jsdelivr.net/npm/jquery-ui-dist@1.12.1/jquery-ui.min.js"></script>
  <script src="/js/assets/autoload.js?v=1.4.2"></script>
  <!-- //<script src="https://live2d-cdn.fghrsh.net/assets/1.4.2/autoload.js></script> --><!-- hexo-inject:begin --><!-- hexo-inject:end -->   
   


<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],e=void 0,0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
