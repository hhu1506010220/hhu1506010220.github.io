<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

	<!-- 百度统计 -->
	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "https://hm.baidu.com/hm.js?e31627579358722b9d300535c8206351";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Vodkazy">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="我想去面试系列——Word2vec">
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="想飞的小菜鸡">
  <!--Type page-->
  
      <meta property="og:type" content="article">
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- Title -->
  
  <title>我想去面试系列——Word2vec - 想飞的小菜鸡</title>


  <link rel="shortcut icon" href="/../images/icon.ico">
  <!--font-awesome-->
  <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css">
  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>


<body>

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="data:image/png;base64,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" data-original="https://cdn2.iconfinder.com/data/icons/weather-color-2/500/weather-01-128.png" alt="想飞的小菜鸡" height="60">
        
      </a>
    </div>
    <a class="header-name" href="/">
            <span>想飞的小菜鸡</span>
            的小窝
        </a>
    <!-- navbar -->
    <nav class="navbar">
      <!--  nav links -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  <i class="fa fa-home "></i>
                
                首页
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  <i class="fa fa-archive "></i>
                
                目录
              </a>
            </li>
          
            <li>
              <a href="/project">
                
                  <i class="fa fa-folder-open "></i>
                
                代码库
              </a>
            </li>
          
            <li>
              <a href="/photo">
                
                  <i class="fa fa-photo "></i>
                
                相册薄
              </a>
            </li>
          
            <li>
              <a href="/lovetree">
                
                  <i class="fa fa-tree "></i>
                
                爱情树
              </a>
            </li>
          
            <li>
              <a href="/guestbook">
                
                  <i class="fa fa-edit "></i>
                
                留言板
              </a>
            </li>
          
            <li>
              <a href="/about">
                
                  <i class="fa fa-user "></i>
                
                关于我
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            我想去面试系列——Word2vec
            
          </h1>
         
        </div>
         <p class="a-posted-on">
          2020-10-19
          </p>
        <!-- Post Main Content -->
        <div class="entry-content">
          <p>搜罗万象，找寻word2vecv背后的奥妙…<br><a id="more"></a></p>
<!-- toc -->
<ul>
<li><a href="#基本原理">基本原理</a><ul>
<li><a href="#n-gram">N-gram</a></li>
<li><a href="#word2vec">Word2vec</a><ul>
<li><a href="#cbow">CBOW</a></li>
<li><a href="#skip-gram">Skip-gram</a></li>
<li><a href="#hierarchical-softmax">Hierarchical Softmax</a></li>
<li><a href="#negative-sampling">Negative Sampling</a></li>
</ul>
</li>
<li><a href="#glove">Glove</a></li>
</ul>
</li>
<li><a href="#细节-面试题搜集">细节 &amp; 面试题搜集</a></li>
<li><a href="#细节-面试题搜集-1">细节 &amp; 面试题搜集</a></li>
<li><a href="#参考文献">参考文献</a></li>
</ul>
<!-- tocstop -->
<h2><span id="基本原理">基本原理</span></h2><h3><span id="n-gram">N-gram</span></h3><p>在NLP领域，如何计算一段文本序列在某种语言下出现的概率？对于一段文本序列$S=w_1, w_2, … , w_T$，统计语言模型将序列的联合概率转化为一系列条件概率的乘积：</p>
<script type="math/tex; mode=display">P(S)=P(w_1, w_2, ..., w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, ..., w_{t-1})</script><p>由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么用。我们更多的是采用其简化版本 —— N-gram模型，常见的如bi-gram模型（N=2）和tri-gram模型（N=3）。事实上，由于模型复杂度和预测精度的限制，我们很少会考虑N&gt;3的模型。我们可以用最大似然法去求解N-gram模型的参数——等价于去统计每个N-gram的条件词频：</p>
<script type="math/tex; mode=display">p(w_t|w_1, w_2, ..., w_{t-1}) \approx p(w_t|w_{t-n+1}, ..., w_{t-1})</script><p><strong>N-gram模型的缺点</strong>是，①无法处理更长的context（N＞3）②没有考虑词与词之间内在的联系性。Ngram本质上是将词当做一个个孤立的原子单元，然后形式化表达为一个个one-hot向量。显然，one-hot向量的维度等于词典的大小。这在动辄上万甚至百万词典的实际应用中，面临着巨大的维度灾难问题（The Curse of Dimensionality）。于是连续的分布式向量表示（Distributed representation）就产生了。Distributed representation可以解决One-Hot编码存在的问题，它的思路是通过训练，<strong>将原来One-Hot编码的每个词都映射到一个较短的词向量</strong>上来，而这个较短的词向量的维度可以由我们自己在训练时根据任务需要来自己指定。</p>
<h3><span id="word2vec">Word2vec</span></h3><p>首先要说明的一点是，<a href="https://arxiv.org/pdf/1301.3781v3.pdf" target="_blank" rel="noopener">word2vec</a>并不是一个深度模型，其背后只是一个浅层神经网络，并且是一个无监督学习模型。另外需要强调的一点是，word2vec是一个计算word vector的开源工具。当我们在说word2vec算法或模型的时候，其实指的是其背后用于计算word vector的CBoW模型和Skip-gram模型。很多人以为word2vec指的是一个算法或模型，这也是一种谬误。</p>
<p>Word2Vec 的训练模型本质上是只具有一个隐含层的神经元网络。</p>
<p><img src="data:image/png;base64,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" data-original="w2c.jpg" alt=""></p>
<p>它的输入是采用One-Hot编码的词汇表向量，它的输出也是One-Hot编码的词汇表向量。使用所有的样本，训练这个神经元网络，等到收敛之后，从输入层到隐含层的那些权重，便是每一个词的采用Distributed Representation的词向量。这样我们就把原本维数为V的词向量变成了维数为N的词向量（N远小于V），并且词向量间保留了一定的相关关系。Word2Vec其实就是指的是网络中的权重矩阵<code>W</code>，因为输入的每个onehot乘以W只会有一列起作用。</p>
<p>Word2Vec的论文中提出了CBOW和Skip-gram两种模型，CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。下面分别进行介绍：</p>
<h4><span id="cbow">CBOW</span></h4><p>从数学上看，CBoW（Continuous Bag-of-Words）模型等价于一个词袋模型的向量乘以一个Embedding矩阵，从而得到一个连续的embedding向量。这也是CBoW模型名称的由来。</p>
<p><img src="data:image/png;base64,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" data-original="1.jpg" alt=""></p>
<ul>
<li>输入层：c个上下文单词向量$c_i \in R^V$，每个向量都是长度为<code>V</code>（词表大小）的one-hot向量，这里的<code>c</code>由<code>skip_window</code>参数决定，<code>c=2*skip_window</code>，即在左右两侧分别选<code>skip_window</code>个上下文单词；边界条件：当中心词位于边缘时，窗口大小减小（word2vec没有做padding处理）</li>
<li>隐藏层：所有onehot分别乘以共享的输入权重矩阵 $W_{V \times N}$，得到维数为N的向量$w_1,w_2,…,w_c$，这里的$N$自行定义为多少。将这些向量$w_1,w_2,…,w_c$加权求平均作为隐藏层向量$h$；</li>
<li>输出层：$h$再乘一个权重矩阵$W’_{N \times V}$，再过一个激活函数（直接对词典里的V个词计算相似度并归一化，显然是一件极其耗时的impossible mission。为此，Mikolov引入了两种优化算法：层次Softmax和负采样），得到$y \in R^V$，该向量的每一维代表了相对应的单词的概率分布。</li>
<li>$y$中概率最大的元素所指示的单词为预测出的中间词（target word），与true label的onehot词向量做比较，误差越小越好。损失函数一般为交叉熵代价函数。训练完成后矩阵$W_{V \times N}$就是所需要的word embedding。</li>
</ul>
<h4><span id="skip-gram">Skip-gram</span></h4><p>Skip-gram是和CBOW反过来，从直观上理解，Skip-Gram是给定input word来预测上下文。它的做法是，将一个词所在的上下文中的词作为输出，而那个词本身作为输入，也就是说，给出一个词，希望预测可能出现的上下文的词。假如我们有一个句子“The dog barked at the mailman”。首先我们选句子中间的一个词作为我们的输入词，例如我们选取“dog”作为input word；有了input word以后，我们再定义一个叫做<code>skip_window</code>的参数，它代表着我们从当前input word的一侧（左边或右边）选取词的数量。</p>
<p><img src="data:image/png;base64,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" data-original="2.jpg" alt=""></p>
<p>如果我们设置<code>skip_window=2</code>，那么我们最终获得窗口中的词（包括input word在内）就是[‘The’, ‘dog’，’barked’, ‘at’]。<code>skip_window=2</code>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口，所以整个窗口大小<code>span=2x2=4</code>。另一个参数叫<code>num_skips</code>，它代表着我们从整个窗口中选取多少个不同的词作为我们的output word，当<code>skip_window=2，num_skips=</code>2时，我们将会得到两组 (input word, output word) 形式的训练数据，即 (‘dog’, ‘barked’)，(‘dog’, ‘the’)。模型训练的时候就相当于有两个样本数据，每次前向传播softmax求的是当前窗口内的这个词和当前输入词在同一窗口的概率。模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现。对于上边的样例，“The dog barked at the mailman”，假设输入词是”barked”，上下文词有[the, dog, at, the]，那么barked这一个batch中就有四个训练样例，[the,barked], [dog,barked], [barked,at], [barked,the]，然后相当于一个barked要训练四次。但是在预测的时候，只会输入一个barked，然后最后对单词表softmax，得到可能作为barked上下文出现的词的概率分布，最终的topK个单词即为最后的上下文中窗口中预测出现的词。（这就出现了一个问题，比如上下文中有两个the，但是只能预测出来一个，所以这就凸显出来了词袋模型的弊端，忽略次数）</p>
<p>需要注意一点的是，实际上skip-gram的预测部分是没人用的，也就是说word2vec它这个模型的初衷就是为了训练而生的，而不是为了下游任务而生的。所以这也验证了之前经常看过的一句话“<strong>word2vec词向量其实是模型的一个副产品而已</strong>”，所以根本就不用纠结怎么拿word2vec进行预测，只需要掌握它训练的精髓就好了。</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="skip.jpeg" alt=""></p>
<h4><span id="hierarchical-softmax">Hierarchical Softmax</span></h4><p>Word2vec 本质上是一个语言模型，它的输出节点数是 V 个，对应了 V 个词语，本质上是一个多分类问题，但实际当中，词语的个数非常非常多，会给计算造成很大困难，所以需要用技巧来加速训练。下边两个技巧其实并不是word2vec的精髓，只需要简单了解下即可。</p>
<p>Hierarchical Softmax是为了降低原始模型中softmax对于大量词表计算时的计算复杂度，<strong>本质是把 N 分类问题变成 log(N)次二分类</strong>。它对原模型的改进主要有两点，第一点是从输入层到隐藏层的映射，没有采用原先的与矩阵W相乘然后相加求平均的方法，而是直接对所有输入的词向量求和。假设输入的词向量为<code>（0,1,0,0）</code>和<code>（0,0,0,1）</code>，那么隐藏层的向量为<code>（0,1,0,1）</code>。第二点改进是采用哈夫曼树（根据频率建模，出现频率高的越靠近根节点）来替换了原先的从隐藏层到输出层的矩阵<code>W&#39;</code>。哈夫曼树的叶节点个数为词汇表的单词个数<code>V</code>，一个叶节点代表一个单词，而从根节点到该叶节点的路径确定了这个单词最终输出的词向量。这棵哈夫曼树除了根结点以外的所有非叶节点中都含有一个由参数<code>θ</code>确定的<code>sigmoid</code>函数，不同节点中的<code>θ</code>不一样。训练时隐藏层的向量与这个<code>sigmoid</code>函数进行运算，根据结果进行分类，若分类为负类则沿左子树向下传递，编码为0；若分类为正类则沿右子树向下传递，编码为1。</p>
<h4><span id="negative-sampling">Negative Sampling</span></h4><p>对于一些不常见、较生僻的词汇，哈夫曼树在计算它们的词向量时仍然需要做大量的运算。负采样是另一种用来提高Word2Vec效率的方法，它是基于这样的观察：训练一个神经网络意味着使用一个训练样本就要稍微调整一下神经网络中所有的权重，这样才能够确保预测训练样本更加精确，如果能设计一种方法每次只更新一部分权重，那么计算复杂度将大大降低。具体方法就是选取一些期望神经网络输出为0的神经元对应的单词进行训练，这样的话就可以通过较少数目的负样本来更新大部分0对应的权重。<strong>本质是预测总体类别的一个子集</strong>。</p>
<h3><span id="glove">Glove</span></h3><p><a href="https://www.aclweb.org/anthology/D14-1162.pdf" target="_blank" rel="noopener">Glove</a>全称Global Vectors for Word Representation，是基于全局词频统计的词表征工具。他结合了LSA算法可以有效收集语料库全局统计信息的优点（但是无法捕捉上下文信息），还结合了word2vec这种滑动窗口机制的可以通过局部上下文特征表达更丰富语义的特点。它的<strong>核心思想</strong>是，对于任意的词$i$和$j$，假如有第三个词$k$，如果词$k$与$i$相比于词$k$与$j$有更深的关联，那么我们可以得到$k$在$i$的窗口词中出现的概率要大于$k$在$j$的窗口词中出现的概率，且数值较大。如果$k$和$i$与$j$的关系都不大，那么前述的概率应该是约等于的。Glove模型表示的语义词向量相似度尽可能接近在统计共现矩阵中统计相似度，并且不同共现的词有不同权值。可形式化为$F(w_i,w_j,\tilde{w}_k) = \frac{P_{ik}}{P_{jk}}$，式子左边是向量相似度函数，右边是全局的共现统计值。具体实现分为三步，可参考<a href="https://zhuanlan.zhihu.com/p/80335195" target="_blank" rel="noopener">这篇文章</a>：</p>
<ul>
<li><p>根据语料库（corpus）构建一个共现矩阵$X$，<strong>矩阵中的每一个元素$X_{ij}$代表单词$i$和上下文单词$j$在特定大小的上下文窗口内共同出现的次数。</strong>一般而言，这个次数的最小单位是1，但是GloVe不这么认为：它根据两个单词在上下文窗口的距离$d$，提出了一个衰减函数：$decay=1/d$用于计算权重，也就是说<strong>距离越远的两个单词所占总计数（total count）的权重越小</strong>。</p>
</li>
<li><p>构建词向量（Word Vector）和共现矩阵$X$之间的近似关系，损失函数描述如下：</p>
<script type="math/tex; mode=display">J = \sum_{i,j=1}^{V} f(X_{ij})(w_{i}^{T}\tilde{w_{j}} + b_i + \tilde{b_j} – \log(X_{ij}) )^2</script><p>其中，$w_{i}^{T}$和$\tilde{w_{j}}$是我们最终要求解的词向量，$b_i$和$\tilde{b_j}$分别是两个词向量的bias term，$f(X_{ij})$的作用主要是描述相似度的程度，比如共现次数多的单词权重要大于那些共现次数少的（非递减），权重不能太过大到了一定程度就应该停止增加，没有共现过的词应该$f(0)=0$，所以采用了一个分段函数</p>
<script type="math/tex; mode=display">f(x)=\begin{equation} \begin{cases} (x/x_{max})^{\alpha}  & \text{if} \ x < x_{max} \\ 1 & \text{otherwise} \end{cases} \end{equation}​</script></li>
<li><p>Glove其实是有监督的，这个label就是$\log(X_{ij})$，$\log(X_{ij})$是通过共现矩阵的统计数据可以直接计算出来的。</p>
</li>
</ul>
<h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><p>后续更新…</p>
<h2><span id="细节-amp-面试题搜集">细节 &amp; 面试题搜集</span></h2><ul>
<li><p>简述word2vec基本思想，并简要描述CBOW和Skip-gram模型.</p>
<p>答：word2vec的基本思想是一个词的意思， 可以由这个词的上下文来表示。 相似词拥有相似的上下文， 这也就是所谓的<strong>离散分布假设</strong>（distributional hypothesis），论文中的做法是通过神经语言模型训练每个词并将其映射成k维实值向量（k一般为模型中的超参数），在高维空间中可以通过词之间的距离来判断语义相似度。Word2vec <strong>本质上是一个多分类问题</strong>。</p>
<p>CBOW(Continuous Bag of Words，连续词袋模型)采用<strong>给定上下文信息来预测一个词</strong>的方法来训练神经网络，而Skip-gram采用<strong>给定一个词来预测上下文</strong>的方法来训练神经网络。CBOW模型直观上很好理解， 而Skip-gram模型类似于给你若干个词，让你扩展成一句话。</p>
</li>
<li><p>word2vec是如何训练模型的，用的什么语料库？参数量是多少？</p>
<p>答：</p>
<ul>
<li>语料用的Google News corpus，包括6B个token，词汇表40w个单词。</li>
<li>word2vec的参数集中在输入层到隐层的权重矩阵和隐层到输出层的权重矩阵，输入层到隐层的权重矩阵的参数量为$V \times embedding_size$，而隐层到输出层有两种优化方式，对于hierarchical softmax，整棵哈夫曼树的参数量为$(V - 1) \times dim(\theta)$，$dim(\theta)$表示每经过一次二分类时对应参数的维度。对于negative sampling不知道。。。</li>
</ul>
</li>
<li><p>word2vec原理，如何得到词向量？</p>
<p>答： CBOW 和 Skip-gram 两个模型，这两个模型分别是为了预测中心词和 context，词向量是模型的副产物。是如何计算 embedding的，就是两个矩阵来回乘，input 到 hidden layer 其实就是一个 one-hot 向量与矩阵的相乘，本质上也是取矩阵的其中一行；而 hidden layer 取均值之后再做一个映射，乘上另一个矩阵得到输出，输出做 softmax 之后计算对数损失，以此来更新网络中的参数。</p>
</li>
<li><p>word2vec中cbow\skip-gram滑动窗口设定大小有何影响？滑动窗口大小以及负采样个数的参数设置以及设置的比例？</p>
<p>答：负采样的个数和滑动窗口的比例尽量控制在0.1-10之间，滑动窗口决定了正样本的数量，负采样的个数决定了负样本的个数，正负样本尽量不要差距太大，建议负采样的个数和滑动窗口的比例控制为1：1。</p>
<ul>
<li>较大的窗口往往会捕获更多主题/域信息，较小的窗口倾向于捕获更多关于单词本身的内容。滑动窗口通常取固定值为5，滑动窗口的大小决定了模型的计算量。滑动窗口设置太小，每一轮迭代速度虽然快，但是会损失掉部分词语之间联系，导致向量表示学习不充分；滑动窗口设置太大，模型计算量增加，由于窗口增大 ，会给不相关的词语建立联系，误导模型训练。不同的场景，不同的行为，滑动窗口大小也需要随之而改变。滑动窗口的大小在一个模型中甚至可以是可变的值，由中心词的“影响力”决定。例如推荐场景，用户不同的行为，有着不同的影响，普通的点击，窗口可能选择5，对于点赞，分享等较强的信号，会适当的扩大滑动窗口为10。(窗口大小一般理解为两侧加起来一共的个数)</li>
<li>负采样一般采样数设置为5，常用值为3-10，0表示不采用负采样，进一步降低了计算量。</li>
<li>源码中学习率是0.025，每处理10000个样本会按照比例减小学习率；同时设定了学习率的最小值，保证学习率不会无限制的减小。</li>
</ul>
</li>
<li><p>Hierarchical softmax、 Negative sampling的作用？</p>
<p>答：层次softmax的作用就是把原来的$O(N)$计算一遍单词表中每个词的概率转化成哈弗曼树二分计算$O(logN)$。负采样的作用是提高训练速度并且改善所得到的词向量的质量。不同于原本每个训练样本更新所有的权重，<strong>负采样每次让一个训练样本仅仅更新一小部分的权重</strong>，这样就会降低梯度下降过程中的计算量。一个是模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢，第二，中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新。</p>
<p>个人的一个观点就是：层次softmax的作用是在原模型原任务上去修改softmax的计算，是<strong>模型</strong>层面的改进；而负采样则是相当于改变了输入，并且改变了任务，是<strong>任务</strong>层面的改进。以CBOW为例，原模型的输入是$x_{t-2},x_{t-1},x_t,x_{t+1},x_{t+2}$，目的是拿除了$x_t$之外的单词去做一个多分类，但是负采样的做法是将$x_t$替换成一个其他的单词，然后最后面不做多分类了，而是做一个二分类来判断是否是正例。这样的话原来softmax多分类要考虑所有单词，而sigmoid只需要关心正例负例，大大降低了复杂度。</p>
</li>
<li><p>Hierarchical softmax的哈弗曼树怎么构造的？</p>
<p>答：hierarchical softmax的哈弗曼树是根据单词在语料库中的词频来构造的，哈夫曼树的构造过程与数据结构中哈夫曼树的构造过程一致。假设词表中有$n$个单词，则构造出来的哈夫曼树有$n$个叶子结点，假设这$n$个单词的<strong>词频</strong>分别为$w_1,w_2,…,w_n$。</p>
<ol>
<li>将$w_1,w_2,…,w_n$当做$𝑛$棵树（每棵树1个结点）组成的森林。</li>
<li>选择根结点权值最小的两棵树，合并，获得一棵新树，且新树的根结点权值为其左、右子树根结点权值之和。<strong>词频大的结点作为左孩子结点，词频小的作为右孩子结点。</strong></li>
<li>从森林中删除被选中的树，保留新树。</li>
<li>重复2、3步，直至森林中只剩下一棵树为止。</li>
</ol>
</li>
<li><p>叶子结点，根结点，非叶子结点都代表了什么？</p>
<p>答：叶子结点代表词表中的每一个单词，根结点代表投影后的词向量，即隐藏层输出的向量，非叶子结点代表从根结点到某个单词路径上的中间结点，在数据结构中的含义是其所有子树对应结点之和。</p>
</li>
<li><p>Hierarchical softmax的训练过程是怎么样的（哈弗曼树是什么构建的）？<strong>层次softmax</strong>每次是怎么更新参数的？</p>
<p>答：Hierarchical softmax建立哈夫曼树的核心在于，<strong>将多分类问题，拆解成多个二分类问题。</strong></p>
<p><img src="data:image/png;base64,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" data-original="../../../%E9%9D%A2%E8%AF%95%E9%A2%98/huffman.png" alt=""></p>
<ul>
<li><p>如上图所示，我们可以沿着哈夫曼树从根节点一直走到我们的叶子结点的词。与神经网络模型相比，我们的哈夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元。其中，根结点的词向量对应我们投影后的词向量，而所有叶子结点就类似于之前神经网络softmax输出层的神经元，叶子结点的个数就是词汇表的大小。也就是说输入一个词，它会沿着子树不断分叉直到叶子节点。出现频率越高的词其二叉树上的路径越短，即二进制编码越短。</p>
</li>
<li><p>在word2vec原论文中，采用了二元逻辑回归的方法，即<strong>规定沿着左子树走，那么就是负类(哈夫曼树编码为1)，沿着右子树走，那么就是正类(哈夫曼树编码0)。</strong>判别正类和负类的方法是使用sigmoid函数$\sigma(\mathbf{x}_w^{\top}\theta) = \frac{1}{1+e^{-\mathbf{x}_w^{\top}\theta}}$，其中$\mathbf{x}_w$是当前内部结点的词向量，而$\theta$则是我们需要通过训练样本训练得到的逻辑回归的模型参数。（源码中在预处理部分，实现了sigmoid函数值的近似计算，将一个极小区间内的值统一缓存成一个值，微元法）将每个分支看做一次二分类，每一次分类就产生一个概率，将这些概率乘起来，就是所需的 $p( w|Context(w))$。</p>
</li>
<li><p>故在这种建模方式下，条件概率 $p( w|Context(w))$ 的公式可以拆解为：$\prod \limits_{j=2}^{l^w}p(d_j^w|\mathbf{x}_w,\theta_{j-1}^w)$，其中$d_j^w$表示路径$p^w$中第$j$个节点对应的编码（0或1，根节点不对应编码），$\mathbf{x}_w$表示输出层向量也就是叶子节点对应的词向量，$\theta_j^w$表示路径$p^w$第$j$个非叶子节点对应的参数向量，每一项等价于用sigmoid去判别一下。</p>
<script type="math/tex; mode=display">p\left(d_{j}^{w} \mid \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left\{\begin{array}{ll}\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), & d_{j}^{w}=0 \\ 1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right), & d_{j}^{w}=1\end{array}\right.</script><p>考虑到$d$只有0和1两种取值，我们可以用指数形式方便地将其写到一起，可得到$p\left(d_{j}^{w} \mid \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{w}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}^{w}}$，再对条件概率(目标函数)取对数似然，则有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L} &=\sum_{w \in \mathcal{C}} \log \prod_{j=2}^{l^{w}}\left\{\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{w}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}^{w}}\right\} \\
&=\sum_{w \in \mathcal{C}} \sum_{j=2}^{l^{w}}\left\{\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]\right\}
\end{aligned}</script><p>其中$\mathcal{C}$表示语料库，为方便梯度推导，将上式中双重求和符号下花括号中的内容简记为$\mathcal{L}(w, j)$，最后的代价函数可以写成如下形式，类似于对数似然函数：<script type="math/tex">\mathcal{L}(w, j)=\left(1-d_{j}^{w}\right) \cdot \log \left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]+d_{j}^{w} \cdot \log \left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right] \tag{*}</script></p>
</li>
<li><p>下面如何将这个函数最大化？即最大化释然函数，word2vec采用的是<strong>随机梯度上升</strong>(这个需要看别的资料学习)。分别考虑$\mathcal{L}(w, j)$对$\theta_{j-1}^w$的梯度和对$\mathbf{x}_w$的梯度，求导即可。对于Skip-gram模型，其推导过程与CBOW模型基本一致，其目标函数相对于CBOW模型，多了一层求和$\sum_{u \in Context(w)}$。</p>
</li>
<li><p>总结起来就是利用哈夫曼树(最优二叉树)将多分类问题拆解成多个二分类问题，即将条件概率$p(w | \text {Context}(w))$拆解为$\prod \limits_{j=2}^{l^w}p(d_j^w|\mathbf{x}_w,\theta_{j-1}^w)$，其中$x_{w}, \theta_{j-1}^{w}, w \in \mathcal{C}, j=2, \ldots, l^{w}$为待更新参数，利用极大似然得到目标函数，利用随机梯度上升更新参数。</p>
</li>
</ul>
</li>
<li><p>层次softmax的对数似然函数和Logistic回归形式差不多，但是Logistic回归一般是加正则项的，word2vec为什么没加？</p>
<p>答：加正则的本质是减少数据中的误差对模型的影响。word2vec中输入数据是one hot encoding没有误差所以不用加。</p>
</li>
<li><p>为什么要用哈夫曼树而不用别的二叉树？有什么好处？</p>
<p>答：</p>
<ul>
<li>树本身可以二分，所以树结构降低计算复杂度是一定的；</li>
<li>利用哈夫曼树可以很方便的按照词频构造的，因此高频词可以在较短时间内找到。本质上是哈夫曼树可以构成“优先队列”，这样，按照词频降序建立哈夫曼树，保证了高频词接近根结点，这样高频词计算少，低频词计算多。（贪心优化思想)</li>
<li>缺点就是对于生僻单词，需要很长的路径才能叶子节点。</li>
</ul>
</li>
<li><p>Negative sampling负采样的基本思想？如何采样？模型参数是如何更新的？</p>
<p>答：</p>
<ul>
<li><p>负采样的基本思想是，比如我们有一个训练样本，中心词是$w$，它周围上下文共有$2c$($c$为窗口大小)个词，记为$context(w)$，那么就得到一个正例的训练样本$(context(w),w)$，通过Negative Sampling采样，我们得到$neg$个和$w$不同的中心词$w_i, i=1, 2, \cdots, neg$，这样$context(w)$和$w_i$就组成了$neg$个并不真实存在的负例。利用这一个正例和$neg$个负例，我们进行二元逻辑回归，得到负采样对应每个词$w_i$对应的模型参数$\theta_i$，和每个词的词向量。</p>
</li>
<li><p>关于如何采样，不同的词在语料中出现次数有高有低，对于那些高频词，被选为负样本的概率就应该比较大；反之，对于那些低频词，其被选中的概率就应该比较小。这就是我们对采样过程的一个大致要求，本质上就是一个<strong>带权采样问题</strong>。<strong>负采样为什么要用词频来做采样概率？</strong>因为这样可以让频率高的词先学习，然后带动其他词的学习。首先在语料库中计算每个词的词频，即某个词的count/语料库中所有单词总count（源码中分子分母都用的是count四分之三次方，并非直接用count），即为被采样到的概率：</p>
<script type="math/tex; mode=display">\operatorname{len}(w)=\frac{[\text { counter }(w)]^{\frac{3}{4}}}{\sum_{u \in \mathcal{D}}[\text { counter }(u)]^{\frac{3}{4}}}</script><p>在word2vec里的具体做法是，经过概率的计算，可将所有词映射到一条线上，这条线是非等距划分的N份（因为N个词的概率不同，N为词汇表的长度），再对同样的线进行等距划分成M份，其中M &gt;&gt; N，在采样时，每次随机生成一个[1,M-1]之间的数，在对应到该小块所对应的单词区域，选取该单词（当选到正例单词自己时，跳过），在源码中，M的取值为$10^8$。</p>
</li>
<li><p>关于参数更新，Negative Sampling也是采用了二元逻辑回归来求解模型参数，通过负采样，我们得到了$neg$个负例$neg(context(w), w_i)  i=1, 2, \cdots, neg$。为了统一描述，我们将正例定义为$(context(w), w_0)$。下面以CBOW模型为例简单讲解一下。在逻辑回归中，我们的正例应该期望满足：<script type="math/tex">P(context(w_0),w_i)=\sigma(x_{w_0}^Tθ^{w_i}),y_i=1,i=0</script>负例期望满足：<script type="math/tex">P(context(w_0),w_i)=1-\sigma(x_{w0}^Tθ^{w_i}),y_i=0,i=1,2,..neg</script>根据极大似然估计，希望最大化<script type="math/tex">\prod \limits_{i=0}^{n e g} P\left(\operatorname{context}\left(w_{0}\right), w_{i}\right)=\sigma\left(x_{w_{0}}^{T} \theta^{w_{0}}\right) \prod \limits_{i=1}^{n e g}\left(1-\sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)\right)</script>进一步可01化简为<script type="math/tex">g(w)=\prod \limits_{i=0}^{n e g} \sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)^{y_{i}}\left(1-\sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)\right)^{1-y_{i}}</script>给定一个语料库$\mathcal{C}$，整体的优化目标可表示为$G=\prod \limits_{w\in \mathcal{C}}g(w)$。此时对应的代价函数就是个对数似然函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathcal{L} &=\log G=\log \prod_{w \in \mathcal{C}} g(w)=\sum_{w \in \mathcal{C}} \log g(w) \\
&=\sum_{w \in \mathcal{C}} \log \prod_{i=0}^{n e g} \sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)^{y_{i}}\left(1-\sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)\right)^{1-y_{i}} \\
&=\sum_{w \in \mathcal{C}} \sum_{i=0}^{n e g} y_{i} \log \left(\sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)\right)+\left(1-y_{i}\right) \log \left(1-\sigma\left(x_{w_{0}}^{T} \theta^{w_{i}}\right)\right)
\end{aligned}</script><p>和hierarchical softmax类似，负采样同样采用随机梯度上升法更新参数，参考上文。</p>
</li>
</ul>
</li>
<li><p>对比一下层次softmax和负采样？</p>
<p>答：优化目标不一样。HS让每个非叶子节点去预测要选择的路径（每个节点是个二分类问题），目标函数是最大化路径上的二分类概率，公式（1）（2）所示。</p>
<script type="math/tex; mode=display">p(w \mid \operatorname{context}(w))=\prod_{j=2}^{l(w)} p\left(d_{j}^{w} \mid \mathbf{x}_{w}, \theta_{j-1}^{w}\right) \tag{1}</script><script type="math/tex; mode=display">p\left(d_{j}^{w} \mid \mathbf{x}_{w}, \theta_{j-1}^{w}\right)=\left[\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{1-d_{j}^{w}} \cdot\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta_{j-1}^{w}\right)\right]^{d_{j}^{w}} \tag{2}</script><p>负采样是最大化正样例概率同时最小化负样例概率，公式（3）所示。</p>
<script type="math/tex; mode=display">g(w)=\sigma\left(\mathbf{x}_{w}^{\top} \theta^{w}\right) \prod_{u \in N E G(w)}\left[1-\sigma\left(\mathbf{x}_{w}^{\top} \theta^{u}\right)\right] \tag{3}</script></li>
<li><p>为什么训练完有两套词向量，为什么一般只用前一套？</p>
<p>答：因为前边这个矩阵和one-hot输入关系更紧密，相当于就是one-hot的一一映射。</p>
</li>
<li><p>怎么衡量学到的embedding的好坏 ？</p>
<p>答：词向量的评价大体上可以分成两种方式：</p>
<ul>
<li>第一种是把词向量融入现有系统中，看对系统性能的提升，比如直接作为输入输给FFN然后接下游任务；</li>
<li>第二种是直接从语言学的角度对词向量进行分析，如相似度、语义偏移、类比（analogy）等。</li>
</ul>
</li>
<li><p>word2vec中为什么使用负采样？负采样中噪声数据是否更新？（word2vec如何提高训练速度？）</p>
<p>答：保证了模型训练的效果，模型每次只需要更新采样的词的权重，不用更新所有的权重，那样会很慢；并且中心词其实只跟它周围的词有关系，位置离着很远的词没有关系，也没必要同时训练更新。<strong>负采样中的噪声数据是更新的。</strong>来定量的感受下负采样节省的计算量：假设有 <code>1W</code> 个单词，<code>300</code> 个隐藏单元，则普通word2vec输出向量的大小为 <code>(300, 10000)</code>，现在我们通过负采样选取了 <code>5</code> 个负例，加上原本的 <code>1</code>个正例共 <code>6</code> 个输出神经元，此时只需要更新 <code>300 * 6</code> 个参数，相当于原来的 <code>0.06%</code>。另外，对于输入向量来说，无论是否使用负采样，其更新权重数量都不会改变。</p>
</li>
<li><p>在语料中有些词比较稀疏又想学好的情况下，使用word2vec时如何选用cbow/skipgram，Hierarchical softmax，Negative sampling？</p>
<p>答：Skip-gram对生僻词处理较好；层次softmax对大词汇量处理较好；负采样加速训练，提高embedding质量。</p>
</li>
<li><p>word2vec对比 Skip-gram 和 CBOW？</p>
<p>答：</p>
<ul>
<li>训练速度上 CBOW 应该会更快一点。因为CBOW每次会更新 context(w) 的词向量，而 Skip-gram 只更新核心词的词向量。两者的预测时间复杂度分别是 <code>O(V)</code>，<code>O(KV)</code> ，这里的$K$我们暂且认为是softmax取topK产生的复杂度。CBOW训练速度要比skip-gram快，原因在于一个滑动窗口内只能产生一个sample，而skip-gram可以产生多个。</li>
<li>Skip-gram 对低频词效果比 CBOW好。因为是尝试用当前词去预测上下文，当前词是低频词还是高频词没有区别。但是 CBOW 相当于是完形填空，会选择最常见或者说概率最大的词来补全，因此不太会选择低频词。Skip-gram 在大一点的数据集可以提取更多的信息。<code>SG 总体比 CBOW 要好一些</code>。</li>
</ul>
</li>
<li><p>word2vec 随机初始化 与 onehot 的区别？</p>
<p>答：one-hot大小受限于词典大小，而随机初始化不受这个限制。此外是否onehot太过于稀疏，使得矩阵运算太慢？<del>暂时没懂。</del></p>
</li>
<li><p>glove与word2vec的区别？</p>
<p>答：可以看<a href="https://zhuanlan.zhihu.com/p/31023929" target="_blank" rel="noopener">这篇文章</a>。</p>
<ul>
<li>从模型上来看，GloVe与word2vec，两个模型都可以根据词汇的“共现co-occurrence”信息，将词汇编码成一个向量（所谓共现，即语料中词汇一块出现的频率）。两者最直观的区别在于，word2vec是“predictive”的模型，而GloVe是“count-based”的模型。predictive 用神经网络进行预测，count-based计算两个词同时出现的频率进行预测。</li>
<li>从数据集语料来看，glove复杂度与数据集规模无关（因为其核心思想就是对共现矩阵重构，是可以并行运算的，所以训练速度更快），word2vec复杂度与数据集规模关系密切（因为预测上下文单词要过一遍词表）。</li>
</ul>
</li>
<li><p>一串连问系列：词向量模型怎么训练的，word2vec的原理，两种训练方式，两种加速技巧，哈夫曼树怎么生成的，有了哈夫曼树之后怎么训练的，怎么就把n分类问题变成了logn个二分类问题？</p>
<p>答：前边已经说过了，自己往前翻。</p>
</li>
<li><p>word2vec（稠密）和TF-IDF（稀疏）的区别？</p>
<p>答：1、稠密的 低维度的 2、表达出相似度； 3、表达能力强；4、泛化能力强。</p>
</li>
<li><p>word2vec优缺点，他基于的假设是啥？</p>
<p>答：优点：</p>
<ul>
<li>由于 word2vec 会考虑上下文，跟之前的 Embedding 方法相比，效果要更好（但不如 18 年之后的方法）；</li>
<li>网络结构简单，而且word embedding维度比较低，训练速度比较快；</li>
<li>通用性很强，可以用在各种 NLP 任务中。</li>
</ul>
<p>缺点：</p>
<ul>
<li>致命缺点是只能embedding出静态词向量，多义词问题无法解决；</li>
<li>忽略了词序；</li>
<li>word2vec训练模式下，考虑的上下文很小，没有用到word在全局的信息统计。</li>
</ul>
<p>基于的假设是<strong>相似词拥有相似的上下文</strong>， 这也就是所谓的<strong>离散分布假设</strong>（distributional hypothesis）。</p>
</li>
<li><p>word2vec和bert做embedding的区别？</p>
<p>答：word2vec是静态的，不包含上下文语境信息；BERT更动态一点，蕴含丰富的上下文。</p>
</li>
<li><p>word2vec里有皇后-女人=国王-男人，BERT里可以吗？</p>
<p>答：不可以。因为BERT给出的词向量编码了上下文信息，不再满足矢量相加。有文章发现这些上下文相关的词向量学到了语法的树结构，可能语义层次更高级更抽象，不再是在矢量运算层面了。</p>
</li>
<li><p>word2vec如何处理未登录词OOV？</p>
<p>答：最常见的做法是随机化。</p>
<ul>
<li>在训练过程中，把训练集中所有出现频率小于某个阈值的词都标记为UNK，那么UNK就会学到了自己的embedding。当推理时遇见一个新词，可以直接视作<code>[UNK]</code>（比如固定对应成0的向量），但是这样的话可能大量单词映射成了同一个向量，也可以选择随机化（这个更好）。</li>
<li>利用Character Model，即把所有的OOV词，拆成单个字符。这样处理的好处就是消灭了全部的OOV。坏处就是文本序列变得非常长，对于性能敏感的系统，这是难以接受的维度增长。</li>
<li>扩大词表，终极解决办法。通常情况不使用大词表，一方面是因为训练数据的多样性有限，另一方面是softmax的计算速度受限。而对于上述两种情况，也有针对性解决办法。对于第一种情况，扩大语料范围。对于第二种情况，相关的加速策略可以将词表扩大10倍而GPU上的预测速度只降低一半（从5W词到50W词）。</li>
</ul>
</li>
<li><p>BERT的masked language model 和CBOW有什么异同之处？</p>
<p>答：见 <a href="https://vodkazy.cn/2020/10/10/我想去面试系列——BERT" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/10/我想去面试系列——BERT</a></p>
</li>
</ul>
<h2><span id="参考文献">参考文献</span></h2><ol>
<li>Word2Vec：<a href="https://zhuanlan.zhihu.com/p/140705629" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/140705629</a></li>
<li>Word2Vec详解：<a href="https://zhuanlan.zhihu.com/p/61635013" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/61635013</a></li>
<li>Glove详解：<a href="http://www.fanyeong.com/2018/02/19/glove-in-detail/" target="_blank" rel="noopener">http://www.fanyeong.com/2018/02/19/glove-in-detail/</a></li>
<li>Glove模型的理解：<a href="https://zhuanlan.zhihu.com/p/80335195" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/80335195</a></li>
<li>【深度学习】Word2Vec：<a href="https://www.hrwhisper.me/deep-learning-word2vec/" target="_blank" rel="noopener">https://www.hrwhisper.me/deep-learning-word2vec/</a></li>
<li>Embedding之word2vec：<a href="https://zhuanlan.zhihu.com/p/59396559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59396559</a></li>
<li>关于word2vec的一些相关问题整理 &amp; 思考：<a href="https://blog.csdn.net/liujian20150808/article/details/105215414" target="_blank" rel="noopener">https://blog.csdn.net/liujian20150808/article/details/105215414</a></li>
<li>搞懂NLP中的词向量，看这一篇就足够：<a href="https://www.infoq.cn/article/PFvZxgGDm27453BbS24W" target="_blank" rel="noopener">https://www.infoq.cn/article/PFvZxgGDm27453BbS24W</a></li>
<li>GloVe与word2vec的区别：<a href="https://zhuanlan.zhihu.com/p/31023929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31023929</a></li>
<li>基于TensorFlow实现Skip-Gram模型：<a href="https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html" target="_blank" rel="noopener">https://www.leiphone.com/news/201706/QprrvzsrZCl4S2lw.html</a></li>
</ol>
<blockquote>
<p>本文来源：「想飞的小菜鸡」的个人网站  <a href="https://vodkazy.cn" target="_blank" rel="noopener">vodkazy.cn</a></p>
<p>版权声明：本文为「想飞的小菜鸡」的原创文章，采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请附上原文出处链接及本声明。</p>
<p>原文链接：<a href="https://vodkazy.cn/2020/10/19/我想去面试系列——Word2vec" target="_blank" rel="noopener">https://vodkazy.cn/2020/10/19/我想去面试系列——Word2vec</a></p>
</blockquote>

        </div>
      </article>
    </div>

	<!-- 打赏 -->
    <div class="reward">
	<div class="reward-button">赏 <span class="reward-code">
		<span class="alipay-code"> <img class="alipay-img wdp-appear" src="data:image/png;base64,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" data-original="/images/alipay.webp"><b>支付宝打赏</b> </span> 
		<span class="wechat-code"> <img class="wechat-img wdp-appear" src="data:image/png;base64,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" data-original="/images/weixin.webp"><b>微信打赏</b> </span> </span>
	</div>
	<p class="reward-notice">如果文章对你有帮助，欢迎点击上方按钮打赏作者，更多文章请访问<a href="https://vodkazy.cn" style="color:blue">想飞的小菜鸡</a></p>
	    <style>
		*,*:before,*:after {
			-webkit-box-sizing: border-box;
			-moz-box-sizing: border-box;
			-ms-box-sizing: border-box;
			box-sizing: border-box
		}

		.reward {
			padding: 5px 0
		}

		.reward .reward-notice {
			font-size: 14px;
			line-height: 14px;
			margin: 15px auto;
			text-align: center
		}

		.reward .reward-button {
			font-size: 28px;
			line-height: 58px;
			position: relative;
			display: block;
			width: 60px;
			height: 60px;
			margin: 0 auto;
			padding: 0;
			-webkit-user-select: none;
			text-align: center;
			vertical-align: middle;
			color: #fff;
			border: 1px solid #f1b60e;
			border-radius: 50%;
			background: #fccd60;
			background: -webkit-gradient(linear,left top,left bottom,color-stop(0,#fccd60),color-stop(100%,#fbae12),color-stop(100%,#2989d8),color-stop(100%,#207cca));
			background: -webkit-linear-gradient(top,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%);
			background: linear-gradient(to bottom,#fccd60 0,#fbae12 100%,#2989d8 100%,#207cca 100%)
		}

		.reward .reward-code {
			position: absolute;
			top: -220px;
			left: 50%;
			display: none;
			width: 350px;
			height: 200px;
			margin-left: -175px;
			padding: 15px;
			border: 1px solid #e6e6e6;
			background: #fff;
			box-shadow: 0 1px 1px 1px #efefef
		}

		.reward .reward-button:hover .reward-code {
			display: block
		}

		.reward .reward-code span {
			display: inline-block;
			width: 150px;
			height: 150px
		}

		.reward .reward-code span.alipay-code {
			float: left
		}

		.reward .reward-code span.alipay-code a {
			padding: 0
		}

		.reward .reward-code span.wechat-code {
			float: right
		}

		.reward .reward-code img {
			display: inline-block;
			float: left;
			width: 150px;
			height: 150px;
			margin: 0 auto;
			border: 0
		}

		.reward .reward-code b {
			font-size: 14px;
			line-height: 26px;
			display: block;
			margin: 0;
			text-align: center;
			color: #666
		}

		.reward .reward-code b.notice {
			line-height: 2rem;
			margin-top: -1rem;
			color: #999
		}

		.reward .reward-code:after,.reward .reward-code:before {
			position: absolute;
			content: '';
			border: 10px solid transparent
		}

		.reward .reward-code:after {
			bottom: -19px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #fff
		}

		.reward .reward-code:before {
			bottom: -20px;
			left: 50%;
			margin-left: -10px;
			border-top-color: #e6e6e6
		}
    </style>


    <!-- Pre or Next -->
    
	<div class="container">
           <ul class="pager">
    	     
      	     <li class="previous">
              <a href="/2020/10/29/我想去面试系列——Attention与Transformer/" rel="prev">上一篇</a>
             </li>
           
           
              <li class="next">
              <a href="/2020/10/14/我想去面试系列——BERT源码品读/" rel="prev">下一篇</a>
            </li>
           
          </ul>
       </div>
   

    <!-- Valine无后端评论系统 -->   
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
    <div id="vcomments"></div>
    <script>
        new Valine({
		    el: '#vcomments' ,
		    appId: 'lH3VkMCd4MHaKtr2n2SRWdoi-MdYXbMMI',
		    appKey: '5aMXSY7b4KwnzfgpzLA0hPLv',
		    notify:true, 
		    verify:false, 
		    placeholder: '填写正确的邮箱和昵称才能收到我的回复哦       ٩( ^o^ )و  ' ,
		    avatar: 'retro'
		});
    </script>
    <!-- Valine无后端评论系统 -->  

  </div>
</div>
</div>

  <!-- Footer -->
  <!-- Footer -->
<footer class="site-info">
  <p>
    <span>想飞的小菜鸡 &copy; 2021</span>
    
      <span class="split">|</span>
      <span>照耀的Blog</span>
    
  </p>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    本站总访问量<span id="busuanzi_value_site_pv"></span>次
    本站访客数<span id="busuanzi_value_site_uv"></span>人次
    本文总阅读量<span id="busuanzi_value_page_pv"></span>次
</footer>

  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>


 
  <!-- 使用 aotuload.js 引入看板娘 -->    
  <!-- //<script src="/js/assets/jquery.min.js?v=3.3.1"></script> -->   
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <!-- //<script src="/js/assets/jquery-ui.min.js?v=1.12.1"></script>   --> 
  <script src="https://cdn.jsdelivr.net/npm/jquery-ui-dist@1.12.1/jquery-ui.min.js"></script>
  <script src="/js/assets/autoload.js?v=1.4.2"></script>
  <!-- //<script src="https://live2d-cdn.fghrsh.net/assets/1.4.2/autoload.js></script> -->   
   


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

<script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var i=n.imageLazyLoadSetting.isSPA,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){i&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,e,a=0;a<r.length;a++)t=r[a],e=void 0,0<=(e=t.getBoundingClientRect()).bottom&&0<=e.left&&e.top<=(n.innerHeight||document.documentElement.clientHeight)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},n.src=i}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>

</html>
